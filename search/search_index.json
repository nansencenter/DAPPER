{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Home <p>DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. The numerical experiments provide support and guidance for new developments in DA. The typical set-up is a synthetic (twin) experiment, where you specify a dynamic model and an observational model, and use these to generate a synthetic truth (multivariate time series), and then estimate that truth given the models and noisy observations.</p> <p> </p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Read &amp; run examples/<code>basic_1.py</code> and <code>basic_2.py</code>,   or their corresponding notebooks    (requires Google login).</li> <li>This screencast   provides an overview to DAPPER.</li> <li>Install.</li> <li>The documentation   includes general guidelines and the API reference,   but most users must expect to read the code as well.</li> <li>If used towards a publication, please cite as   The experiments used (inspiration from) DAPPER [ref], version 1.6.0,   or similar, where [ref] points to .</li> <li>Also see the interactive tutorials on DA theory with Python.</li> </ul>"},{"location":"#highlights","title":"Highlights","text":"<p>DAPPER enables the numerical investigation of DA methods through a variety of typical test cases and statistics. It (a) reproduces numerical benchmarks results reported in the literature, and (b) facilitates comparative studies, thus promoting the (a) reliability and (b) relevance of the results. For example, the figure below is generated by <code>docs/examples/basic_3.py</code>, reproduces figure 5.7 of these lecture notes. DAPPER is \u00a9 open source, written in Python, and (d) focuses on readability; this promotes the \u00a9 reproduction and (d) dissemination of the underlying science, and makes it easy to adapt and extend.</p> <p></p> <p>DAPPER demonstrates how to parallelise ensemble forecasts (e.g., the QG model), local analyses (e.g., the LETKF), and independent experiments (e.g., <code>docs/examples/basic_3.py</code>). It includes a battery of diagnostics and statistics, which all get averaged over subdomains (e.g., \"ocean\" and \"land\") and then in time. Confidence intervals are computed, including correction for auto-correlations, and used for uncertainty quantification, and significant digits printing. Several diagnostics are included in the on-line \"liveplotting\" illustrated below, which may be paused for further interactive inspection. In summary, DAPPER is well suited for teaching and fundamental DA research. Also see its drawbacks.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Successfully tested on Linux/Mac/Windows.</p>"},{"location":"#prerequisite-python312","title":"Prerequisite: Python&gt;=3.12","text":"<p>If you're an expert, setup a python environment however you like. Otherwise: Install Anaconda, then open the Anaconda terminal and run the following commands:</p> <pre><code>conda create --yes --name dapper-env python=3.12\nconda activate dapper-env\npython --version\n</code></pre> <p>Ensure the printed version is as desired. Keep using the same terminal for the commands below.</p>"},{"location":"#install","title":"Install","text":""},{"location":"#either-install-for-development-recommended","title":"Either: Install for development (recommended)","text":"<p>Do you want the DAPPER code available to play around with? Then</p> <ul> <li>Download and unzip (or <code>git clone</code>) DAPPER.</li> <li>Move the resulting folder wherever you like,   and <code>cd</code> into it (ensure you're in the folder with a <code>setup.py</code> file).</li> <li><code>pip install -e '.'</code> </li> </ul>"},{"location":"#or-install-as-library","title":"Or: Install as library","text":"<p>Do you just want to run a script that requires DAPPER? Then</p> <ul> <li>If the script comes with a <code>requirements.txt</code> file that lists DAPPER, then do <code>pip install -r path/to/requirements.txt</code>.</li> <li>If not, hopefully you know the version of DAPPER needed. Run <code>pip install dapper==1.6.0</code> to get version <code>1.6.0</code> (as an example).</li> </ul>"},{"location":"#finally-test-the-installation","title":"Finally: Test the installation","text":"<p>You should now be able to do run your script with <code>python path/to/script.py</code>. For example, if you are in the DAPPER dir,</p> <pre><code>python docs/examples/basic_1.py\n</code></pre> <p>PS: If you closed the terminal (or shut down your computer), you'll first need to run <code>conda activate dapper-env</code></p>"},{"location":"#da-methods","title":"DA methods","text":"Method Literature reproduced EnKF <sup>1</sup> Sakov08, Hoteit15, Grudzien2020 EnKF-N Bocquet12, Bocquet15 EnKS, EnRTS Raanes2016 iEnKS / iEnKF / EnRML / ES-MDA <sup>2</sup> Sakov12, Bocquet12, Bocquet14 LETKF, local &amp; serial EAKF Bocquet11 Sqrt. model noise methods Raanes2014 Particle filter (bootstrap) <sup>3</sup> Bocquet10 Optimal/implicit Particle filter  <sup>3</sup> Bocquet10 NETF To\u0308dter15, Wiljes16 Rank histogram filter (RHF) Anderson10 4D-Var 3D-Var Extended KF Optimal interpolation Climatology <p><sup>1</sup>: Stochastic, DEnKF (i.e. half-update), ETKF (i.e. sym. sqrt.). Serial forms are also available. Tuned with inflation and \"random, orthogonal rotations\". <sup>2</sup>: Also supports the bundle version, and \"EnKF-N\"-type inflation. <sup>3</sup>: Resampling: multinomial (including systematic/universal and residual). The particle filter is tuned with \"effective-N monitoring\", \"regularization/jittering\" strength, and more.</p> <p>For a list of ready-made experiments with suitable, tuned settings for a given method (e.g., the <code>iEnKS</code>), use:</p> <pre><code>grep -r \"xp.*iEnKS\" dapper/mods\n</code></pre>"},{"location":"#test-cases-models","title":"Test cases (models)","text":"<p>Simple models facilitate the reliability, reproducibility, and interpretability of experiment results.</p> Model Lin TLM<code>**</code> PDE? Phys.dim. State len Lyap\u22650 Implementer Id Yes Yes No N/A <code>*</code> 0 Raanes Linear Advect. (LA) Yes Yes Yes 1d 1000 <code>*</code> 51 Evensen/Raanes DoublePendulum No Yes No 0d 4 2 Matplotlib/Raanes Ikeda No Yes No 0d 2 1 Raanes LotkaVolterra No Yes No 0d 5 <code>*</code> 1 Wikipedia/Raanes Lorenz63 No Yes \"Yes\" 0d 3 2 Sakov Lorenz84 No Yes No 0d 3 2 Raanes Lorenz96 No Yes No 1d 40 <code>*</code> 13 Raanes Lorenz96s No Yes No 1d 10 <code>*</code> 4 Grudzien LorenzUV No Yes No 2x 1d 256 + 8 <code>*</code> \u224860 Raanes LorenzIII No No No 1d 960 <code>*</code> \u2248164 Raanes Vissio-Lucarini 20 No Yes No 1d 36 <code>*</code> 10 Yumeng Kuramoto-Sivashinsky No Yes Yes 1d 128 <code>*</code> 11 Kassam/Raanes Quasi-Geost (QG) No No Yes 2d 129\u00b2\u224817k \u2248140 Sakov <ul> <li><code>*</code>: Flexible; set as necessary</li> <li><code>**</code>: Tangent Linear Model included?</li> </ul> <p>The models are found as subdirectories within <code>dapper/mods</code>. A model should be defined in a file named <code>__init__.py</code>, and illustrated by a file named <code>demo.py</code>. Most other files within a model subdirectory are usually named <code>authorYEAR.py</code> and define a <code>HMM</code> object, which holds the settings of a specific twin experiment, using that model, as detailed in the corresponding author/year's paper. A list of these files can be obtained using</p> <pre><code>find dapper/mods -iname '[a-z]*[0-9]*.py'\n</code></pre> <p>Some files contain settings used by several papers. Moreover, at the bottom of each such file should be (in comments) a list of suitable, tuned settings for various DA methods, along with their expected, average <code>rmse.a</code> score for that experiment. As mentioned above, DAPPER reproduces literature results. You will also find results that were not reproduced by DAPPER.</p>"},{"location":"#similar-projects","title":"Similar projects","text":"<p>DAPPER is aimed at research and teaching (see discussion up top). Example of limitations:</p> <ul> <li>It is not suited for very big models (&gt;60k unknowns).</li> <li>Non-uniform time sequences.</li> </ul> <p>The scope of DAPPER is restricted because</p> <p></p> <p>Moreover, even straying beyond basic configurability appears unrewarding when already building on a high-level language such as Python. Indeed, you may freely fork and modify the code of DAPPER, which should be seen as a set of templates, and not a framework.</p> <p>Also, DAPPER comes with no guarantees/support. Therefore, if you have an operational or real-world application, such as WRF, you should look into one of the alternatives, sorted by approximate project size.</p> Name Developers Purpose (approximately) DART NCAR General PDAF AWI General JEDI JCSDA (NOAA, NASA, ++) General OpenDA TU Delft General EMPIRE Reading (Met) General ERT Statoil History matching (Petroleum DA) PIPT CIPR History matching (Petroleum DA) MIKE DHI Oceanographic OAK Li\u00e8ge Oceanographic Siroco OMP Oceanographic Verdandi INRIA Biophysical DA PyOSSE Edinburgh, Reading Earth-observation DA <p>Below is a list of projects with a purpose more similar to DAPPER's (research in DA, and not so much using DA):</p> Name Developers Notes DAPPER Raanes, Chen, Grudzien Python SANGOMA Conglomerate* Fortran, Matlab hIPPYlib Villa, Petra, Ghattas Python, adjoint-based PDE methods FilterPy R. Labbe Python. Engineering oriented. DASoftware Yue Li, Stanford Matlab. Large inverse probs. Pomp U of Michigan R EnKF-Matlab Sakov Matlab EnKF-C Sakov C. Light-weight, off-line DA pyda Hickman Python PyDA Shady-Ahmed Python DasPy Xujun Han Python DataAssim.jl Alexander-Barth Julia DataAssimilationBenchmarks.jl Grudzien Julia, Python EnsembleKalmanProcesses.jl Clim. Modl. Alliance Julia, EKI (optim) Datum Raanes Matlab IEnKS code Bocquet Python <p>The <code>EnKF-Matlab</code> and <code>IEnKS</code> codes have been inspirational in the development of DAPPER.</p> <p><code>*</code>: AWI/Liege/CNRS/NERSC/Reading/Delft</p>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#issues-and-pull-requests","title":"Issues and Pull requests","text":"<p>Do not hesitate to open an issue, whether to report a problem or ask a question. It may take some time for us to get back to you, since DAPPER is primarily a volunteer effort. Please start by perusing the documentation and searching the issue tracker for similar items.</p> <p>Pull requests are very welcome. Examples: adding a new DA method, dynamical models, experimental configuration reproducing literature results, or improving the features and capabilities of DAPPER. Please keep in mind the intentional limitations and read the developers guidelines.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>Patrick N. Raanes, Yumeng Chen, Colin Grudzien, Maxime Tondeur, Remy Dubois</p> <p>DAPPER is developed and maintained at NORCE (Norwegian Research Institute) and the Nansen Environmental and Remote Sensing Center (NERSC), in collaboration with the University of Reading, the UK National Centre for Earth Observation (NCEO), and the Center for Western Weather and Water Extremes (CW3E).</p> <p> </p>"},{"location":"#publications","title":"Publications","text":"<ul> <li>Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: A case study with the Lorenz 96 model</li> <li>Adaptive covariance inflation in the ensemble Kalman filter by Gaussian scale mixtures</li> <li>Revising the stochastic iterative ensemble smoother</li> <li>p-Kernel Stein Variational Gradient Descent for Data Assimilation and History Matching</li> <li>Springer book chapter: Data Assimilation for Chaotic Dynamics</li> </ul>"},{"location":"dev_guide/","title":"Developer guide","text":""},{"location":"dev_guide/#conventions","title":"Conventions","text":"<ul> <li>Ensemble (data) matrices are <code>np.ndarrays</code> with shape <code>N-by-Nx</code>.   This shape (orientation) is contrary to the EnKF literature,   but has the following advantages:<ul> <li>Improves speed in row-by-row accessing,   since that's <code>np</code>'s default orientation.</li> <li>Facilitates broadcasting for, e.g. centring the matrix.</li> <li>Fewer indices: <code>[n,:]</code> yields same as <code>[n]</code></li> <li>Beneficial operator precedence without <code>()</code>.   E.g. <code>dy @ Rinv @ Y.T @ Pw</code> (where <code>dy</code> is a vector)</li> <li>Less transposing for for ensemble space formulae.</li> <li>It's the standard for data matrices in the broader statistical literature.</li> </ul> </li> <li>Naming:<ul> <li><code>E</code>: ensemble matrix</li> <li><code>w</code>: ensemble weights or coefficients</li> <li><code>X</code>: centred ensemble (\"anomalies\")</li> <li><code>N</code>: ensemble size</li> <li><code>Nx</code>: state size</li> <li><code>Ny</code>: observation size</li> <li>Double letters means a sequence of something.   For example:<ul> <li><code>xx</code>: Time series of truth; shape <code>(K+1, Nx)</code></li> <li><code>yy</code>: Time series of observations; shape <code>(Ko+1, Nx)</code></li> <li><code>EE</code>: Time series of ensemble matrices</li> <li><code>ii</code>, <code>jj</code>: Sequences of indices (integers)</li> </ul> </li> <li><code>xps</code>: an <code>xpList</code> or <code>xpDict</code>,   where <code>xp</code> abbreviates \"experiment\".</li> </ul> </li> </ul>"},{"location":"dev_guide/#install-for-development","title":"Install for development","text":"<p>Include the dev tools as part of the installation (detailed in the README):</p> <pre><code>pip install -e '.[dev]'\n</code></pre> <p>PS: If you want to be able to use static analysis tools (<code>pyright</code>) with dapper all the while working from outside its directory, you should also append <code>--config-settings editable_mode=compat</code> to the above command. Ref pyright doc and pyright issue. Alternatively, there is the <code>extraPaths</code> setting.</p>"},{"location":"dev_guide/#run-tests","title":"Run tests","text":"<p>By default, only <code>doctests</code> are run when executing <code>pytest</code>. To run the main tests, do this:</p> <pre><code>pytest tests\n</code></pre> <p>You can also append <code>test_plotting.py</code> for example, which is otherwise ignored for being slow.</p> <p>If the test with the <code>QG</code> model in <code>test_HMM.py</code> fails (simply because you have not compiled it) that is fine (that test does not run in CI either).</p>"},{"location":"dev_guide/#pre-commit-and-linting","title":"Pre-commit and linting","text":"<p>Pull requests (PR) to DAPPER are checked with continuous integration (CI), which runs the tests, and also linting, plus some <code>pre-commit</code> hooks. To avoid having to wait for the CI server to run all of this, you'll want to run them on your own computer:</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre> <p>Now every time you commit, these tests will run on the staged files. For detailed linting messages, run</p> <pre><code>ruff check --output-format=concise\n</code></pre> <p>You may also want to display linting issues in your editor as you code.</p>"},{"location":"dev_guide/#writing-documentation","title":"Writing documentation","text":"<p>The documentation is built with <code>mkdocs</code> and should be written in markdown syntax. You can preview the rendered html docs by running</p> <pre><code>mkdocs serve\n</code></pre> <ul> <li>Temporarily disable <code>mkdocs-jupyter</code> in <code>mkdocs.yml</code> to speed up build reloads.</li> <li>Set <code>validation: unrecognized_links: warn</code> to get warnings about linking issues.</li> </ul> <p>Docstrings should be written in the style of numpy. Additional details on the documentation system are collected in the following subsection.</p>"},{"location":"dev_guide/#linking-to-pages","title":"Linking to pages","text":"<p>You should use relative page links, including the <code>.md</code> extension. For example, <code>[link label](sibling-page.md)</code>.</p> <p>The following works, but does not get validated! <code>[link label](../sibling-page)</code></p> <p>Why not absolute links?</p> <p>The downside of relative links is that if you move/rename source or destination, then they will need to be changed, whereas only the destination needs be watched when using absolute links.</p> <p>Previously, absolute links were not officially supported by MkDocs, meaning \"not modified at all\". Thus, if made like so <code>[label](/DAPPER/references)</code>, i.e. without <code>.md</code> and including <code>/DAPPER</code>, then they would work (locally with <code>mkdocs serve</code> and with GitHub hosting). Since #3485 you can instead use <code>[label](/references)</code> i.e. omitting <code>DAPPER</code> (or whatever domain sub-dir is applied in <code>site_url</code>) by setting <code>mkdocs.yml: validation: absolute_links: relative_to_docs</code>. A different workaround is the <code>mkdocs-site-url</code> plugin.</p> <p>Either way</p> <p>It will not be link that your editor can follow to the relevant markdown file (unless you create a symlink in your file system root?) nor will GitHub's internal markdown rendering manage to make sense of it, so my advise is not to use absolute links.</p>"},{"location":"dev_guide/#linking-to-headersanchors","title":"Linking to headers/anchors","text":"<p>Thanks to the <code>autorefs</code> plugin, links to headings (including page titles) don't even require specifying the page path! Syntax: <code>[visible label][link]</code> i.e. double pairs of brackets. Shorthand: <code>[link][]</code>.</p> <p>Info</p> <ul> <li>Clearly, non-unique headings risk being confused with others in this way.</li> <li>The link (anchor) must be lowercase!</li> </ul> <p>This facilitates linking to</p> <ul> <li>API (code reference) items.   For example, <code>[`da_methods.ensemble`][]</code>,   where the backticks are optional (makes the link look like a code reference).</li> <li>References. For example <code>[`bocquet2016`][]</code>,</li> </ul>"},{"location":"dev_guide/#docstring-injection","title":"Docstring injection","text":"<p>Use the following syntax to inject the docstring of a code object.</p> <pre><code>::: da_methods.ensemble\n</code></pre> <p>But we generally don't do so manually. Instead it's taken care of by the reference generation via <code>docs/gen_ref_pages.py</code>.</p>"},{"location":"dev_guide/#including-other-files","title":"Including other files","text":"<p>The <code>pymdown</code> extension \"snippets\" enables the following syntax to include text from other files.</p> <p><code>--8&lt;-- \"/path/from/project/root/filename.ext\"</code></p>"},{"location":"dev_guide/#adding-to-the-examples","title":"Adding to the examples","text":"<p>Example scripts are very useful, and contributions are very desirable.  As well as showcasing some feature, new examples should make sure to reproduce some published literature results.  After making the example, consider converting the script to the Jupyter notebook format (or vice versa) so that the example can be run on Colab without users needing to install anything (see <code>docs/examples/README.md</code>). This should be done using the <code>jupytext</code> plug-in (with the <code>lightscript</code> format), so that the paired files can be kept in synch.</p>"},{"location":"dev_guide/#bibliography","title":"Bibliography","text":"<p>In order to add new references, insert their bibtex into <code>docs/bib/refs.bib</code>, then run <code>docs/bib/bib2md.py</code> which will format and add entries to <code>docs/references.md</code> that can be cited with regular cross-reference syntax, e.g. <code>[bocquet2010a][]</code>.</p>"},{"location":"dev_guide/#hosting","title":"Hosting","text":"<p>The above command is run by a GitHub Actions workflow whenever the <code>master</code> branch gets updated. The <code>gh-pages</code> branch is no longer being used. Instead actions/deploy-pages creates an artefact that is deployed to Github Pages.</p>"},{"location":"dev_guide/#profiling","title":"Profiling","text":"<ul> <li>Launch your python script using <code>kernprof -l -v my_script.py</code></li> <li>Functions decorated with <code>profile</code> will be timed, line-by-line.</li> <li>If your script is launched regularly, then <code>profile</code> will not be   present in the <code>builtins.</code> Instead of deleting your decorations,   you could also define a pass-through fallback.</li> </ul>"},{"location":"dev_guide/#publishing-a-release-on-pypi","title":"Publishing a release on PyPI","text":"<p><code>cd DAPPER</code></p> <p>Bump version number in <code>__init__.py</code></p> <p>Merge <code>dev1</code> into <code>master</code></p> <pre><code>git checkout master\ngit merge --no-commit --no-ff dev1\n# Fix conflicts, e.g\n# git rm &lt;unwanted-file&gt;\ngit commit\n</code></pre> <p>Tag</p> <pre><code>git tag -a v$(python setup.py --version) -m 'My description'\ngit push origin --tags\n</code></pre> <p>Clean</p> <pre><code>rm -rf build/ dist *.egg-info .eggs\n</code></pre> <p>Add new files to <code>package_data</code> and <code>packages</code> in <code>setup.py</code></p> <p>Build</p> <pre><code>./setup.py sdist bdist_wheel\n</code></pre> <p>Upload to PyPI</p> <pre><code>twine upload --repository pypi dist/*\n</code></pre> <p>Upload to Test.PyPI</p> <p><pre><code>twine upload --repository testpypi dist/*\n</code></pre> where <code>~/.pypirc</code> contains</p> <pre><code>[distutils]\nindex-servers=\n                pypi\n                testpypi\n\n[pypi]\nusername: myuser\npassword: mypass\n\n[testpypi]\nrepository: https://test.pypi.org/legacy/\nusername: myuser\npassword: mypass\n</code></pre> <p>Upload to <code>Test.PyPI</code></p> <pre><code>git checkout dev1\n</code></pre>"},{"location":"dev_guide/#test-installation","title":"Test installation","text":"<p>Install from <code>Test.PyPI</code></p> <pre><code>pip install --extra-index-url https://test.pypi.org/simple/ dapper\n</code></pre> <p>Install from <code>PyPI</code></p> <pre><code>pip install dapper\n</code></pre> <ul> <li> <p>Install into specific dir (includes all of the dependencies) <code>pip install dapper -t MyDir</code></p> </li> <li> <p>Install with options <code>pip install dapper[dev,Qt]</code></p> </li> </ul> <p>Install from local (makes installation accessible from everywhere)</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"references/","title":"Bibliography","text":""},{"location":"references/#raanes2016thesis","title":"<code>raanes2016thesis</code>","text":"<p>Patrick N. Raanes. Improvements to Ensemble Methods for Data Assimilation in the Geosciences. PhD thesis, University of Oxford, January 2016. \\url  https://ora.ox.ac.uk/objects/uuid:9f9961f0-6906-4147-a8a9-ca9f2d0e4a12.</p>"},{"location":"references/#bocquet2010a","title":"<code>bocquet2010a</code>","text":"<p>Marc Bocquet, Carlos A. Pires, and Lin Wu. <code>\"Beyond Gaussian statistical modeling in geophysical data assimilation\"</code>. Monthly Weather Review, 138(8):2997\u20133023, 2010.</p>"},{"location":"references/#bocquet2015","title":"<code>bocquet2015</code>","text":"<p>Marc Bocquet, Patrick N. Raanes, and Alexis Hannart. <code>\"Expanding the validity of the ensemble Kalman filter without the intrinsic need for inflation\"</code>. Nonlinear Processes in Geophysics, 22(6):645\u2013662, 2015.</p>"},{"location":"references/#sakov2008a","title":"<code>sakov2008a</code>","text":"<p>Pavel Sakov and Peter R. Oke. <code>\"Implications of the form of the ensemble transformation in the ensemble square root filters\"</code>. Monthly Weather Review, 136(3):1042\u20131053, 2008.</p>"},{"location":"references/#wiljes2016","title":"<code>wiljes2016</code>","text":"<p>Jana de Wiljes, Walter Acevedo, and Sebastian Reich. <code>\"Second-order accurate ensemble transform particle filters\"</code>. arXiv preprint arXiv:1608.08179, 2016.</p>"},{"location":"references/#bocquet2011","title":"<code>bocquet2011</code>","text":"<p>Marc Bocquet. <code>\"Ensemble Kalman filtering without the intrinsic need for inflation\"</code>. Nonlinear Processes in Geophysics, 18(5):735\u2013750, 2011.</p>"},{"location":"references/#hoteit2015a","title":"<code>hoteit2015a</code>","text":"<p>I. Hoteit, D.-T. Pham, M. E. Gharamti, and X. Luo. <code>\"Mitigating observation perturbation sampling errors in the stochastic EnKF\"</code>. Monthly Weather Review, 143(7):2918\u20132936, 2015.</p>"},{"location":"references/#bocquet2013","title":"<code>bocquet2013</code>","text":"<p>Marc Bocquet and Pavel Sakov. <code>\"Joint state and parameter estimation with an iterative ensemble Kalman smoother\"</code>. Nonlinear Processes in Geophysics, 20(5):803\u2013818, 2013.</p>"},{"location":"references/#hunt2007","title":"<code>hunt2007</code>","text":"<p>Brian R. Hunt, Eric J. Kostelich, and Istvan Szunyogh. <code>\"Efficient data assimilation for spatiotemporal chaos: a local ensemble transform Kalman filter\"</code>. Physica D: Nonlinear Phenomena, 230(1):112\u2013126, 2007.</p>"},{"location":"references/#karspeck2007","title":"<code>karspeck2007</code>","text":"<p>Alicia R. Karspeck and Jeffrey L. Anderson. <code>\"Experimental implementation of an ensemble adjustment filter for an intermediate ENSO model\"</code>. Journal of Climate, 20(18):4638\u20134658, 2007.</p>"},{"location":"references/#todter2015a","title":"<code>todter2015a</code>","text":"<p>Julian T\u00f6dter and Bodo Ahrens. <code>\"A second-order exact ensemble square root filter for nonlinear data assimilation\"</code>. Monthly Weather Review, 143(4):1347\u20131367, 2015.</p>"},{"location":"references/#chen2003","title":"<code>chen2003</code>","text":"<p>Zhe Chen. <code>\"Bayesian filtering: from Kalman filters to particle filters, and beyond\"</code>. Statistics, 182(1):1\u201369, 2003.</p>"},{"location":"references/#bocquet2012a","title":"<code>bocquet2012a</code>","text":"<p>Marc Bocquet and Pavel Sakov. <code>\"Combining inflation-free and iterative ensemble Kalman filters for strongly nonlinear systems\"</code>. Nonlinear Processes in Geophysics, 19(3):383\u2013399, 2012.</p>"},{"location":"references/#doucet2001sequential","title":"<code>doucet2001sequential</code>","text":"<p>Arnaud Doucet, Nando De Freitas, and Neil Gordon. Sequential Monte Carlo Methods in Practice. Springer, 2001.</p>"},{"location":"references/#liu2001theoretical","title":"<code>liu2001theoretical</code>","text":"<p>Jun S. Liu, Rong Chen, and Tanya Logvinenko. <code>\"A theoretical framework for sequential importance sampling with resampling\"</code>. In Sequential Monte Carlo methods in practice, pages 225\u2013246. Springer, 2001.</p>"},{"location":"references/#bocquet2014","title":"<code>bocquet2014</code>","text":"<p>Marc Bocquet and Pavel Sakov. <code>\"An iterative ensemble Kalman smoother\"</code>. Quarterly Journal of the Royal Meteorological Society, 140(682):1521\u20131535, 2014.</p>"},{"location":"references/#sakov2008b","title":"<code>sakov2008b</code>","text":"<p>Pavel Sakov and Peter R. Oke. <code>\"A deterministic formulation of the ensemble Kalman filter: an alternative to ensemble square root filters\"</code>. Tellus A, 60(2):361\u2013371, 2008.</p>"},{"location":"references/#zupanski2005","title":"<code>zupanski2005</code>","text":"<p>Milija Zupanski. <code>\"Maximum likelihood ensemble filter: theoretical aspects\"</code>. Monthly Weather Review, 133(6):1710\u20131726, 2005.</p>"},{"location":"references/#sakov2012a","title":"<code>sakov2012a</code>","text":"<p>Pavel Sakov, Dean S. Oliver, and Laurent Bertino. <code>\"An iterative EnKF for strongly nonlinear systems\\.\"</code>. Monthly Weather Review, 140(6):1988\u20132004, 2012.</p>"},{"location":"references/#evensen2009a","title":"<code>evensen2009a</code>","text":"<p>G. Evensen. <code>\"The ensemble Kalman filter for combined state and parameter estimation\"</code>. Control Systems, IEEE, 29(3):83\u2013104, 2009.</p>"},{"location":"references/#raanes2015a","title":"<code>raanes2015a</code>","text":"<p>Patrick Nima Raanes. <code>\"On the ensemble Rauch-Tung-Striebel smoother and its equivalence to the ensemble Kalman smoother\"</code>. Quarterly Journal of the Royal Meteorological Society, 142(696):1259\u20131264, 2016.</p>"},{"location":"references/#anderson2010","title":"<code>anderson2010</code>","text":"<p>Jeffrey L. Anderson. <code>\"A non-Gaussian ensemble filter update for data assimilation\\.\"</code>. Monthly Weather Review, 138(11):4186\u20134198, 2010.</p>"},{"location":"references/#raanes2019","title":"<code>raanes2019</code>","text":"<p>Patrick Nima Raanes, Andreas St\u00f8rksen Stordal, and Geir Evensen. <code>\"Revising the stochastic iterative ensemble smoother\"</code>. Nonlinear Processes in Geophysics, 26(3):325\u2013338, 2019.</p>"},{"location":"references/#van2009","title":"<code>van2009</code>","text":"<p>Peter Jan van Leeuwen. <code>\"Particle filtering in geophysical systems\"</code>. Monthly Weather Review, 137(12):4089\u20134114, 2009.</p>"},{"location":"references/#raanes2019a","title":"<code>raanes2019a</code>","text":"<p>Patrick N. Raanes, Marc Bocquet, and Alberto Carrassi. <code>\"Adaptive covariance inflation in the ensemble Kalman filter by Gaussian scale mixtures\"</code>. Quarterly Journal of the Royal Meteorological Society, 145(718):53\u201375, 2019. doi:10.1002/qj.3386.</p>"},{"location":"references/#lei2011","title":"<code>lei2011</code>","text":"<p>Jing Lei and Peter Bickel. <code>\"A moment matching ensemble filter for nonlinear non-Gaussian data assimilation\"</code>. Monthly Weather Review, 139(12):3964\u20133973, 2011.</p>"},{"location":"references/#wikle2007","title":"<code>wikle2007</code>","text":"<p>C. K. Wikle and L. M. Berliner. <code>\"A Bayesian tutorial for data assimilation\"</code>. Physica D: Nonlinear Phenomena, 230(1-2):1\u201316, 2007.</p>"},{"location":"references/#doucet2009","title":"<code>doucet2009</code>","text":"<p>Arnaud Doucet and Adam M. Johansen. <code>\"A tutorial on particle filtering and smoothing: fifteen years later\"</code>. Handbook of Nonlinear Filtering, pages 656\u2013704, 2009.</p>"},{"location":"references/#bocquet2016","title":"<code>bocquet2016</code>","text":"<p>Marc Bocquet. <code>\"Localization and the iterative ensemble Kalman smoother\"</code>. Quarterly Journal of the Royal Meteorological Society, 142(695):1075\u20131089, 2016.</p>"},{"location":"references/#raanes2014","title":"<code>raanes2014</code>","text":"<p>Patrick N. Raanes, Alberto Carrassi, and Laurent Bertino. <code>\"Extending the square root method to account for model noise in the ensemble Kalman filter\"</code>. Monthly Weather Review, 143(10):3857\u20133873, 2015.</p>"},{"location":"references/#lorenz1998","title":"<code>lorenz1998</code>","text":"<p>Edward N. Lorenz and Kerry A. Emanuel. <code>\"Optimal sites for supplementary weather observations: simulation with a small model\"</code>. Journal of the Atmospheric Sciences, 55(3):399\u2013414, 1998.</p>"},{"location":"references/#frei2013","title":"<code>frei2013</code>","text":"<p>Marco Frei and Hans R K\u00fcnsch. <code>\"Mixture ensemble kalman filters\"</code>. Computational Statistics &amp; Data Analysis, 58:127\u2013138, 2013.</p>"},{"location":"references/#frei2013a","title":"<code>frei2013a</code>","text":"<p>Marco Frei and Hans R K\u00fcnsch. <code>\"Bridging the ensemble Kalman and particle filters\"</code>. Biometrika, pages ast020, 2013.</p>"},{"location":"references/#miyoshi2011a","title":"<code>miyoshi2011a</code>","text":"<p>Takemasa Miyoshi. <code>\"The Gaussian approach to adaptive covariance inflation and its implementation with the local ensemble transform Kalman filter\"</code>. Monthly Weather Review, 139(5):1519\u20131535, 2011.</p>"},{"location":"references/#mandel2016hybrid","title":"<code>mandel2016hybrid</code>","text":"<p>J. Mandel, E. Bergou, S. G\u00fcrol, S. Gratton, and I. Kasanick\u00fd. <code>\"Hybrid Levenberg-Marquardt and weak-constraint ensemble Kalman smoother method\"</code>. Nonlinear Processes in Geophysics, 23(2):59\u201373, 2016.</p>"},{"location":"references/#anderson2009a","title":"<code>anderson2009a</code>","text":"<p>Jeffrey L. Anderson. <code>\"Spatially and temporally varying adaptive covariance inflation for ensemble filters\"</code>. Tellus A, 61(1):72\u201383, 2009.</p>"},{"location":"references/#bengtsson2003","title":"<code>bengtsson2003</code>","text":"<p>Thomas Bengtsson, Chris Snyder, and Doug Nychka. <code>\"Toward a nonlinear ensemble filter for high-dimensional systems\"</code>. Journal of Geophysical Research: Atmospheres, 2003.</p>"},{"location":"references/#lorenz1996predictability","title":"<code>lorenz1996predictability</code>","text":"<p>Edward N. Lorenz. <code>\"Predictability: a problem partly solved\"</code>. In Proc. ECMWF Seminar on Predictability, volume 1, 1\u201318. Reading, UK, 1996.</p>"},{"location":"references/#wilks2005","title":"<code>wilks2005</code>","text":"<p>Daniel S. Wilks. <code>\"Effects of stochastic parametrizations in the Lorenz'96 system\"</code>. Quarterly Journal of the Royal Meteorological Society, 131(606):389\u2013407, 2005.</p>"},{"location":"references/#harty2021","title":"<code>harty2021</code>","text":"<p>Travis Harty, Matthias Morzfeld, and Chris Snyder. <code>\"Eigenvector-spatial localisation\"</code>. Tellus A: Dynamic Meteorology and Oceanography, 73(1):1\u201318, 2021.</p>"},{"location":"references/#rainwater2013","title":"<code>rainwater2013</code>","text":"<p>Sabrina Rainwater and Brian Hunt. <code>\"Mixed-resolution ensemble data assimilation\"</code>. Monthly weather review, 141(9):3007\u20133021, 2013.</p>"},{"location":"references/#counillon2009a","title":"<code>counillon2009a</code>","text":"<p>Fran\u00e7ois Counillon, Pavel Sakov, and Laurent Bertino. <code>\"Application of a hybrid \\</code>enkf-oi` to ocean forecasting\"`. Ocean Science, 5(4):389\u2013401, 2009.</p>"},{"location":"references/#pajonk2012a","title":"<code>pajonk2012a</code>","text":"<p>Oliver Pajonk, Bojana V. Rosi\u0107, Alexander Litvinenko, and Hermann G. Matthies. <code>\"A deterministic filter for non-Gaussian Bayesian estimation\u2014applications to dynamical system estimation with noisy measurements\"</code>. Physica D: Nonlinear Phenomena, 241(7):775\u2013788, 2012.</p>"},{"location":"references/#lorenz2005","title":"<code>lorenz2005</code>","text":"<p>Edward N. Lorenz. <code>\"A look at some details of the growth of initial uncertainties\"</code>. Tellus A: Dynamic Meteorology and Oceanography, 57(1):1\u201311, 2005.</p>"},{"location":"references/#bocquet2019a","title":"<code>bocquet2019a</code>","text":"<p>Marc Bocquet and Alban Farchi. <code>\"On the consistency of the local ensemble square root kalman filter perturbation update\"</code>. Tellus A: Dynamic Meteorology and Oceanography, 71(1):1613142, 2019.</p>"},{"location":"references/#lorenz1984","title":"<code>lorenz1984</code>","text":"<p>Edward N. Lorenz. <code>\"Irregularity: a fundamental property of the atmosphere\"</code>. Tellus A, 36(2):98\u2013110, 1984.</p>"},{"location":"references/#vano2006","title":"<code>vano2006</code>","text":"<p>J. A. Vano, J. C. Wildenberg, M. B. Anderson, J. K. Noel, and J. C. Sprott. <code>\"Chaos in low-dimensional Lotka\u2013Volterra models of competition\"</code>. Nonlinearity, 19(10):2391, 2006.</p>"},{"location":"references/#emerick2012","title":"<code>emerick2012</code>","text":"<p>Alexandre A. Emerick and Albert C. Reynolds. <code>\"History matching time-lapse seismic data using the ensemble Kalman filter with multiple data assimilations\"</code>. Computational Geosciences, 16(3):639\u2013659, 2012.</p>"},{"location":"references/#pinheiro2019a","title":"<code>pinheiro2019a</code>","text":"<p>Flavia R. Pinheiro, Peter J. van Leeuwen, and Gernot Geppert. <code>\"Efficient nonlinear data assimilation using synchronization in a particle filter\"</code>. Quarterly Journal of the Royal Meteorological Society, 145(723):2510\u20132523, 2019.</p>"},{"location":"references/#vissio2020","title":"<code>vissio2020</code>","text":"<p>Gabriele Vissio and Valerio Lucarini. <code>\"Mechanics and thermodynamics of a new minimal model of the atmosphere\"</code>. The European Physical Journal Plus, 135(10):1\u201321, 2020.</p>"},{"location":"references/#grudzien2020a","title":"<code>grudzien2020a</code>","text":"<p>Colin Grudzien, Marc Bocquet, and Alberto Carrassi. <code>\"On the numerical integration of the lorenz-96 model, with scalar additive noise, for benchmark twin experiments\"</code>. Geoscientific Model Development, 13(4):1903\u20131924, 2020.</p>"},{"location":"references/#lorenz2005a","title":"<code>lorenz2005a</code>","text":"<p>Edward. N. Lorenz. <code>\"Designing chaotic models\"</code>. Journal of the Atmospheric Sciences, 62(5):1574\u20131587, 2005.</p>"},{"location":"examples/","title":"Examples","text":"<p>Here are some example scripts using DAPPER. They all consist of one (or more) synthetic experiments.</p> <p>Run them using <code>python docs/examples/the_script.py</code>, or with the <code>%run</code> command inside <code>ipython</code>.</p> <p>Some of the scripts have also been converted to Jupyter notebooks (<code>.ipynb</code>). You can try them out without installing anything by pressing this button (but note that some plotting features won't work, and that it requires a Google login): </p>"},{"location":"examples/#description","title":"Description","text":"<p>When adapting the scripts to your needs, you should begin with <code>basic_1.py</code> before incorporating the aspects of <code>basic_2</code> and <code>basic_3</code>.</p> <ul> <li><code>basic_1.py</code>: A single experiment, with Liveplotting.</li> <li><code>basic_2.py</code>: Comparison of several DA methods.</li> <li><code>basic_3.py</code>: Comparison of many DA methods and other experiment settings.</li> <li><code>time-dep-obs-operator.py</code>: Similar to <code>basic_1</code>, but with \"double\" Lorenz-63 systems   evolving independently, and observations of each \"half\" at alternating times.</li> <li><code>param_estim.py</code>: Similar to <code>basic_2</code>, but with parameter estimation.</li> <li><code>stoch_model1.py</code>: A perfect-yet-random model, with various integration schemes.</li> <li><code>stoch_models.py</code>: As above, but studies the relationship between   model uncertainty and numerical discretization error.</li> </ul>"},{"location":"examples/basic_1/","title":"Basic 1","text":"In\u00a0[1]: Copied! <pre>%matplotlib notebook\n</pre> %matplotlib notebook In\u00a0[2]: Copied! <pre>import dapper as dpr\nimport dapper.da_methods as da\n</pre> import dapper as dpr import dapper.da_methods as da <p>Generate the same random numbers each time this script is run:</p> In\u00a0[3]: Copied! <pre>dpr.set_seed(3000);\n</pre> dpr.set_seed(3000); In\u00a0[4]: Copied! <pre>from dapper.mods.Lorenz63.sakov2012 import HMM\nHMM  # \u21d2 printout (in notebooks)\n</pre> from dapper.mods.Lorenz63.sakov2012 import HMM HMM  # \u21d2 printout (in notebooks) Out[4]: <pre>HiddenMarkovModel({\n     'Dyn': \n         Operator({\n              'M': 3,\n              'model':\n                  rk4 integration of &lt;function dapper.mods.Lorenz63.dxdt(x)&gt;\n                  &lt;function with_rk4.&lt;locals&gt;.step at 0x119028720&gt;,\n              'noise':\n                  GaussRV({\n                           'mu': array([0., 0., 0.]),\n                            'M': 3,\n                            'C': 0\n                          }),\n              'linear': &lt;function dstep_dx at 0x119028540&gt;\n             }),\n     'Obs': \n         &lt;TimeDependentOperator&gt; CONSTANT operator sepcified by .Op1:\n         Operator({\n              'M': 3,\n              'model':\n                  Direct obs. at [0 1 2]\n                  &lt;function partial_Id_Obs.&lt;locals&gt;.model at 0x119028c20&gt;,\n              'noise':\n                  GaussRV({\n                            'C': &lt;CovMat&gt;\n                                       M: 3\n                                    kind: 'diag'\n                                   trunc: 1.0\n                                      rk: 3\n                                    full:\n                                      [[2. 0. 0.]\n                                       [0. 2. 0.]\n                                       [0. 0. 2.]]\n                                    diag:\n                                       [2. 2. 2.],\n                           'mu': array([0., 0., 0.]),\n                            'M': 3\n                          }),\n              'linear':\n                  Constant matrix\n                  [[1. 0. 0.]\n                   [0. 1. 0.]\n                   [0. 0. 1.]]\n                  &lt;function partial_Id_Obs.&lt;locals&gt;.linear at 0x119028e00&gt;\n             }),\n     'tseq': \n         &lt;Chronology&gt;\n           -      K: 25025\n           -     Ko: 1000\n           -      T: 250.25\n           - BurnIn: 16.0\n           -    dto: 0.25\n           -     dt: 0.01,\n     'X0': \n         GaussRV({\n                   'C': &lt;CovMat&gt;\n                              M: 3\n                           kind: 'diag'\n                          trunc: 1.0\n                             rk: 3\n                           full:\n                             [[2. 0. 0.]\n                              [0. 2. 0.]\n                              [0. 0. 2.]]\n                           diag:\n                              [2. 2. 2.],\n                  'mu': array([ 1.509, -1.531, 25.46 ]),\n                   'M': 3\n                 }),\n     'liveplotters': \n         [(1, &lt;class 'dapper.tools.liveplotting.correlations'&gt;),\n          (1, &lt;function sliding_marginals.&lt;locals&gt;.init at\n          0x11903c720&gt;), (1, &lt;function phase_particles.&lt;locals&gt;.init\n          at 0x11903c5e0&gt;)],\n     'sectors': {},\n     'name': 'Lorenz63/sakov2012.py'\n    })</pre> In\u00a0[5]: Copied! <pre>HMM.tseq.T = 30  # shorten experiment\nxx, yy = HMM.simulate()\n</pre> HMM.tseq.T = 30  # shorten experiment xx, yy = HMM.simulate() <pre>Truth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3000/3000 [00:00&lt;00:00, 42197.63it/s]\n</pre> In\u00a0[6]: Copied! <pre># xp = da.OptInterp()\n# xp = da.Var3D()\n# xp = da.ExtKF(infl=90)\nxp = da.EnKF(\"Sqrt\", N=10, infl=1.02, rot=True)\n# xp = da.PartFilt(N=100, reg=2.4, NER=0.3)\nxp  # \u21d2 printout (in notebooks)\n</pre> # xp = da.OptInterp() # xp = da.Var3D() # xp = da.ExtKF(infl=90) xp = da.EnKF(\"Sqrt\", N=10, infl=1.02, rot=True) # xp = da.PartFilt(N=100, reg=2.4, NER=0.3) xp  # \u21d2 printout (in notebooks) Out[6]: <pre>EnKF(upd_a='Sqrt', N=10, infl=1.02, rot=True, fnoise_treatm='Stoch')</pre> In\u00a0[7]: Copied! <pre>xp.assimilate(HMM, xx, yy)\n</pre> xp.assimilate(HMM, xx, yy) <pre>EnKF: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3000/3000 [00:00&lt;00:00, 14291.97it/s]\n</pre> In\u00a0[8]: Copied! <pre># print(xp.stats)  # \u21d2 long printout\nxp.stats.average_in_time()\n</pre> # print(xp.stats)  # \u21d2 long printout xp.stats.average_in_time() <p>Print some of these time-averages</p> In\u00a0[9]: Copied! <pre># print(xp.avrgs)  # \u21d2 long printout\nprint(xp.avrgs.tabulate([\"rmse.a\", \"rmv.a\"]))\n</pre> # print(xp.avrgs)  # \u21d2 long printout print(xp.avrgs.tabulate([\"rmse.a\", \"rmv.a\"])) <pre>rmse.a  1\u03c3    rmv.a  1\u03c3  \n------------  -----------\n  0.78 \u00b10.06   0.57 \u00b10.07\n</pre> In\u00a0[10]: Copied! <pre>xp.stats.replay(\n    # speed=.6  # `speed` does not work in notebooks\n)\n</pre> xp.stats.replay(     # speed=.6  # `speed` does not work in notebooks ) <pre>Initializing liveplots...\n</pre> <pre>EnKF (replay): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3000/3000 [00:00&lt;00:00, 15191.34it/s]\n</pre> In\u00a0[11]: Copied! <pre>import dapper.tools.viz as viz\n</pre> import dapper.tools.viz as viz In\u00a0[12]: Copied! <pre>viz.plot_rank_histogram(xp.stats)\nviz.plot_err_components(xp.stats)\nviz.plot_hovmoller(xx)\n</pre> viz.plot_rank_histogram(xp.stats) viz.plot_err_components(xp.stats) viz.plot_hovmoller(xx)"},{"location":"examples/basic_1/#a-single-interactive-synthetic-twin-experiment","title":"A single, interactive, synthetic (\"twin\") experiment\u00b6","text":"<p>If run as a script in a terminal (i.e. not as a jupyter notebook) then the liveplotting is interactive (can be paused, skipped, etc).</p>"},{"location":"examples/basic_1/#imports","title":"Imports\u00b6","text":"<p>NB: On Gooble Colab, replace <code>%matplotlib notebook</code> (right below) by <code>!python -m pip install git+https://github.com/nansencenter/DAPPER.git</code></p>"},{"location":"examples/basic_1/#load-experiment-setup-the-hidden-markov-model-hmm","title":"Load experiment setup: the hidden Markov model (HMM)\u00b6","text":""},{"location":"examples/basic_1/#simulate-synthetic-truth-xx-and-noisy-obs-yy","title":"Simulate synthetic truth (xx) and noisy obs (yy)\u00b6","text":"<p>A variable named <code>&lt;char&gt;&lt;char&gt;</code> conventionally refers to a time series of <code>&lt;char&gt;</code>.</p>"},{"location":"examples/basic_1/#specify-a-da-method","title":"Specify a DA method\u00b6","text":"<p>Here \"xp\" is short for \"experiment\" configuration.</p>"},{"location":"examples/basic_1/#assimilate-yy","title":"Assimilate yy\u00b6","text":"<p>Note that the assimilation \"knows\" the HMM, but <code>xx</code> is only used for performance assessment.</p>"},{"location":"examples/basic_1/#average-the-time-series","title":"Average the time series\u00b6","text":"<p>Computes a set of different statistics.</p>"},{"location":"examples/basic_1/#replay-liveplotters","title":"Replay liveplotters\u00b6","text":""},{"location":"examples/basic_1/#further-diagnostic-plots","title":"Further diagnostic plots\u00b6","text":""},{"location":"examples/basic_1/#excercise-methods","title":"Excercise (methods)\u00b6","text":"<ul> <li>Try out each of the above DA methods (currently commented out).</li> <li>Next, remove the call to <code>replay</code>, and set <code>liveplots=False</code> above.</li> <li>Now, use the iterative EnKS (<code>iEnKS</code>), and try to find a parameter combination for it so that you achieve a lower <code>rmse.a</code> than with the <code>PartFilt</code>.</li> </ul> <p>Hint: In general, there is no free lunch. Similarly, not all methods work for all problems; additionally, methods often have parameters that require tuning. Luckily, in DAPPER, you should be able to find suitably tuned configuration settings for various DA methods in the files that define the HMM. If you do not find a suggested configuration for a given method, you will have to tune it yourself. The example script <code>basic_2</code> shows how DAPPER facilitates the tuning process, and <code>basic_3</code> takes this further.</p>"},{"location":"examples/basic_1/#excercise-models","title":"Excercise (models)\u00b6","text":"<p>Run an experiment for each of these models</p> <ul> <li>LotkaVolterra</li> <li>Lorenz96</li> <li>LA</li> <li>QG</li> </ul>"},{"location":"examples/basic_1/#excercise-diagnostics","title":"Excercise (diagnostics)\u00b6","text":"<ul> <li>Create a new code cell, and copy-paste the above <code>print(...tabulate)</code> command into it. Then, replace <code>rmse</code> by <code>err.rms</code>. This should yield the same printout, as is merely an abbreviation of the latter.</li> <li>Next, figure out how to print the time average forecast (i.e. prior) error (and <code>rmv</code>) instead. Explain (in broad terms) why the values are larger than for the analysis values.</li> <li>Finally, instead of the <code>rms</code> spatial/field averages, print the regular mean (<code>.m</code>) averages. Explain why <code>err.m</code> is nearly zero, in contrast to <code>err.rms</code>.</li> </ul>"},{"location":"examples/basic_1/#excercise-memory","title":"Excercise (memory)\u00b6","text":"<p>Why are the replay plots not as smooth as the liveplot?</p> <p>Hint: provide the keyword <code>store_u=True</code> to <code>assimilate()</code> to avoid this.</p>"},{"location":"examples/basic_2/","title":"Basic 2","text":"In\u00a0[1]: Copied! <pre>%matplotlib notebook\nimport dapper as dpr  # noqa: I001\nimport dapper.da_methods as da\n</pre> %matplotlib notebook import dapper as dpr  # noqa: I001 import dapper.da_methods as da In\u00a0[2]: Copied! <pre>from dapper.mods.Lorenz63.sakov2012 import HMM  # Expected rmse.a:\n\nxps = dpr.xpList()\nxps += da.Climatology()                                      # 7.6\nxps += da.OptInterp()                                        # 1.25\n#xps += da.Persistence()                                      # 10.7\n#xps += da.PreProg(lambda k, xx, yy: xx[k])                   # 0\nxps += da.Var3D(xB=0.1)                                      # 1.03\nxps += da.ExtKF(infl=90)                                     # 0.87\nxps += da.EnKF('Sqrt'   , N=3   , infl=1.30)                 # 0.82\nxps += da.EnKF('Sqrt'   , N=10  , infl=1.02, rot=True)       # 0.63\nxps += da.EnKF('PertObs', N=500 , infl=0.95, rot=False)      # 0.56\nxps += da.EnKF_N(         N=10             , rot=True)       # 0.54\nxps += da.iEnKS('Sqrt'  , N=10  , infl=1.02, rot=True)       # 0.31\nxps += da.PartFilt(       N=100 , reg=2.4  , NER=0.3)        # 0.38\nxps += da.PartFilt(       N=800 , reg=0.9  , NER=0.2)        # 0.28\n# xps += da.PartFilt(       N=4000, reg=0.7  , NER=0.05)       # 0.27\n# xps += da.PFxN(xN=1000,   N=30  , Qs=2     , NER=0.2)        # 0.56\n</pre> from dapper.mods.Lorenz63.sakov2012 import HMM  # Expected rmse.a:  xps = dpr.xpList() xps += da.Climatology()                                      # 7.6 xps += da.OptInterp()                                        # 1.25 #xps += da.Persistence()                                      # 10.7 #xps += da.PreProg(lambda k, xx, yy: xx[k])                   # 0 xps += da.Var3D(xB=0.1)                                      # 1.03 xps += da.ExtKF(infl=90)                                     # 0.87 xps += da.EnKF('Sqrt'   , N=3   , infl=1.30)                 # 0.82 xps += da.EnKF('Sqrt'   , N=10  , infl=1.02, rot=True)       # 0.63 xps += da.EnKF('PertObs', N=500 , infl=0.95, rot=False)      # 0.56 xps += da.EnKF_N(         N=10             , rot=True)       # 0.54 xps += da.iEnKS('Sqrt'  , N=10  , infl=1.02, rot=True)       # 0.31 xps += da.PartFilt(       N=100 , reg=2.4  , NER=0.3)        # 0.38 xps += da.PartFilt(       N=800 , reg=0.9  , NER=0.2)        # 0.28 # xps += da.PartFilt(       N=4000, reg=0.7  , NER=0.05)       # 0.27 # xps += da.PFxN(xN=1000,   N=30  , Qs=2     , NER=0.2)        # 0.56 In\u00a0[3]: Copied! <pre># from dapper.mods.Lorenz96.sakov2008 import HMM   # Expected rmse.a:\n# xps = dpr.xpList()\n# xps += da.Climatology()                                     # 3.6\n# xps += da.OptInterp()                                       # 0.95\n# xps += da.Var3D(xB=0.02)                                    # 0.41\n# xps += da.ExtKF(infl=6)                                     # 0.24\n# xps += da.EnKF('PertObs', N=40, infl=1.06)                  # 0.22\n# xps += da.EnKF('Sqrt'   , N=28, infl=1.02, rot=True)        # 0.18\n# # More sophisticated:\n# xps += da.EnKF_N(         N=24, rot=True)                   # 0.21\n# xps += da.EnKF_N(         N=24, rot=True, xN=2)             # 0.18\n# xps += da.iEnKS('Sqrt'  , N=40, infl=1.01, rot=True)        # 0.17\n# # With localisation:\n# xps += da.LETKF(          N=7 , infl=1.04, rot=True, loc_rad=4)  # 0.22\n# xps += da.SL_EAKF(        N=7 , infl=1.07, rot=True, loc_rad=6)  # 0.23\n</pre> # from dapper.mods.Lorenz96.sakov2008 import HMM   # Expected rmse.a: # xps = dpr.xpList() # xps += da.Climatology()                                     # 3.6 # xps += da.OptInterp()                                       # 0.95 # xps += da.Var3D(xB=0.02)                                    # 0.41 # xps += da.ExtKF(infl=6)                                     # 0.24 # xps += da.EnKF('PertObs', N=40, infl=1.06)                  # 0.22 # xps += da.EnKF('Sqrt'   , N=28, infl=1.02, rot=True)        # 0.18 # # More sophisticated: # xps += da.EnKF_N(         N=24, rot=True)                   # 0.21 # xps += da.EnKF_N(         N=24, rot=True, xN=2)             # 0.18 # xps += da.iEnKS('Sqrt'  , N=40, infl=1.01, rot=True)        # 0.17 # # With localisation: # xps += da.LETKF(          N=7 , infl=1.04, rot=True, loc_rad=4)  # 0.22 # xps += da.SL_EAKF(        N=7 , infl=1.07, rot=True, loc_rad=6)  # 0.23 In\u00a0[4]: Copied! <pre># from dapper.mods.LA           .evensen2009 import HMM\n# from dapper.mods.KS           .bocquet2019 import HMM\n# from dapper.mods.LotkaVolterra.settings101 import HMM\n</pre> # from dapper.mods.LA           .evensen2009 import HMM # from dapper.mods.KS           .bocquet2019 import HMM # from dapper.mods.LotkaVolterra.settings101 import HMM <p>Write some more non-arg parameters to the <code>xps</code>. In this case we set the seed, so that repeat experiments produce exactly the same result.</p> In\u00a0[5]: Copied! <pre>for xp in xps:\n    xp.seed = 3000\n</pre> for xp in xps:     xp.seed = 3000 <p>Adjust experiment duration</p> In\u00a0[6]: Copied! <pre>HMM.tseq.T = 50\n</pre> HMM.tseq.T = 50 <p>Run/assimilate (for each <code>xp</code> in <code>xps</code>)</p> In\u00a0[7]: Copied! <pre>save_as = xps.launch(HMM, liveplots=False)\n</pre> save_as = xps.launch(HMM, liveplots=False) <pre>Experiment gets stored at /Users/para/data/dpr_data/noname/run_2024-10-31__17-12-44\n</pre> <pre>Truth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 41765.45it/s]\nClimatology: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 58936.74it/s]\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 42079.80it/s]\nOptInterp: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 25958.39it/s]\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 42096.02it/s]\nVar3D: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 26433.50it/s]\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 38184.81it/s]\nExtKF infl:90: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 19740.78it/s]\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 41851.22it/s]\nEnKF infl:1.3 upd_a:Sqrt N:3 rot:False: 100%|\u2588| 5000/5000 [00:00&lt;00:00\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 40812.37it/s]\nEnKF infl:1.02 upd_a:Sqrt N:10 rot:True: 100%|\u2588| 5000/5000 [00:00&lt;00:0\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 42888.91it/s]\nEnKF infl:0.95 upd_a:PertObs N:500 rot:False: 100%|\u2588| 5000/5000 [00:00\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 40611.08it/s]\nEnKF_N infl:1 N:10 rot:True xN:1: 100%|\u2588| 5000/5000 [00:00&lt;00:00, 1325\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 40498.23it/s]\niEnKS infl:1.02 upd_a:Sqrt N:10 rot:True: 100%|\u2588| 201/201 [00:03&lt;00:00\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 42317.38it/s]\nPartFilt N:100 reg:2.4 NER:0.3: 100%|\u2588| 5000/5000 [00:00&lt;00:00, 13981.\nTruth &amp; Obs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00&lt;00:00, 42299.37it/s]\nPartFilt N:800 reg:0.9 NER:0.2: 100%|\u2588| 5000/5000 [00:00&lt;00:00, 8458.1\n</pre> In\u00a0[8]: Copied! <pre>print(xps.tabulate_avrgs(statkeys=[\"rmse.a\"]))\n</pre> print(xps.tabulate_avrgs(statkeys=[\"rmse.a\"])) <pre>    da_method     infl  upd_a      N  rot    xN  reg  NER  |  rmse.a  1\u03c3  \n--  -----------  -----  -------  ---  -----  --  ---  ---  -  ------------\n 0  Climatology                                            |    7.4  \u00b10.3 \n 1  OptInterp                                              |    1.23 \u00b10.04\n 2  Var3D                                                  |    1.07 \u00b10.06\n 3  ExtKF        90                                        |    0.88 \u00b10.09\n 4  EnKF          1.3   Sqrt       3  False                |    0.8  \u00b10.1 \n 5  EnKF          1.02  Sqrt      10  True                 |    0.59 \u00b10.06\n 6  EnKF          0.95  PertObs  500  False                |    1.1  \u00b10.3 \n 7  EnKF_N        1               10  True    1            |    0.71 \u00b10.07\n 8  iEnKS         1.02  Sqrt      10  True                 |    0.23 \u00b10.02\n 9  PartFilt                     100             2.4  0.3  |    0.32 \u00b10.03\n10  PartFilt                     800             0.9  0.2  |    0.20 \u00b10.03\n</pre>"},{"location":"examples/basic_2/#multiple-synthetic-benchmark-experiments","title":"Multiple synthetic benchmark experiments\u00b6","text":""},{"location":"examples/basic_2/#imports","title":"Imports\u00b6","text":"<p>NB: If you're on Gooble Colab, then replace <code>%matplotlib notebook</code> below by <code>!python -m pip install git+https://github.com/nansencenter/DAPPER.git</code> Also note that liveplotting does not work on Colab.</p>"},{"location":"examples/basic_2/#da-method-configurations","title":"DA method configurations\u00b6","text":""},{"location":"examples/basic_2/#with-lorenz-96-instead","title":"With Lorenz-96 instead\u00b6","text":""},{"location":"examples/basic_2/#other-models-suitable-xps-listed-in-hmm-files","title":"Other models (suitable xp's listed in HMM files):\u00b6","text":""},{"location":"examples/basic_2/#launch","title":"Launch\u00b6","text":""},{"location":"examples/basic_2/#print-results","title":"Print results\u00b6","text":""},{"location":"reference/","title":"dapper","text":"<p>Root package of DAPPER (Data Assimilation with Python: a Package for Experimental Research)</p>"},{"location":"reference/#dapper--adding-your-own-modelmethod","title":"Adding your own model/method","text":"<p>If you wish to illustrate and run benchmarks with your own model or method, then</p> <ul> <li>If it is a complex one, you may be better off using DAPPER   merely as inspiration (but you can still   cite it)   rather than trying to squeeze everything into its templates.</li> <li>If it is relatively simple, however, you may well want to use DAPPER.   In that case, read this:<ul> <li><code>mods</code></li> <li><code>da_methods</code></li> </ul> </li> </ul> <p>Since the generality of DAPPER is limited it is quite likely you will also need to make changes to the DAPPER code itself.</p> <p>Modules:</p> Name Description <code>da_methods</code> <p>Contains the data assimilation methods included with DAPPER.</p> <code>dpr_config</code> <p>Load default and user configurations into the <code>rc</code> dict.</p> <code>mods</code> <p>Contains models included with DAPPER.</p> <code>stats</code> <p>Statistics for the assessment of DA methods.</p> <code>tools</code> <p>If the only tool you have is a hammer, it's hard to eat spaghetti.</p> <code>xp_launch</code> <p>Tools (notably <code>xpList</code>) for setup and running of experiments (known as <code>xp</code>s).</p> <code>xp_process</code> <p>Tools (notably <code>xpSpace</code>) for processing and presenting experiment data.</p>"},{"location":"reference/dpr_config/","title":"dpr_config","text":"<p>Load default and user configurations into the <code>rc</code> dict.</p> <p>The <code>rc</code> dict can be updated (after startup) as any normal dict.</p> <p>The config file should reside in your <code>$HOME/</code>, or <code>$HOME/.config/</code> or <code>$PWD/</code>. If several exist, the last one found (from the above ordering) is used. The default configuration is given below.</p> <pre><code># Where to store the experimental settings and results.\n# For example, you don't want this to be in your Dropbox.\n# Use \"$cwd\" for PWD, \"$dapper\" for where the DAPPER dir is.\ndata_root: \"~\"\n\n# Methods used to average multivariate (\"field\") stats\n# PS: If the model is computationally trivial, the stats\n# computations take up most time. Therefore, not all of\n# these are activated by default.\nfield_summaries:\n  - m    # plain mean\n  #- ms   # mean-square\n  - rms  # root-mean-square\n  - ma   # mean-absolute\n  #- gm   # geometric mean\n\n# Curtail heavy computations\ncomps:\n  error_only: False\n  max_spectral: 51\n\nsigfig: 4             # Default significant figures\nstore_u: no           # Store stats between analysis times?\nliveplotting: yes     # Enable liveplotting?\nplace_figs: False     # Place (certain) figures automatically (experimental)?\n</code></pre>"},{"location":"reference/stats/","title":"stats","text":"<p>Statistics for the assessment of DA methods.</p> <p><code>Stats</code> is a data container for ([mostly] time series of) statistics. It comes with a battery of methods to compute the default statistics.</p> <p><code>Avrgs</code> is a data container for the same statistics, but after they have been averaged in time (after the assimilation has finished).</p> <p>Instances of these objects are created by <code>da_methods.da_method</code> (i.e. \"<code>xp</code>\") objects and written to their <code>.stats</code> and <code>.avrgs</code> attributes.</p>"},{"location":"reference/stats/#stats--default-statistics","title":"Default statistics","text":"<p>List them using</p> <pre><code>list(vars(xp.stats))\nlist(vars(xp.avrgs))\n</code></pre>"},{"location":"reference/stats/#stats--the-faust-keyattribute","title":"The <code>FAUSt</code> key/attribute","text":"<p>The time series of statistics (the attributes of <code>.stats</code>) may have attributes <code>.f</code>, <code>.a</code>, <code>.s</code>, <code>.u</code>, referring to whether the statistic is for a \"forecast\", \"analysis\", or \"smoothing\" estimate (as is decided when the calls to Stats.assess is made), or a \"universal\" (forecast, but at intermediate [non-obs.-time]) estimate.</p> <p>The same applies for the time-averages of <code>.avrgs</code>.</p>"},{"location":"reference/stats/#stats--field-summaries","title":"Field summaries","text":"<p>The statistics are also averaged in space. This is done according to the methods listed in <code>rc.field_summaries</code> of the <code>dpr_config</code>.</p> <p>Note</p> <p>Although sometimes pretty close, <code>rmv</code> (a.k.a. <code>spread.rms</code>) is not (supposed to be) an un-biased estimator of <code>rmse</code> (a.k.a. <code>err.rms</code>).  This is because of the square roots involved in the field summary.  Instead, <code>spread.ms</code> (i.e. the mean variance) is the unbiased estimator of <code>err.ms</code>.</p>"},{"location":"reference/stats/#stats--regional-field-summaries","title":"Regional field summaries","text":"<p>If the <code>HiddenMarkovModel</code> has the attribute <code>.sectors</code> with value, e.g.,</p> <pre><code>HMM.sectors = {\n    \"ocean\": inds_of_state_of_the_ocean,\n    \"atmos\": inds_of_state_of_the_atmosphere,\n}\n</code></pre> <p>then <code>.stats.rms</code> and <code>.avrgs.rms</code> will also have attributes named after the keys of <code>HMM.sectors</code>, e.g. <code>stats.err.rms.ocean</code>. This also goes for any other (than <code>rms</code>) type of field summary method.</p>"},{"location":"reference/stats/#stats--declaring-new-custom-statistics","title":"Declaring new, custom statistics","text":"<p>Only the time series created with Stats.new_series will be in the format operated on by Stats.average_in_time.  For example, create <code>ndarray</code> of length <code>Ko+1</code> to hold the time series of estimated inflation values:</p> <pre><code>self.stats.new_series('infl', 1, Ko+1)\n</code></pre> <p>Alternatively you can overwrite a default statistic; for example:</p> <pre><code>error_time_series_a = xx - ensemble_time_series_a.mean(axis=1)\nself.stats.err.rms.a = np.sqrt(np.mean(error_time_series_a**2, axis=-1))\n</code></pre> <p>Of course, you could just do this</p> <pre><code>self.stats.my_custom_stat = value\n</code></pre> <p>However, <code>xp_launch.run_experiment</code> (without <code>free=False</code>) will delete the <code>Stats</code> object from <code>xp</code> after the assimilation, in order to save memory. Therefore, in order to have <code>my_custom_stat</code> be available among <code>xp.avrgs</code>, it must be \"registered\":</p> <pre><code>self.stats.stat_register.append(\"my_custom_stat\")\n</code></pre> <p>Alternatively, you can do both at once</p> <pre><code>self.stat(\"my_custom_stat\", value)\n</code></pre> <p>Modules:</p> Name Description <code>liveplotting</code> <p>On-line (live) plots of the DA process for various models and methods.</p> <code>series</code> <p>Time series management and processing.</p>"},{"location":"reference/stats/#stats.Avrgs","title":"<code>Avrgs</code>","text":"<p>               Bases: <code>StatPrint</code>, <code>DotDict</code></p> <p>A <code>dict</code> specialized for the averages of statistics.</p> <p>Embellishments:</p> <ul> <li><code>tools.series.StatPrint</code></li> <li><code>Avrgs.tabulate</code></li> <li><code>getattr</code> that supports abbreviations.</li> </ul>"},{"location":"reference/stats/#stats.Avrgs.__getattribute__","title":"<code>__getattribute__(key)</code>","text":"<p>Support deep and abbreviated lookup.</p>"},{"location":"reference/stats/#stats.Avrgs.tabulate","title":"<code>tabulate(statkeys=(), decimals=None)</code>","text":"<p>Tabulate using <code>tabulate_avrgs</code></p>"},{"location":"reference/stats/#stats.Stats","title":"<code>Stats</code>","text":"<p>               Bases: <code>StatPrint</code></p> <p>Contains and computes statistics of the DA methods.</p>"},{"location":"reference/stats/#stats.Stats.__init__","title":"<code>__init__(xp, HMM, xx, yy, liveplots=False, store_u=rc.store_u)</code>","text":"<p>Init the default statistics.</p>"},{"location":"reference/stats/#stats.Stats.assess","title":"<code>assess(k, ko=None, faus=None, E=None, w=None, mu=None, Cov=None)</code>","text":"<p>Common interface for both <code>Stats</code>.assess_ens and <code>Stats</code>.assess_ext.</p> <p>The <code>_ens</code> assessment function gets called if <code>E is not None</code>, and <code>_ext</code> if <code>mu is not None</code>.</p> <p>faus: One or more of <code>['f',' a', 'u', 's']</code>, indicating       that the result should be stored in (respectively)       the forecast/analysis/universal attribute.       Default: <code>'u' if ko is None else 'au' ('a' and 'u')</code>.</p>"},{"location":"reference/stats/#stats.Stats.assess_ens","title":"<code>assess_ens(now, x, E, w)</code>","text":"<p>Ensemble and Particle filter (weighted/importance) assessment.</p>"},{"location":"reference/stats/#stats.Stats.assess_ext","title":"<code>assess_ext(now, x, mu, P)</code>","text":"<p>Kalman filter (Gaussian) assessment.</p>"},{"location":"reference/stats/#stats.Stats.average_in_time","title":"<code>average_in_time(kk=None, kko=None, free=False)</code>","text":"<p>Avarage all univariate (scalar) time series.</p> <ul> <li><code>kk</code>    time inds for averaging</li> <li><code>kko</code> time inds for averaging obs</li> </ul>"},{"location":"reference/stats/#stats.Stats.derivative_stats","title":"<code>derivative_stats(now)</code>","text":"<p>Stats that derive from others, and are not specific for <code>_ens</code> or <code>_ext</code>).</p>"},{"location":"reference/stats/#stats.Stats.new_series","title":"<code>new_series(name, shape, length='FAUSt', field_mean=False, **kws)</code>","text":"<p>Create (and register) a statistics time series, initialized with <code>nan</code>s.</p> <p>If <code>length</code> is an integer, a <code>DataSeries</code> (a trivial subclass of <code>numpy.ndarray</code>) is made. By default, though, a tools.series.FAUSt is created.</p> <p>NB: The <code>sliding_diagnostics</code> liveplotting relies on detecting <code>nan</code>'s     to avoid plotting stats that are not being used.     Thus, you cannot use <code>dtype=bool</code> or <code>int</code> for stats that get plotted.</p>"},{"location":"reference/stats/#stats.Stats.replay","title":"<code>replay(figlist='default', speed=np.inf, t1=0, t2=None, **kwargs)</code>","text":"<p>Replay LivePlot with what's been stored in 'self'.</p> <ul> <li>t1, t2: time window to plot.</li> <li>'figlist' and 'speed': See LivePlot's doc.</li> </ul> <p>Note</p> <p><code>store_u</code> (whether to store non-obs-time stats) must have been <code>True</code> to have smooth graphs as in the actual LivePlot.</p> <p>Note</p> <p>Ensembles are generally not stored in the stats and so cannot be replayed.</p>"},{"location":"reference/stats/#stats.Stats.summarize_marginals","title":"<code>summarize_marginals(now)</code>","text":"<p>Compute Mean-field and RMS values.</p>"},{"location":"reference/stats/#stats.Stats.write","title":"<code>write(stat_dict, k, ko, sub)</code>","text":"<p>Write <code>stat_dict</code> to series at <code>(k, ko, sub)</code>.</p>"},{"location":"reference/stats/#stats.align_col","title":"<code>align_col(col, pad='\u2423', missingval='', just='&gt;')</code>","text":"<p>Align column.</p> <p>Treats <code>int</code>s and fixed-point <code>float</code>/<code>str</code> especially, aligning on the point.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xx = [1, 1., 1.234, 12.34, 123.4, \"1.2e-3\", None, np.nan, \"inf\", (1, 2)]\n&gt;&gt;&gt; print(*align_col(xx), sep=\"\\n\")\n\u2423\u24231\u2423\u2423\u2423\u2423\n\u2423\u24231.0\u2423\u2423\n\u2423\u24231.234\n\u242312.34\u2423\n123.4\u2423\u2423\n\u24231.2e-3\n\u2423\u2423\u2423\u2423\u2423\u2423\u2423\n\u2423\u2423\u2423\u2423nan\n\u2423\u2423\u2423\u2423inf\n\u2423(1, 2)\n</code></pre>"},{"location":"reference/stats/#stats.center","title":"<code>center(E, axis=0, rescale=False)</code>","text":"<p>Center ensemble.</p> <p>Makes use of <code>np</code> features: keepdims and broadcasting.</p> <p>Parameters:</p> Name Type Description Default <code>E</code> <code>ndarray</code> <p>Ensemble which going to be inflated</p> required <code>axis</code> <code>int</code> <p>The axis to be centered. Default: 0</p> <code>0</code> <code>rescale</code> <code>bool</code> <p>If True, inflate to compensate for reduction in the expected variance. The inflation factor is \\(\\sqrt{\\frac{N}{N - 1}}\\) where N is the ensemble size. Default: False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Ensemble anomaly</p> <code>x</code> <code>ndarray</code> <p>Mean of the ensemble</p>"},{"location":"reference/stats/#stats.crps_ens","title":"<code>crps_ens(x, ensemble, weights=None)</code>","text":"<p>Compute CRPS for <code>ensemble</code> given obs/truth <code>x</code>.</p> <p>Tested to reproduce values from <code>properscoring.crps_ensemble()</code>.</p> <p>The 0<sup>th</sup> axis of <code>ensemble</code> is taken to enumerate the members, and must have same length as the 1d <code>weights</code> (if any) If <code>x.ndim == ensemble.ndim</code>, the 0<sup>th</sup> axis of <code>x</code> is taken to enumerate multiple x. The CRPS is computed independently (and efficiently) for any/all other dimensions. Thus <code>ensemble.shape[1:]</code> must be matched by <code>x.shape[1:]</code> or <code>x.shape</code>, and the output gets the shape of <code>x</code>.</p> <p>Examples:</p> <p>In 1D:</p> <pre><code>&gt;&gt;&gt; ens = np.array([-1.5, -1.0, 1.0, 1.5])\n&gt;&gt;&gt; crps_ens(0, ens)\narray(0.5625)\n</code></pre> <pre><code>&gt;&gt;&gt; crps_ens([0], ens)\narray([0.5625])\n</code></pre> <pre><code>&gt;&gt;&gt; crps_ens([-2, -1.5, 0, 1, 1.5, 2], ens)\narray([1.3125, 0.8125, 0.5625, 0.5625, 0.8125, 1.3125])\n</code></pre> <p>In 2D:</p> <pre><code>&gt;&gt;&gt; ens2 = np.vstack([ens, ens]).T\n&gt;&gt;&gt; crps_ens([1, 2], ens2)\narray([0.5625, 1.3125])\n</code></pre> <pre><code>&gt;&gt;&gt; crps_ens([[1, 2]], ens2)\narray([[0.5625, 1.3125]])\n</code></pre> <pre><code>&gt;&gt;&gt; crps_ens([[1, 2], [-2, -1.5]], ens2)\narray([[0.5625, 1.3125],\n       [1.3125, 0.8125]])\n</code></pre> <p>Try weighting:</p> <pre><code>&gt;&gt;&gt; from scipy.stats import norm\n&gt;&gt;&gt; rng = np.random.default_rng(3000)\n&gt;&gt;&gt; ens = rng.standard_normal(10**3)\n&gt;&gt;&gt; grd = np.linspace(-5, 5, num=len(ens))\n&gt;&gt;&gt; a1 = crps_ens(0, ens)\n&gt;&gt;&gt; a2 = crps_ens(0, ens, weights=norm.pdf(grd))\n&gt;&gt;&gt; np.allclose(a1, a2, atol=1e-2)\nTrue\n</code></pre>"},{"location":"reference/stats/#stats.crps_ext","title":"<code>crps_ext(x, mu, var)</code>","text":"<p>Adapted from <code>properscoring.crps_gaussian()</code>.</p> <p>The shapes of <code>x</code>, <code>mu</code>, and <code>var</code> must match, but can be anything, and yields output of same shape.</p> <p>Ref <code>http://cran.nexr.com/web/packages/scoringRules/vignettes/crpsformulas.html</code></p>"},{"location":"reference/stats/#stats.inflate_ens","title":"<code>inflate_ens(E, factor)</code>","text":"<p>Inflate the ensemble (center, inflate, re-combine).</p> <p>Parameters:</p> Name Type Description Default <code>E</code> <code>ndarray</code> <p>Ensemble which going to be inflated</p> required <code>factor</code> <code>`float`</code> <p>Inflation factor</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Inflated ensemble</p>"},{"location":"reference/stats/#stats.mean0","title":"<code>mean0(E, axis=0, rescale=True)</code>","text":"<p>Like <code>center</code>, but only return the anomalies (not the mean).</p> <p>Uses <code>rescale=True</code> by default, which is beneficial when used to center observation perturbations.</p>"},{"location":"reference/stats/#stats.np2builtin","title":"<code>np2builtin(v)</code>","text":"<p>Sometimes necessary since NEP-50</p>"},{"location":"reference/stats/#stats.register_stat","title":"<code>register_stat(self, name, value)</code>","text":"<p>Do <code>self.name = value</code> and register <code>name</code> as in self's <code>stat_register</code>.</p> <p>Note: <code>self</code> is not always a <code>Stats</code> object, but could be a \"child\" of it.</p>"},{"location":"reference/stats/#stats.tabulate_avrgs","title":"<code>tabulate_avrgs(avrgs_list, statkeys=(), decimals=None)</code>","text":"<p>Tabulate avrgs (val\u00b1prec).</p>"},{"location":"reference/stats/#stats.unbias_var","title":"<code>unbias_var(w=None, N_eff=None, avoid_pathological=False)</code>","text":"<p>Compute unbias-ing factor for variance estimation.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>Importance weights. Must sum to 1. Only one of <code>w</code> and <code>N_eff</code> can be <code>None</code>. Default: <code>None</code></p> <code>None</code> <code>N_eff</code> <code>float</code> <p>The \"effective\" size of the weighted ensemble. If not provided, it is computed from the weights. The unbiasing factor is $$ N_{eff} / (N_{eff} - 1) $$.</p> <code>None</code> <code>avoid_pathological</code> <code>bool</code> <p>Avoid weight collapse. Default: <code>False</code></p> <code>False</code> <p>Returns:</p> Name Type Description <code>ub</code> <code>float</code> <p>factor used to unbiasing variance</p> Reference <p>Wikipedia</p>"},{"location":"reference/stats/#stats.unpack_uqs","title":"<code>unpack_uqs(uq_list, decimals=None)</code>","text":"<p>Convert list of <code>uq</code>s into dict of lists (of equal-length) of attributes.</p> <p>The attributes are obtained by <code>vars(uq)</code>, and may get formatted somehow (e.g. cast to strings) in the output.</p> <p>If <code>uq</code> is <code>None</code>, then <code>None</code> is inserted in each list. Else, <code>uq</code> must be an instance of <code>tools.rounding.UncertainQtty</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uq_list</code> <code>list</code> <p>List of <code>uq</code>s.</p> required <code>decimals</code> <code>int</code> <p>Desired number of decimals. Used for (only) the columns \"val\" and \"prec\". Default: <code>None</code>. In this case, the formatting is left to the <code>uq</code>s.</p> <code>None</code>"},{"location":"reference/stats/#stats.weight_degeneracy","title":"<code>weight_degeneracy(w, prec=1e-10)</code>","text":"<p>Check if the weights are degenerate.</p> <p>If it is degenerate, the maximum weight should be nearly one since sum(w) = 1</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>Importance weights. Must sum to 1.</p> required <code>prec</code> <code>float</code> <p>Tolerance of the distance between w and one. Default:1e-10</p> <code>1e-10</code> <p>Returns:</p> Type Description <code>bool</code> <p>If weight is degenerate True, else False</p>"},{"location":"reference/xp_launch/","title":"xp_launch","text":"<p>Tools (notably <code>xpList</code>) for setup and running of experiments (known as <code>xp</code>s).</p> <p>See <code>da_methods.da_method</code> for the strict definition of <code>xp</code>s.</p> <p>Modules:</p> Name Description <code>dapper</code> <p>Root package of DAPPER</p> <code>pb</code> <p>Make <code>progbar</code> (wrapper around <code>tqdm</code>) and <code>read1</code>.</p>"},{"location":"reference/xp_launch/#xp_launch.xpList","title":"<code>xpList</code>","text":"<p>               Bases: <code>list</code></p> <p>Subclass of <code>list</code> specialized for experiment (\"xp\") objects.</p> <p>Main use: administrate experiment launches.</p> <p>Modifications to <code>list</code>:</p> <ul> <li><code>xpList.append</code> supports <code>unique</code> to enable lazy <code>xp</code> declaration.</li> <li><code>__iadd__</code> (<code>+=</code>) supports adding single <code>xp</code>s.   this is hackey, but convenience is king.</li> <li><code>__getitem__</code> supports lists, similar to <code>np.ndarray</code></li> <li><code>__repr__</code>: prints the list as rows of a table,   where the columns represent attributes whose value is not shared among all <code>xp</code>s.   Refer to <code>xpList.prep_table</code> for more information.</li> </ul> <p>Add-ons:</p> <ul> <li><code>xpList.launch</code>: run the experiments in current list.</li> <li><code>xpList.prep_table</code>: find all attributes of the <code>xp</code>s in the list;   classify as distinct, redundant, or common.</li> <li><code>xpList.gen_names</code>: use <code>xpList.prep_table</code> to generate   a short &amp; unique name for each <code>xp</code> in the list.</li> <li><code>xpList.tabulate_avrgs</code>: tabulate time-averaged results.</li> <li><code>xpList.inds</code> to search by kw-attrs.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>entries</code> <p>Nothing, or a list of <code>xp</code>s.</p> <code>()</code> <code>unique</code> <code>bool</code> <p>Duplicates won't get appended. Makes <code>append</code> (and <code>__iadd__</code>) relatively slow. Use <code>extend</code> or <code>__add__</code> or <code>combinator</code> to bypass this validation.</p> <code>False</code> Also see <ul> <li>Examples: <code>docs/examples/basic_2</code>, <code>docs/examples/basic_3</code></li> <li><code>xp_process.xpSpace</code>, which is used for experient result presentation,   as opposed to this class (<code>xpList</code>), which handles launching experiments.</li> </ul>"},{"location":"reference/xp_launch/#xp_launch.xpList.da_methods","title":"<code>da_methods</code>","text":"<p>List <code>da_method</code> attributes in this list.</p>"},{"location":"reference/xp_launch/#xp_launch.xpList.__getitem__","title":"<code>__getitem__(keys)</code>","text":"<p>Indexing, also by a list</p>"},{"location":"reference/xp_launch/#xp_launch.xpList.append","title":"<code>append(xp)</code>","text":"<p>Append if not <code>self.unique</code> &amp; present.</p>"},{"location":"reference/xp_launch/#xp_launch.xpList.gen_names","title":"<code>gen_names(abbrev=6, tab=False)</code>","text":"<p>Similiar to <code>self.__repr__()</code>, but:</p> <ul> <li>returns list of names</li> <li>tabulation is optional</li> <li>attaches (abbreviated) labels to each attribute</li> </ul>"},{"location":"reference/xp_launch/#xp_launch.xpList.inds","title":"<code>inds(strict=True, missingval='NONSENSE', **kws)</code>","text":"<p>Find (all) indices of <code>xps</code> whose attributes match kws.</p> <p>If strict, then <code>xp</code>s lacking a requested attr. will not match, unless the <code>missingval</code> matches the required value.</p>"},{"location":"reference/xp_launch/#xp_launch.xpList.launch","title":"<code>launch(HMM, save_as='noname', mp=False, fail_gently=None, **kwargs)</code>","text":"<p>Essentially: <code>for xp in self: run_experiment(xp, ..., **kwargs)</code>.</p> <p>See <code>run_experiment</code> for documentation on the <code>kwargs</code> and <code>fail_gently</code>. See <code>tools.datafiles.create_run_dir</code> for documentation <code>save_as</code>.</p> <p>Depending on <code>mp</code>, <code>run_experiment</code> is delegated as follows:</p> <ul> <li><code>False</code>: caller process (no parallelisation)</li> <li><code>True</code> or <code>\"MP\"</code> or an <code>int</code>: multiprocessing on this host</li> <li><code>\"GCP\"</code> or <code>\"Google\"</code> or <code>dict(server=\"GCP\")</code>: the DAPPER server   (Google Cloud Computing with HTCondor).<ul> <li>Specify a list of files as <code>mp[\"files\"]</code> to include them   in working directory of the server workers.</li> <li>In order to use absolute paths, the list should cosist   of tuples, where the first item is relative to the second   (which is an absolute path). The root is then not included   in the working directory of the server.</li> <li>If this dict field is empty, then all python files   in <code>sys.path[0]</code> are uploaded.</li> </ul> </li> </ul> <p>See <code>docs/examples/basic_2.py</code> and <code>docs/examples/basic_3.py</code> for example use.</p>"},{"location":"reference/xp_launch/#xp_launch.xpList.prep_table","title":"<code>prep_table(nomerge=())</code>","text":"<p>Classify all attrs. of all <code>xp</code>s as <code>distinct</code>, <code>redundant</code>, or <code>common</code>.</p> <p>An attribute of the <code>xp</code>s is inserted in one of the 3 dicts as follows: The attribute names become dict keys. If the values of an attribute (collected from all of the <code>xp</code>s) are all equal, then the attribute is inserted in <code>common</code>, but only with a single value. If they are all the same or missing, then it is inserted in <code>redundant</code> with a single value. Otherwise, it is inserted in <code>distinct</code>, with its full list of values (filling with <code>None</code> where the attribute was missing in the corresponding <code>xp</code>).</p> <p>The attrs in <code>distinct</code> are sufficient to (but not generally necessary, since there might exist a subset of attributes that) uniquely identify each <code>xp</code> in the list (the <code>redundant</code> and <code>common</code> can be \"squeezed\" out). Thus, a table of the <code>xp</code>s does not need to list all of the attributes. This function also does the heavy lifting for xp_process.xpSpace.squeeze.</p> <p>Parameters:</p> Name Type Description Default <code>nomerge</code> <code>list</code> <p>Attributes that should always be seen as distinct.</p> <code>()</code>"},{"location":"reference/xp_launch/#xp_launch.combinator","title":"<code>combinator(param_dict, **glob_dict)</code>","text":"<p>Mass creation of <code>xp</code>'s by combining the value lists in the <code>param_dict</code>.</p> <p>Returns a function (<code>for_params</code>) that creates all possible combinations of parameters (from their value list) for a given <code>da_methods.da_method</code>. This is a good deal more efficient than relying on <code>xpList</code>'s <code>unique</code>. Parameters</p> <ul> <li>not found among the args of the given DA method are ignored by <code>for_params</code>.</li> <li>specified as keywords to the <code>for_params</code> fix the value   preventing using the corresponding (if any) value list in the <code>param_dict</code>.</li> </ul> <p>Warning</p> <p>Beware! If, eg., <code>infl</code> or <code>rot</code> are in <code>param_dict</code>, aimed at the <code>EnKF</code>, but you forget that they are also attributes some method where you don't actually want to use them (eg. <code>SVGDF</code>), then you'll create many more than you intend.</p>"},{"location":"reference/xp_launch/#xp_launch.run_experiment","title":"<code>run_experiment(xp, label, savedir, HMM, setup=seed_and_simulate, free=True, statkeys=False, fail_gently=False, **stat_kwargs)</code>","text":"<p>Used by xp_launch.xpList.launch to run each single (DA) experiment (\"xp\").</p> <p>This involves steps similar to <code>docs/examples/basic_1.py</code>, i.e.:</p> <ul> <li><code>setup</code>                    : Initialize experiment.</li> <li><code>xp.assimilate</code>            : run DA, pass on exception if fail_gently</li> <li><code>xp.stats.average_in_time</code> : result averaging</li> <li><code>xp.avrgs.tabulate</code>        : result printing</li> <li><code>dill.dump</code>                : result storage</li> </ul> <p>Parameters:</p> Name Type Description Default <code>xp</code> <code>object</code> <p>Type: a <code>da_methods.da_method</code>-decorated class.</p> required <code>label</code> <code>str</code> <p>Name attached to progressbar during assimilation.</p> required <code>savedir</code> <code>str</code> <p>Path of folder wherein to store the experiment data.</p> required <code>HMM</code> <code>HiddenMarkovModel</code> <p>Container defining the system.</p> required <code>free</code> <code>bool</code> <p>Whether (or not) to <code>del xp.stats</code> after the experiment is done, so as to free up memory and/or not save this data (just keeping <code>xp.avrgs</code>).</p> <code>True</code> <code>statkeys</code> <code>list</code> <p>A list of names (possibly in the form of abbreviations) of the statistical averages that should be printed immediately afther this xp.</p> <code>False</code> <code>fail_gently</code> <code>bool</code> <p>Whether (or not) to propagate exceptions.</p> <code>False</code> <code>setup</code> <code>function</code> <p>This function must take two arguments: <code>HMM</code> and <code>xp</code>, and return the <code>HMM</code> to be used by the DA methods (typically the same as the input <code>HMM</code>, but could be modified), and the (typically synthetic) truth and obs time series.</p> <p>This gives you the ability to customize almost any aspect of the individual experiments within a batch launch of experiments (i.e. not just the parameters of the DA. method).  Typically you will grab one or more parameter values stored in the <code>xp</code> (see <code>da_methods.da_method</code>) and act on them, usually by assigning them to some object that impacts the experiment.  Thus, by generating a new <code>xp</code> for each such parameter value you can investigate the impact/sensitivity of the results to this parameter.  Examples include:</p> <ul> <li>Setting the seed. See the default <code>setup</code>, namely <code>seed_and_simulate</code>,   for how this is, or should be, done.</li> <li> <p>Setting some aspect of the <code>HMM</code> such as the observation noise,     or the interval between observations. This could be achieved for example by:</p> <pre><code>def setup(hmm, xp):\n    hmm.Obs.noise = GaussRV(M=hmm.Nx, C=xp.obs_noise)\n    hmm.tseq.dkObs = xp.time_between_obs\n    import dapper as dpr\n    return dpr.seed_and_simulate(hmm, xp)\n</code></pre> <p>This process could involve more steps, for example loading a full covariance matrix from a data file, as specified by the <code>obs_noise</code> parameter, before assigning it to <code>C</code>. Also note that the import statement is not strictly necessary (assuming <code>dapper</code> was already imported in the outer scope, typically the main script), except when running the experiments on a remote server.</p> <p>Sometimes, the parameter you want to set is not accessible as one of the conventional attributes of the <code>HMM</code>. For example, the <code>Force</code> in the Lorenz-96 model. In that case you can add these lines to the setup function:</p> <pre><code>import dapper.mods.Lorenz96 as core\ncore.Force = xp.the_force_parameter\n</code></pre> <p>However, if your model is an OOP instance, the import approach will not work because it will serve you the original model instance, while <code>setup()</code> deals with a copy of it. Instead, you could re-initialize the entire model in <code>setup()</code> and overwrite <code>HMM.Dyn</code>. However, it is probably easier to just assign the instance to some custom attribute before launching the experiments, e.g. <code>HMM.Dyn.object = the_model_instance</code>, enabling you to set parameters on <code>HMM.Dyn.object</code> in <code>setup()</code>. Note that this approach won't work for modules (for ex., combining the above examples, <code>HMM.Dyn.object = core</code>) because modules are not serializable.</p> </li> <li> <p>Using a different <code>HMM</code> entirely for the truth/obs (<code>xx</code>/<code>yy</code>) generation,   than the one that will be used by the DA. Or loading the truth/obs   time series from file. In both cases, you might also have to do some   cropping or slicing of <code>xx</code> and <code>yy</code> before returning them.</p> </li> </ul> <code>seed_and_simulate</code>"},{"location":"reference/xp_launch/#xp_launch.seed_and_simulate","title":"<code>seed_and_simulate(HMM, xp)</code>","text":"<p>Default experiment setup (sets seed and simulates truth and obs).</p> <p>Used by xp_launch.xpList.launch via xp_launch.run_experiment.</p> <p>Parameters:</p> Name Type Description Default <code>HMM</code> <code>HiddenMarkovModel</code> <p>Container defining the system.</p> required <code>xp</code> <code>object</code> <p>Type: a <code>da_methods.da_method</code>-decorated class.</p> <p><code>xp.seed</code> should be set (and <code>int</code>).</p> <p>Without <code>xp.seed</code> the seed does not get set, and different <code>xp</code>s will use different seeds (unless you do some funky hacking). Reproducibility for a script as a whole can still be achieved by setting the seed at the outset of the script. To avoid even that, set <code>xp.seed</code> to <code>None</code> or <code>\"clock\"</code>.</p> required <p>Returns:</p> Type Description <code>tuple(xx, yy)</code> <p>The simulated truth and observations.</p>"},{"location":"reference/xp_process/","title":"xp_process","text":"<p>Tools (notably <code>xpSpace</code>) for processing and presenting experiment data.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace","title":"<code>SparseSpace</code>","text":"<p>               Bases: <code>dict</code></p> <p>Subclass of <code>dict</code> that enforces key conformity to a given <code>namedtuple</code>.</p> <p>Like a normal <code>dict</code>, it can hold any type of objects. But, since the keys must conform, they effectively follow a coordinate system, so that the <code>dict</code> becomes a vector space.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dct = xpSpace([\"x\", \"y\", \"z\"])\n&gt;&gt;&gt; dct[(1, 2, 3)] = \"pointA\"\n</code></pre> <p>The coordinate system is specified by the <code>dims</code>: a list of keys defining the <code>namedtuple</code> of <code>self.Coord</code>. The above dict only has three <code>dims</code>, so this fails:</p> <pre><code>&gt;&gt;&gt; dct[(1, 2, 3, 4)] = \"pointB\"\nTraceback (most recent call last):\n...\nTypeError: The key (1, 2, 3, 4) did not fit the coord.  system\nwhich has dims ('x', 'y', 'z')\n</code></pre> <p>Coordinates can contain any value, including <code>None</code>:</p> <pre><code>&gt;&gt;&gt; dct[(1, 2, None)] = \"pointB\"\n</code></pre> <p>In intended usage, this space is highly sparse, meaning there are many coordinates with no entry. Indeed, as a data format for nd-arrays, it may be called \"coordinate list representation\", used e.g. by <code>scipy.sparse.coo_matrix</code>.</p> <p>Thus, operations across (potentially multiple) <code>dims</code>, such as optimization or averaging, should be carried out by iterating -- not over the <code>dims</code> -- but over the the list of items.</p> <p>The most important method is <code>nest</code>, which is used (by <code>xpSpace.table_tree</code>) to print and plot results. This is essentially a \"groupby\" operation, and indeed the case could be made that this class should be replaced by <code>pandas.DataFrame</code>, or better yet: https://github.com/pydata/xarray.</p> <p>The <code>__getitem__</code> is quite flexible, allowing accessing by:</p> <ul> <li> <p>The actual key, a <code>self.Coord</code> object, or a standard tuple.   Returns single item. Example:</p> <pre><code>&gt;&gt;&gt; dct[1, 2, 3] == dct[(1, 2, 3)] == dct[dct.Coord(1, 2, 3)] == \"pointA\"\nTrue\n</code></pre> </li> <li> <p>A <code>slice</code> or <code>list</code>.   Returns list. PS: indexing by slice or list assumes that the dict is ordered,   which we inherit from the builtin <code>dict</code> since Python 3.7.   Moreover, it is a reflection of the fact that the internals of this class   work by looping over items.</p> </li> </ul> <p>In addition, the <code>subspace</code> method (also aliased to <code>__call__</code>, and is implemented via <code>coords_matching</code>) can be used to select items by the values of a subset of their attributes. It returns a <code>SparseSpace</code>. If there is only a single item it can be accessed as in <code>dct[()]</code>.</p> <p>Inspired by</p> <ul> <li>https://stackoverflow.com/a/7728830</li> <li>https://stackoverflow.com/q/3387691</li> </ul>"},{"location":"reference/xp_process/#xp_process.SparseSpace.__call__","title":"<code>__call__(**kwargs)</code>","text":"<p>Shortcut (syntactic sugar) for xp_process.SparseSpace.subspace.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Also allows list-indexing by <code>list</code> and <code>slice</code>.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.__init__","title":"<code>__init__(dims)</code>","text":"<p>Usually initialized through <code>xpSpace.from_list</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>list or tuple</code> <p>The attributes defining the coordinate system.</p> required"},{"location":"reference/xp_process/#xp_process.SparseSpace.__setitem__","title":"<code>__setitem__(key, val)</code>","text":"<p>Setitem ensuring coordinate conforms.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.append_dim","title":"<code>append_dim(dim)</code>","text":"<p>Expand <code>self.Coord</code> by <code>dim</code>. For each item, insert <code>None</code> in new dim.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.coord_from_attrs","title":"<code>coord_from_attrs(obj)</code>","text":"<p>Form a <code>coord</code> for this <code>xpSpace</code> by extracting attrs. from <code>obj</code>.</p> <p>For instances of <code>self.Coord</code>, this is the identity opeartor, i.e.</p> <pre><code>self.coord_from_attrs(coord) == coord\n</code></pre>"},{"location":"reference/xp_process/#xp_process.SparseSpace.coords_matching","title":"<code>coords_matching(**kwargs)</code>","text":"<p>Get all <code>coord</code>s matching kwargs.</p> <p>Used by xp_process.SparseSpace.label_xSection and xp_process.SparseSpace.subspace. Unlike the latter, this function returns a list of keys of the original subspace.</p> <p>Note that the <code>missingval</code> shenanigans of xp_launch.xpList.inds are here unnecessary since each coordinate is complete.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.intersect_dims","title":"<code>intersect_dims(attrs)</code>","text":"<p>Rm those <code>a</code> in <code>attrs</code> that are not in <code>self.dims</code>.</p> <p>This enables sloppy <code>dims</code> allotment, for ease-of-use.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.label_xSection","title":"<code>label_xSection(label, *NoneAttrs, **sub_coord)</code>","text":"<p>Insert duplicate entries for the given cross-section.</p> <p>Works by adding the attr. <code>xSection</code> to the dims of <code>SparseSpace</code>, and setting it to <code>label</code> for entries matching <code>sub_coord</code>, reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up by the operations of <code>nest</code>.</p> <p>If you wish, you can specify the <code>NoneAttrs</code>, which are consequently set to None for the duplicated entries, preventing them from being shown in plot labels and tuning panels.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.nest","title":"<code>nest(inner_dims=None, outer_dims=None)</code>","text":"<p>Project along <code>inner_acces</code> to yield a new <code>xpSpace</code> with dims <code>outer_dims</code></p> <p>The entries of this <code>xpSpace</code> are themselves <code>xpSpace</code>s, with dims <code>inner_dims</code>, each one regrouping the entries with the same (projected) coordinate.</p> <p>Note: this method could also be called <code>groupby</code>. Note: this method is also called by <code>__getitem__(key)</code> if <code>key</code> is dict.</p>"},{"location":"reference/xp_process/#xp_process.SparseSpace.subspace","title":"<code>subspace(**kwargs)</code>","text":"<p>Get an affine subspace.</p> <p>NB: If you're calling this repeatedly (for all values of the same <code>kwargs</code>) then you should consider using xp_process.SparseSpace.nest instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xp_dict.subspace(da_method=\"EnKF\", infl=1, seed=3)\n</code></pre>"},{"location":"reference/xp_process/#xp_process.SparseSpace.update","title":"<code>update(items)</code>","text":"<p>Update dict, using the custom <code>__setitem__</code> to ensure key conformity.</p> <p>NB: the <code>kwargs</code> syntax is not supported because it only works for keys that consist of (a single) string, which is not very interesting for <code>SparseSpace</code>.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace","title":"<code>xpSpace</code>","text":"<p>               Bases: <code>SparseSpace</code></p> <p>Functionality to facilitate working with <code>xps</code> and their results.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.fill","title":"<code>fill(xps)</code>","text":"<p>Mass insertion.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.from_list","title":"<code>from_list(xps, tick_ordering=None)</code>","text":"<p>Init. from a list of objects, typically experiments referred to as <code>xp</code>s.</p> <ul> <li>Computes the relevant <code>dims</code> from the attributes, and</li> <li>Fills the dict by <code>xp</code>s.</li> <li>Computes and writes the attribute <code>ticks</code>.</li> </ul> <p>This creates a <code>SparseSpace</code> of <code>xp</code>s. However, the nested subspaces generated by <code>xpSpace.table_tree</code> (for printing and plotting) will hold objects of type <code>UncertainQtty</code>, because it calls <code>mean</code> which calls <code>get_stat(statkey)</code>.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.get_stat","title":"<code>get_stat(statkey)</code>","text":"<p>Make <code>xpSpace</code> with same <code>Coord</code> as <code>self</code>, but values <code>xp.avrgs.statkey</code>.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.make_ticks","title":"<code>make_ticks(dct, ordering=None)</code>","text":"<p>Unique &amp; sort, for each individual \"dim\" in <code>dct</code>. Assign to <code>self.ticks</code>.</p> <p>NB: <code>self.ticks</code> will not \"propagate\" through <code>SparseSpace.nest</code> or the like.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.mean","title":"<code>mean(dims=None)</code>","text":"<p>Compute mean over <code>dims</code> (a list). Returns <code>xpSpace</code> without those <code>dims</code>.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.plot","title":"<code>plot(statkey, dims, get_style=default_styles, fignum=None, figsize=None, panels=None, costfun=None, title1=None, title2=None, unique_labels=True, squeeze_labels=True)</code>","text":"<p>Plot (tables of) results.</p> <p>Analagously to <code>xpSpace.print</code>, the averages are grouped by <code>dims[\"inner\"]</code>, which here plays the role of the x-axis.</p> <p>The averages can also be grouped by <code>dims[\"outer\"]</code>, producing a figure with multiple (columns of) panels.</p> <p>The optimal points/parameters/attributes are plotted in smaller panels below the main plot. This can be turned off by providing the figure dims through the <code>panels</code> argument.</p> <p>The parameters <code>statkey</code>, <code>dims</code>, <code>costfun</code>, <code>sqeeze_labels</code> are documented in <code>xpSpace.print</code>.</p> <p>Parameters:</p> Name Type Description Default <code>get_style</code> <code>function</code> <p>A function that takes an object, and returns a dict of line styles, usually as a function of the object's attributes.</p> <code>default_styles</code> <code>title1</code> <code>anything</code> <p>Figure title (in addition to the the defaults).</p> <code>None</code> <code>title2</code> <code>anything</code> <p>Figure title (in addition to the defaults). Goes on a new line.</p> <code>None</code> <code>unique_labels</code> <code>bool</code> <p>Only show a given line label once, even if it appears in several panels.</p> <code>True</code> <code>squeeze_labels</code> <p>Don't include redundant attributes in the labels.</p> <code>True</code>"},{"location":"reference/xp_process/#xp_process.xpSpace.print","title":"<code>print(statkey, dims, subcols=True, decimals=None, costfun=None, squeeze_labels=True, colorize=True, title=None)</code>","text":"<p>Print tables of results.</p> <p>Parameters:</p> Name Type Description Default <code>statkey</code> <code>str</code> <p>The statistic to extract from the <code>xp.avrgs</code> for each <code>xp</code>. Examples: <code>\"rmse.a\"</code> (i.e. <code>\"err.rms.a\"</code>), <code>\"rmse.ocean.a\"</code>, <code>\"duration\"</code>.</p> required <code>dims</code> <code>dict</code> <p>Allots (maps) the dims of <code>xpSpace</code> to different roles in the tables.</p> <ul> <li>The \"role\" <code>outer</code> should list the dims/attributes   used to define the splitting of the results into separate tables:   one table for each distinct combination of attributes.</li> <li>Similarly , the role <code>inner</code> determines which attributes   split a table into its columns.</li> <li><code>mean</code> lists the attributes over which the mean is taken   (for that row &amp; column)</li> <li><code>optim</code> lists the attributes used over which the optimum    is searched for (after taking the mean).</li> </ul> <p>Example:</p> <pre><code>dict(outer='da_method', inner='N', mean='seed',\n     optim=('infl','loc_rad'))\n</code></pre> <p>Equivalently, use <code>mean=(\"seed\",)</code>. It is acceptible to leave this empty: <code>mean=()</code> or <code>mean=None</code>.</p> required <code>subcols</code> <code>bool</code> <p>If <code>True</code>, then subcolumns are added to indicate</p> <ul> <li><code>1\u03c3</code>: the confidence interval. If <code>mean=None</code> is used, this simply reports   the value <code>.prec</code> of the <code>statkey</code>, providing this is an <code>UncertainQtty</code>.   Otherwise, it is computed as <code>sqrt(var(xps)/N)</code>,   where <code>xps</code> is the set of statistic gathered over the <code>mean</code> dimensions.</li> <li><code>*(optim)</code>: the optimal point (among all <code>optim</code> attributes),   as defined by <code>costfun</code>.</li> <li><code>\u2620</code>: the number of failures (non-finite values) at that point.</li> <li><code>\u2713</code>: the number of successes that go into the value</li> </ul> <code>True</code> <code>decimals</code> <code>int</code> <p>Number of decimals to print. If <code>None</code>, this is determined for each statistic by its uncertainty.</p> <code>None</code> <code>costfun</code> <code>str or function</code> <p>Use <code>'increasing'</code> (default) or <code>'decreasing'</code> to indicate that the optimum is defined as the lowest or highest value of the <code>statkey</code> found.</p> <code>None</code> <code>squeeze_labels</code> <code>bool</code> <p>Don't include redundant attributes in the line labels. Caution: <code>get_style</code> will not be able to access the eliminated attrs.</p> <code>True</code> <code>colorize</code> <code>bool</code> <p>Add color to tables for readability.</p> <code>True</code>"},{"location":"reference/xp_process/#xp_process.xpSpace.squeeze","title":"<code>squeeze()</code>","text":"<p>Eliminate unnecessary dimensions.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.table_tree","title":"<code>table_tree(statkey, dims, *, costfun=None)</code>","text":"<p>Make hierarchy <code>outer &gt; inner &gt; mean &gt; optim</code> using <code>SparseSpace.nest</code>.</p> <p>The dimension passed to <code>nest</code> (at each level) is specified by <code>dims</code>. The dimensions of <code>dims['mean']</code> and <code>dims['optim']</code> get eliminated by the mean/tune operations. The <code>dims['outer']</code> and `dims['inner'] become the keys for the output hierarchy.</p> <p>Note</p> <p>Cannot support multiple <code>statkey</code>s because it's not (obviously) meaningful when optimizing over <code>dims['optim']</code>.</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.tickz","title":"<code>tickz(dim_name)</code>","text":"<p>Dimension (axis) ticks without None</p>"},{"location":"reference/xp_process/#xp_process.xpSpace.tune","title":"<code>tune(dims=None, costfun=None)</code>","text":"<p>Get (compile/tabulate) a stat. optimised wrt. tuning params (<code>dims</code>).</p>"},{"location":"reference/da_methods/","title":"da_methods","text":"<p>Contains the data assimilation methods included with DAPPER.</p> <p>Also see this section on DA Methods for an overview of the methods included with DAPPER.</p>"},{"location":"reference/da_methods/#da_methods--defining-your-own-method","title":"Defining your own method","text":"<p>Follow the example of one of the methods within one of the sub-directories/packages. The simplest example is perhaps <code>da_methods.ensemble.EnKF</code>.</p>"},{"location":"reference/da_methods/#da_methods--general-advice-for-programmingdebugging-scientific-experiments","title":"General advice for programming/debugging scientific experiments","text":"<ul> <li> <p>Start with something simple.   This helps make sure the basics of the experiment are reasonable.   For example, start with</p> <ul> <li>a pre-existing example,</li> <li>something you are able to reproduce,</li> <li> <p>a small/simple model.</p> <ul> <li>Set the observation error to be small.</li> <li>Observe everything.</li> <li>Don't include model error and/or noise to begin with.</li> </ul> </li> </ul> </li> <li> <p>Additionally, test a simple/baseline method to begin with.   When including an ensemble method, start with using a large ensemble,   and introduce localisation later.</p> </li> <li> <p>Take incremental steps towards your ultimate experiment setup.   Validate each incremental setup with prints/plots.   If results change, make sure you understand why.</p> </li> <li> <p>Use short experiment duration.   You probably don't need statistical significance while debugging.</p> </li> </ul> <p>Modules:</p> Name Description <code>baseline</code> <p>Unsophisticated\" but robust (widely applicable) DA methods.</p> <code>ensemble</code> <p>The EnKF and other ensemble-based methods.</p> <code>extended</code> <p>The extended KF (EKF) and the (Rauch-Tung-Striebel) smoother.</p> <code>other</code> <p>More experimental or esoteric DA methods.</p> <code>particle</code> <p>Weight- &amp; resampling-based DA methods.</p> <code>variational</code> <p>Variational DA methods (iEnKS, 4D-Var, etc).</p>"},{"location":"reference/da_methods/#da_methods.da_method","title":"<code>da_method(*default_dataclasses)</code>","text":"<p>Turn a dataclass-style class into a DA method for DAPPER (<code>xp</code>).</p> <p>This decorator applies to classes that define DA methods. An instances of the resulting class is referred to (in DAPPER) as an <code>xp</code> (short for experiment).</p> <p>The decorated classes are defined like a <code>dataclass</code>, but are decorated by <code>@da_method()</code> instead of <code>@dataclass</code>.</p> <p>Note</p> <p>The classes must define a method called <code>assimilate</code>. This method gets slightly enhanced by this wrapper which provides:</p> <ul> <li>Initialisation of the <code>Stats</code> object, accessible by <code>self.stats</code>.</li> <li><code>fail_gently</code> functionality.</li> <li>Duration timing</li> <li>Progressbar naming magic.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @da_method()\n... class Sleeper():\n...     \"Do nothing.\"\n...     seconds : int  = 10\n...     success : bool = True\n...     def assimilate(self, *args, **kwargs):\n...         for k in range(self.seconds):\n...             time.sleep(1)\n...         if not self.success:\n...             raise RuntimeError(\"Sleep over. Failing as intended.\")\n</code></pre> <p>Internally, <code>da_method</code> is just like <code>dataclass</code>, except that adds an outer layer (hence the empty parantheses in the above) which enables defining default parameters which can be inherited, similar to subclassing.</p> <pre><code>&gt;&gt;&gt; class ens_defaults:\n...     infl : float = 1.0\n...     rot  : bool  = False\n</code></pre> <pre><code>&gt;&gt;&gt; @da_method(ens_defaults)\n... class EnKF:\n...     N     : int\n...     upd_a : str = \"Sqrt\"\n...\n...     def assimilate(self, HMM, xx, yy):\n...         ...\n</code></pre> <p>Note</p> <p>Apart from what's listed in the above <code>Note</code>, there is nothing special to the resulting <code>xp</code>.  That is, just like any Python object, it can serve as a data container, and you can write any number of attributes to it (at creation-time, or later).  For example, you can set attributes that are not used by the <code>assimilate</code> method, but are instead used to customize other aspects of the experiments (see <code>xp_launch.run_experiment</code>).</p>"},{"location":"reference/da_methods/baseline/","title":"baseline","text":"<p>Unsophisticated\" but robust (widely applicable) DA methods.</p> <p>Many are based on raanes2016thesis.</p> <p>Modules:</p> Name Description <code>series</code> <p>Time series management and processing.</p>"},{"location":"reference/da_methods/baseline/#da_methods.baseline.Climatology","title":"<code>Climatology</code>","text":"<p>A baseline/reference method.</p> <p>Note that the \"climatology\" is computed from truth, which might be (unfairly) advantageous if the simulation is too short (vs mixing time).</p>"},{"location":"reference/da_methods/baseline/#da_methods.baseline.OptInterp","title":"<code>OptInterp</code>","text":"<p>Optimal Interpolation -- a baseline/reference method.</p> <p>Uses the Kalman filter equations, but with a prior from the Climatology.</p>"},{"location":"reference/da_methods/baseline/#da_methods.baseline.Persistence","title":"<code>Persistence</code>","text":"<p>Sets estimate to the true state at the previous time index.</p> <p>The analysis (<code>.a</code>) stat uses the previous obs. time. The forecast and universal (<code>.f</code> and <code>.u</code>) stats use previous integration time index.</p>"},{"location":"reference/da_methods/baseline/#da_methods.baseline.PreProg","title":"<code>PreProg</code>","text":"<p>Simply look-up the estimates in user-specified function (<code>schedule</code>).</p> <p>For example, with <code>schedule</code> given by <code>lambda k, xx, yy: xx[k]</code> the error (<code>err.rms, err.ma, ...</code>) should be 0.</p>"},{"location":"reference/da_methods/baseline/#da_methods.baseline.Var3D","title":"<code>Var3D</code>","text":"<p>3D-Var -- a baseline/reference method.</p> <p>This implementation is not \"Var\"-ish: there is no iterative optimzt. Instead, it does the full analysis update in one step: the Kalman filter, with the background covariance being user specified, through B and xB.</p>"},{"location":"reference/da_methods/baseline/#da_methods.baseline.fit_sigmoid","title":"<code>fit_sigmoid(Sb, L, kb)</code>","text":"<p>Return a sigmoid [function S(k)] for approximating error dynamics.</p> <p>We use the logistic function for the sigmoid; it's the solution of the \"population growth\" ODE: <code>dS/dt = a*S*(1-S/S(\u221e))</code>. NB: It might be better to use the \"error growth ODE\" of Lorenz/Dalcher/Kalnay, but this has a significantly more complicated closed-form solution, and reduces to the above ODE when there's no model error (ODE source term).</p> <p>The \"normalized\" sigmoid, <code>S1</code>, is symmetric around 0, and <code>S1(-\u221e)=0</code> and <code>S1(\u221e)=1</code>.</p> <p>The sigmoid <code>S(k) = S1(a*(k-kb) + b)</code> is fitted with</p> <ul> <li><code>a</code> corresponding to a given corr. length <code>L</code>.</li> <li><code>b</code> to match values of <code>S(kb)</code> and <code>Sb</code></li> </ul> Illustration"},{"location":"reference/da_methods/ensemble/","title":"ensemble","text":"<p>The EnKF and other ensemble-based methods.</p> <p>Modules:</p> Name Description <code>multiproc</code> <p>Paralellisation via multiprocessing. Limit num. of CPUs used by <code>numpy</code> to 1.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.EnKF","title":"<code>EnKF</code>","text":"<p>The ensemble Kalman filter.</p> <p>Refs: evensen2009a.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.EnKF_N","title":"<code>EnKF_N</code>","text":"<p>Finite-size EnKF (EnKF-N).</p> <p>Refs: bocquet2011, bocquet2015</p> <p>This implementation is pedagogical, prioritizing the \"dual\" form. In consequence, the efficiency of the \"primal\" form suffers a bit. The primal form is included for completeness and to demonstrate equivalence. In <code>da_methods.variational.iEnKS</code>, however, the primal form is preferred because it already does optimization for w (as treatment for nonlinear models).</p> <p><code>infl</code> should be unnecessary (assuming no model error, or that Q is correct).</p> <p><code>Hess</code>: use non-approx Hessian for ensemble transform matrix?</p> <p><code>g</code> is the nullity of A (state anomalies's), ie. g=max(1,N-Nx), compensating for the redundancy in the space of w. But we have made it an input argument instead, with default 0, because mode-finding (of p(x) via the dual) completely ignores this redundancy, and the mode gets (undesireably) modified by g.</p> <p><code>xN</code> allows tuning the hyper-prior for the inflation. Usually, I just try setting it to 1 (default), or 2. Further description in hyperprior_coeffs().</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.EnKS","title":"<code>EnKS</code>","text":"<p>The ensemble Kalman smoother.</p> <p>Refs: evensen2009a</p> <p>The only difference to the EnKF is the management of the lag and the reshapings.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.EnRTS","title":"<code>EnRTS</code>","text":"<p>EnRTS (Rauch-Tung-Striebel) smoother.</p> <p>Refs: raanes2016thesis</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.LETKF","title":"<code>LETKF</code>","text":"<p>Same as EnKF (Sqrt), but with localization.</p> <p>Refs: hunt2007.</p> <p>NB: Multiproc. yields slow-down for <code>mods.Lorenz96</code>, even with <code>batch_size=(1,)</code>. But for <code>mods.QG</code> (<code>batch_size=(2,2)</code> or less) it is quicker.</p> <p>NB: If <code>len(ii)</code> is small, analysis may be slowed-down with '-N' infl.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.SL_EAKF","title":"<code>SL_EAKF</code>","text":"<p>Serial, covariance-localized EAKF.</p> <p>Refs: karspeck2007.</p> <p>In contrast with LETKF, this iterates over the observations rather than over the state (batches).</p> <p>Used without localization, this should be equivalent (full ensemble equality) to the <code>EnKF</code> with <code>upd_a='Serial'</code>.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.ens_method","title":"<code>ens_method</code>","text":"<p>Declare default ensemble arguments.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.EnKF_analysis","title":"<code>EnKF_analysis(E, Eo, hnoise, y, upd_a, stats=None, ko=None)</code>","text":"<p>Perform the EnKF analysis update.</p> <p>This implementation includes several flavours and forms, specified by <code>upd_a</code>.</p> <p>Main references: sakov2008b, sakov2008a, hoteit2015a</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.Newton_m","title":"<code>Newton_m(fun, deriv, x0, is_inverted=False, conf=1.0, xtol=0.0001, ytol=1e-07, itermax=10 ** 2)</code>","text":"<p>Find root of <code>fun</code>.</p> <p>This is a simple (and pretty fast) implementation of Newton's method.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.add_noise","title":"<code>add_noise(E, dt, noise, method)</code>","text":"<p>Treatment of additive noise for ensembles.</p> <p>Refs: raanes2014</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.effective_N","title":"<code>effective_N(YR, dyR, xN, g)</code>","text":"<p>Effective ensemble size N.</p> <p>As measured by the finite-size EnKF-N</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.hyperprior_coeffs","title":"<code>hyperprior_coeffs(s, N, xN=1, g=0)</code>","text":"<p>Set EnKF-N inflation hyperparams.</p> <p>The EnKF-N prior may be specified by the constants:</p> <ul> <li><code>eN</code>: Effect of unknown mean</li> <li><code>cL</code>: Coeff in front of log term</li> </ul> <p>These are trivial constants in the original EnKF-N, but are further adjusted (corrected and tuned) for the following reasons.</p> <ul> <li> <p>Reason 1: mode correction.   These parameters bridge the Jeffreys (<code>xN=1</code>) and Dirac (<code>xN=Inf</code>) hyperpriors   for the prior covariance, B, as discussed in bocquet2015.   Indeed, mode correction becomes necessary when $$ R \\rightarrow \\infty $$   because then there should be no ensemble update (and also no inflation!).   More specifically, the mode of <code>l1</code>'s should be adjusted towards 1   as a function of $$ I - K H $$ (\"prior's weight\").   PS: why do we leave the prior mode below 1 at all?   Because it sets up \"tension\" (negative feedback) in the inflation cycle:   the prior pulls downwards, while the likelihood tends to pull upwards.</p> </li> <li> <p>Reason 2: Boosting the inflation prior's certainty from N to xN*N.   The aim is to take advantage of the fact that the ensemble may not   have quite as much sampling error as a fully stochastic sample,   as illustrated in section 2.1 of raanes2019a.</p> </li> <li> <p>Its damping effect is similar to work done by J. Anderson.</p> </li> </ul> <p>The tuning is controlled by:</p> <ul> <li><code>xN=1</code>: is fully agnostic, i.e. assumes the ensemble is generated   from a highly chaotic or stochastic model.</li> <li><code>xN&gt;1</code>: increases the certainty of the hyper-prior,   which is appropriate for more linear and deterministic systems.</li> <li><code>xN&lt;1</code>: yields a more (than 'fully') agnostic hyper-prior,   as if N were smaller than it truly is.</li> <li><code>xN&lt;=0</code> is not meaningful.</li> </ul>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.local_analyses","title":"<code>local_analyses(E, Eo, R, y, state_batches, obs_taperer, mp=map, xN=None, g=0)</code>","text":"<p>Perform local analysis update for the LETKF.</p>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.post_process","title":"<code>post_process(E, infl, rot)</code>","text":"<p>Inflate, Rotate.</p> <p>To avoid recomputing/recombining anomalies, this should have been inside <code>EnKF_analysis</code></p> <p>But it is kept as a separate function</p> <ul> <li>for readability;</li> <li>to avoid inflating/rotationg smoothed states (for the <code>EnKS</code>).</li> </ul>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.serial_inds","title":"<code>serial_inds(upd_a, y, cvR, A)</code>","text":"<p>Get the indices used for serial updating.</p> <ul> <li>Default: random ordering</li> <li>if \"mono\" in <code>upd_a</code>: <code>1, 2, ..., len(y)</code></li> <li>if \"sorted\" in <code>upd_a</code>: sort by variance</li> </ul>"},{"location":"reference/da_methods/ensemble/#da_methods.ensemble.zeta_a","title":"<code>zeta_a(eN, cL, w)</code>","text":"<p>EnKF-N inflation estimation via w.</p> <p>Returns <code>zeta_a = (N-1)/pre-inflation^2</code>.</p> <p>Using this inside an iterative minimization as in the <code>da_methods.variational.iEnKS</code> effectively blends the distinction between the primal and dual EnKF-N.</p>"},{"location":"reference/da_methods/extended/","title":"extended","text":"<p>The extended KF (EKF) and the (Rauch-Tung-Striebel) smoother.</p>"},{"location":"reference/da_methods/extended/#da_methods.extended.ExtKF","title":"<code>ExtKF</code>","text":"<p>The extended Kalman filter.</p> <p>If everything is linear-Gaussian, this provides the exact solution to the Bayesian filtering equations.</p> <ul> <li>infl (inflation) may be specified.   Default: 1.0 (i.e. none), as is optimal in the lin-Gauss case.   Gets applied at each dt, with infl_per_dt := inlf**(dt), so that   infl_per_unit_time == infl.   Specifying it this way (per unit time) means less tuning.</li> </ul>"},{"location":"reference/da_methods/extended/#da_methods.extended.ExtRTS","title":"<code>ExtRTS</code>","text":"<p>The extended Rauch-Tung-Striebel (or \"two-pass\") smoother.</p>"},{"location":"reference/da_methods/other/","title":"other","text":"<p>More experimental or esoteric DA methods.</p>"},{"location":"reference/da_methods/other/#da_methods.other.LNETF","title":"<code>LNETF</code>","text":"<p>The Nonlinear-Ensemble-Transform-Filter (localized).</p> <p>Refs: wiljes2016, todter2015a.</p> <p>It is (supposedly) a deterministic upgrade of the NLEAF of lei2011.</p>"},{"location":"reference/da_methods/other/#da_methods.other.RHF","title":"<code>RHF</code>","text":"<p>Rank histogram filter.</p> <p>Refs: anderson2010.</p> <p>Quick &amp; dirty implementation without attention to (de)tails.</p>"},{"location":"reference/da_methods/other/#da_methods.other.laplace_lklhd","title":"<code>laplace_lklhd(xx)</code>","text":"<p>Compute a Laplacian likelihood.</p> <p>Compute likelihood of xx wrt. the sampling distribution RVs.LaplaceParallelRV(C=I), i.e., for x in xx: p(x) = exp(-sqrt(2)*|x|_1) / sqrt(2).</p>"},{"location":"reference/da_methods/particle/","title":"particle","text":"<p>Weight- &amp; resampling-based DA methods.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.OptPF","title":"<code>OptPF</code>","text":"<p>'Optimal proposal' particle filter, also known as 'Implicit particle filter'.</p> <p>Ref: bocquet2010a.</p> <p>!!! note Regularization (<code>Qs</code>) is here added BEFORE Bayes' rule.     If <code>Qs==0</code>: OptPF should be equal to the bootstrap filter <code>PartFilt</code>.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.PFa","title":"<code>PFa</code>","text":"<p>PF with weight adjustment withOUT compensating for the bias it introduces.</p> <p>'alpha' sets wroot before resampling such that N_effective becomes &gt;alpha*N.</p> <p>Using alpha\u2248NER usually works well.</p> <p>Explanation: Recall that the bootstrap particle filter has \"no\" bias, but significant variance (which is reflected in the weights). The EnKF is quite the opposite. Similarly, by adjusting the weights we play on the bias-variance spectrum.</p> <p>NB: This does not mean that we make a PF-EnKF hybrid -- we're only playing on the weights.</p> <p>Hybridization with xN did not show much promise.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.PFxN","title":"<code>PFxN</code>","text":"<p>Particle filter with buckshot duplication during analysis.</p> <p>Idea: sample xN duplicates from each of the N kernels. Let resampling reduce it to N.</p> <p>Additional idea: employ w-adjustment to obtain N unique particles, without jittering.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.PFxN_EnKF","title":"<code>PFxN_EnKF</code>","text":"<p>Particle filter with EnKF-based proposal, q.</p> <p>Also employs xN duplication, as in PFxN.</p> <p>Recall that the proposals: Opt.: q_n(x) = c_n\u00b7N(x|x_n,Q     )\u00b7N(y|Hx,R)  (1) EnKF: q_n(x) = c_n\u00b7N(x|x_n,bar{B})\u00b7N(y|Hx,R)  (2) with c_n = p(y|x^{k-1}_n) being the composite proposal-analysis weight, and with Q possibly from regularization (rather than actual model noise).</p> <p>Here, we will use the posterior mean of (2) and cov of (1). Or maybe we should use x_a^n distributed according to a sqrt update?</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.PartFilt","title":"<code>PartFilt</code>","text":"<p>Particle filter \u2261 Sequential importance (re)sampling SIS (SIR).</p> <p>Refs: wikle2007, van2009, chen2003</p> <p>This is the bootstrap version: the proposal density is just</p> \\[ q(x_{0:t} \\mid y_{1:t}) = p(x_{0:t}) = p(x_t \\mid x_{t-1}) p(x_{0:t-1}) \\] <p>Tuning settings:</p> <ul> <li>NER: Trigger resampling whenever <code>N_eff &lt;= N*NER</code>.    If resampling with some variant of 'Multinomial',    no systematic bias is introduced.</li> <li>qroot: \"Inflate\" (anneal) the proposal noise kernels    by this root to increase diversity.    The weights are updated to maintain un-biased-ness.    See chen2003, section VI-M.2</li> </ul>"},{"location":"reference/da_methods/particle/#da_methods.particle.particle_method","title":"<code>particle_method</code>","text":"<p>Declare default particle arguments.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.all_but_1_is_None","title":"<code>all_but_1_is_None(*args)</code>","text":"<p>Check if only 1 of the items in list are Truthy.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.auto_bandw","title":"<code>auto_bandw(N, M)</code>","text":"<p>Optimal bandwidth (not bandwidth^2), as per Scott's rule-of-thumb.</p> <p>Refs: doucet2001sequential section 12.2.2, [Wik17]_ section \"Rule_of_thumb\"</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.mask_unique_of_sorted","title":"<code>mask_unique_of_sorted(idx)</code>","text":"<p>Find unique values assuming <code>idx</code> is sorted.</p> <p>NB: returns a mask which is <code>True</code> at <code>[i]</code> iff <code>idx[i]</code> is not unique.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.raw_C12","title":"<code>raw_C12(E, w)</code>","text":"<p>Compute the 'raw' matrix-square-root of the ensemble' covariance.</p> <p>The weights are used both for the mean and anomalies (raw sqrt).</p> <p>Note: anomalies (and thus cov) are weighted, and also computed based on a weighted mean.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.regularize","title":"<code>regularize(C12, E, idx, no_uniq_jitter)</code>","text":"<p>Jitter (add noise).</p> <p>After resampling some of the particles will be identical. Therefore, if noise.is_deterministic: some noise must be added. This is adjusted by the regularization 'reg' factor (so-named because Dirac-deltas are approximated  Gaussian kernels), which controls the strength of the jitter. This causes a bias. But, as N\u2192\u221e, the reg. bandwidth\u21920, i.e. bias\u21920. Ref: doucet2001sequential, section 12.2.2.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.resample","title":"<code>resample(w, kind='Systematic', N=None, wroot=1.0)</code>","text":"<p>Multinomial resampling.</p> <p>Refs: doucet2009, van2009, liu2001theoretical.</p> <ul> <li> <p>kind: 'Systematic', 'Residual' or 'Stochastic'.   'Stochastic' corresponds to <code>rng.choice</code> or <code>rng.multinomial</code>.   'Systematic' and 'Residual' are more systematic (less stochastic)   varaitions of 'Stochastic' sampling.   Among the three, 'Systematic' is fastest, introduces the least noise,   and brings continuity benefits for localized particle filters,   and is therefore generally prefered.   Example: see <code>docs/images/snippets/ex_resample.py</code>.</p> </li> <li> <p>N can be different from len(w)   (e.g. in case some particles have been elimintated).</p> </li> <li> <p>wroot: Adjust weights before resampling by this root to   promote particle diversity and mitigate thinning.   The outcomes of the resampling are then weighted to maintain un-biased-ness.   Ref: liu2001theoretical, section 3.1</p> </li> </ul> <p>Note: (a) resampling methods are beneficial because they discard low-weight (\"doomed\") particles and reduce the variance of the weights. However, (b) even unbiased/rigorous resampling methods introduce noise; (increases the var of any empirical estimator, see [1], section 3.4). How to unify the seemingly contrary statements of (a) and (b) ? By recognizing that we're in the sequential/dynamical setting, and that future variance may be expected to be lower by focusing on the high-weight particles which we anticipate will have more informative (and less variable) future likelihoods.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.reweight","title":"<code>reweight(w, lklhd=None, logL=None, innovs=None)</code>","text":"<p>Do Bayes' rule (for the empirical distribution of an importance sample).</p> <p>Do computations in log-space, for at least 2 reasons:</p> <ul> <li>Normalization: will fail if <code>sum==0</code> (if all innov's are large).</li> <li>Num. precision: <code>lklhd*w</code> should have better precision in log space.</li> </ul> <p>Output is non-log, for the purpose of assessment and resampling.</p> <p>If input is 'innovs', then \\(\\(\\text{likelihood} = \\mathcal{N}(\\text{innovs}|0,I)\\)\\).</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.sample_quickly_with","title":"<code>sample_quickly_with(C12, N=None)</code>","text":"<p>Gaussian sampling in the quickest fashion.</p> <p>Method depends on the size of the colouring matrix <code>C12</code>.</p>"},{"location":"reference/da_methods/particle/#da_methods.particle.trigger_resampling","title":"<code>trigger_resampling(w, NER, stat_args)</code>","text":"<p>Return boolean: N_effective &lt;= threshold. Also write self.stats.</p>"},{"location":"reference/da_methods/variational/","title":"variational","text":"<p>Variational DA methods (iEnKS, 4D-Var, etc).</p>"},{"location":"reference/da_methods/variational/#da_methods.variational.Var4D","title":"<code>Var4D</code>","text":"<p>4D-Var.</p> <p>Cycling scheme is same as in iEnKS (i.e. the shift is always 1*ko).</p> <p>This implementation does NOT do gradient decent (nor quasi-Newton) in an inner loop, with simplified models. Instead, each (outer) iteration is computed non-iteratively as a Gauss-Newton step. Thus, since the full (approximate) Hessian is formed, there is no benefit to the adjoint trick (back-propagation). =&gt; This implementation is not suited for big systems.</p> <p>Incremental formulation is used, so the formulae look like the ones in iEnKS.</p>"},{"location":"reference/da_methods/variational/#da_methods.variational.iEnKS","title":"<code>iEnKS</code>","text":"<p>Iterative EnKS.</p> <p>Special cases: EnRML, ES-MDA, iEnKF, EnKF raanes2019.</p> <p>As in bocquet2014, optimization uses Gauss-Newton. See bocquet2012a for Levenberg-Marquardt. If MDA=True, then there's not really any optimization, but rather Gaussian annealing.</p> <p>Args:   upd_a (str):     Analysis update form (flavour). One of:</p> <pre><code>- \"Sqrt\"   : as in ETKF  , using a deterministic matrix square root transform.\n- \"PertObs\": as in EnRML , using stochastic, perturbed-observations.\n- \"Order1\" : as in DEnKF of [sakov2008b][].\n</code></pre> <p>Lag:     Length of the DA window (DAW, multiples of dko, i.e. cycles).</p> <pre><code>- Lag=1 (default) =&gt; iterative \"filter\" iEnKF [sakov2012a][].\n- Lag=0           =&gt; maximum-likelihood filter [zupanski2005][].\n</code></pre> <p>Shift : How far (in cycles) to slide the DAW.           Fixed at 1 for code simplicity.</p> <p>nIter : Maximal num. of iterations used (&gt;=1). Default: 10.           Supporting nIter==0 requires more code than it's worth.</p> <p>wtol  : Rel. tolerance defining convergence.           Default: 0 =&gt; always do nIter iterations.           Recommended: 1e-5.</p> <p>MDA   : Use iterations of the \"multiple data assimlation\" type.           Ref emerick2012</p> <p>bundle: Use finite-diff. linearization instead of of least-squares regression.           Makes the iEnKS very much alike the iterative, extended KF (IEKS).</p> <p>xN    : If set, use EnKF_N() pre-inflation. See further documentation there.</p> <p>Total number of model simulations (of duration dto): N * (nIter*Lag + 1). (due to boundary cases: only asymptotically valid)</p> <p>Refs: bocquet2012a, bocquet2013, bocquet2014.</p>"},{"location":"reference/da_methods/variational/#da_methods.variational.var_method","title":"<code>var_method</code>","text":"<p>Declare default variational arguments.</p>"},{"location":"reference/da_methods/variational/#da_methods.variational.iEnKS_update","title":"<code>iEnKS_update(upd_a, E, DAW, HMM, stats, EPS, y, time, Rm12, xN, MDA, threshold)</code>","text":"<p>Perform the iEnKS update.</p> <p>This implementation includes several flavours and forms, specified by <code>upd_a</code> (See <code>iEnKS</code>)</p>"},{"location":"reference/mods/","title":"mods","text":"<p>Contains models included with DAPPER.</p> <p>See the README section on test cases (models) for a table overview of the included models.</p>"},{"location":"reference/mods/#mods--defining-your-own-model","title":"Defining your own model","text":"<p>Below is a sugested structuring followed by most models already within DAPPER. However, you are free to organize your model as you see fit, as long as it culminates in the definition of one or more <code>mods.HiddenMarkovModel</code>. For the sake of modularity, try not to import stuff from DAPPER outside of <code>mods</code> and <code>tools.liveplotting</code>.</p> <ul> <li>Make a directory: <code>my_model</code>. It does not have to reside within the <code>dapper/mods</code> folder,   but make sure to look into some of the other dirs thereunder as examples,   for example <code>dapper/mods/DoublePendulum</code>.</li> <li>Make a file: <code>my_model/__init__.py</code> to hold the core workings of the model.   Further details are given below, but the   main work lies in defining a <code>step(x, t, dt)</code> function   (you can name it however you like, but <code>step</code> is the convention),   to implement the dynamical model/system mapping the state <code>x</code>   from one time <code>t</code> to another <code>t + dt</code>.</li> <li>Make a file: <code>my_model/demo.py</code> to run <code>step</code> and visually showcase   a simulation of the model without any DA, and verify it's working.</li> <li>Make a file: <code>my_model/my_settings_1.py</code> that defines   (or \"configures\", since there is usually little programming logic and flow taking place)   a complete <code>mods.HiddenMarkovModel</code> ready for a synthetic experiment   (also called \"twin experiment\" or OSSE).</li> <li>Once you've made some experiments you believe are noteworthy you should add a   \"suggested settings/tunings\" section in comments at the bottom of   <code>my_model/my_settings_1.py</code>, listing some of the relevant DA method   configurations that you tested, along with the RMSE (or other stats) that   you obtained for those methods.  You will find plenty of examples already in   DAPPER, used for cross-referenced with literature to verify the workings of DAPPER   (and the reproducibility of publications).</li> </ul>"},{"location":"reference/mods/#mods--details-on-my_model__init__py","title":"Details on <code>my_model/__init__.py</code>","text":"<ul> <li> <p>The <code>step</code> function must support 2D-array (i.e. ensemble)   and 1D-array (single realization) input, and return output of the same   number of dimensions (as the input).   See</p> <ul> <li><code>mods.Lorenz63</code>: use of <code>ens_compatible</code>.</li> <li><code>mods.Lorenz96</code>: use of relatively clever slice notation.</li> <li> <p><code>mods.LorenzUV</code>: use of cleverer slice notation: <code>...</code> (ellipsis).   Consider pre-defining the slices like so:   <pre><code>iiX = (..., slice(None, Nx))\niiP = (..., slice(Nx, None))\n</code></pre>   to abbreviate the indexing elsewhere.</p> </li> <li> <p><code>mods.QG</code>: use of parallelized for loop (map).</p> </li> </ul> <p>Note</p> <p>To begin with, test whether the model works on 1 realization, before running it with several (simultaneously). Also, start with a small integration time step, before using more efficient/adventurous time steps. Note that the time step might need to be shorter in assimilation, because it may cause instabilities.</p> <p>Note</p> <p>Most models are defined using simple procedural style. However, <code>mods.LorenzUV</code> and <code>mods.QG</code> use OOP, which is perhaps more robust when different control-variable settings are to be investigated. The choice is yours.</p> <p>In parameter estimation problems, the parameters are treated as input variables to the \"forward model\". This does not necessarily require OOP. See <code>docs/examples/param_estim.py</code>.</p> </li> <li> <p>Optional: define a suggested/example initial state, <code>x0</code>.   This facilitates the specification of initial conditions for different synthetic   experiments, as random variables centred on <code>x0</code>.  It is also a   convenient way just to specify the system size as <code>len(x0)</code>.  In many   experiments, the specific value of <code>x0</code> does not matter, because most   systems are chaotic, and the average of the stats are computed only for   <code>time &gt; BurnIn &gt; 0</code>, which will not depend on <code>x0</code> if the experiment is   long enough.  Nevertheless, it's often convenient to pre-define a point   on the attractor, or basin, or at least ensure \"physicality\", for   quicker spin-up (burn-in).</p> </li> <li> <p>Optional: define a number called <code>Tplot</code> which defines   the (sliding) time window used by the liveplotting of diagnostics.</p> </li> <li> <p>Optional: To use the (extended) Kalman filter, or 4D-Var,   you will need to define the model linearization, typically called <code>dstep_dx</code>.   Note: this only needs to support 1D input (single realization).</p> </li> </ul> <p>Modules:</p> Name Description <code>DoublePendulum</code> <p>The motion of a pendulum with another pendulum attached to its end.</p> <code>Id</code> <p>The identity model (that does nothing, i.e. sets <code>output = input</code>).</p> <code>Ikeda</code> <p>The \"Ikeda map\" is a discrete-time dynamical system of size 2.</p> <code>KS</code> <p>Kuramoto-Sivashinsky (KS) system: the simplest (?) PDE admitting chaos.</p> <code>LA</code> <p>Linear advection (i.e. translation) in 1D.</p> <code>Lorenz05</code> <p>A multi-scale, smooth version of the classic Lorenz-96.</p> <code>Lorenz63</code> <p>The classic exhibitor of chaos, consisting of 3 coupled ODEs.</p> <code>Lorenz84</code> <p>A chaotic system of size 3, like Lorenz-63, but with +complex geometry.</p> <code>Lorenz96</code> <p>A 1D emulator of chaotic atmospheric behaviour.</p> <code>Lorenz96s</code> <p>A perfect-random version of Lorenz-96.</p> <code>LorenzUV</code> <p>The 2-scale/layer/speed coupled version of Lorenz-96.</p> <code>LotkaVolterra</code> <p>The generalized predator-prey model, with settings for chaotic dynamics.</p> <code>QG</code> <p>Quasi-geostraphic 2D flow. Described in detail by sakov2008b.</p> <code>VL20</code> <p>Single-scale Lorenz-96 with an added thermodynamic component.</p> <code>explore_props</code> <p>Estimtate the Lyapunov spectrum and other props. of the dynamics of the models.</p> <code>integration</code> <p>Time stepping (integration) tools.</p> <code>pb</code> <p>Make <code>progbar</code> (wrapper around <code>tqdm</code>) and <code>read1</code>.</p> <code>utils</code> <p>Utilities to help define hidden Markov models.</p>"},{"location":"reference/mods/#mods.HiddenMarkovModel","title":"<code>HiddenMarkovModel</code>","text":"<p>               Bases: <code>NicePrint</code></p> <p>Container class (with some embellishments) for a Hidden Markov Model (HMM).</p> <p>Should contain the details necessary to run synthetic DA experiments, also known as \"twin experiment\", or OSSE (observing system simulation experiment). The synthetic truth and observations may then be obtained by running <code>.simulate</code>.</p> <p>Note</p> <p>Each model included with DAPPER comes with several examples of model settings from the literature. See, for example, <code>mods.Lorenz63.sakov2012</code>.</p> <p>Warning</p> <p>These example configs do not necessarily hold a high programming standard, as they may have been whipped up at short notice to replicate some experiments, and are not intended for re-use. Nevertheless, sometimes they are re-used by another configuration script, leading to a major gotcha/pitfall: changes made to the imported <code>HMM</code> (or the model's module itself) also impact the original object (since they are mutable and thereby referenced).  This usually isn't an issue, since one rarely imports two/more separate configurations. However, the test suite imports all configurations, which might then unintentionally interact. To avoid this, you should use the <code>copy</code> method of the <code>HMM</code> before making any changes to it.</p>"},{"location":"reference/mods/#mods.HiddenMarkovModel.__init__","title":"<code>__init__(Dyn, Obs, tseq, X0, liveplotters=None, sectors=None, name=None, **kwargs)</code>","text":"<p>Initialize.</p> <p>Parameters:</p> Name Type Description Default <code>Dyn</code> <code>Operator or dict</code> <p>Operator for the dynamics.</p> required <code>Obs</code> <code>Operator or TimeDependentOperator or dict</code> <p>Operator for the observations Can also be time-dependent, ref <code>TimeDependentOperator</code>.</p> required <code>tseq</code> <code>Chronology</code> <p>Time sequence of the HMM process.</p> required <code>X0</code> <code>RV</code> <p>Random distribution of initial condition</p> required <code>liveplotters</code> <code>list</code> <p>A list of tuples. See example use in function <code>LPs</code> of <code>mods.Lorenz63</code>. - The first element of the tuple determines if the liveplotter is shown by default. If <code>False</code>, the liveplotter is only shown when included among the <code>liveplots</code> argument of <code>assimilate</code> - The second element in the tuple gives the corresponding liveplotter function/class.</p> <code>None</code> <code>sectors</code> <code>dict</code> <p>Labelled indices referring to parts of the state vector. When defined, field-mean statistics are computed for each sector. Example use can be found in  <code>docs/examples/param_estim.py</code> and <code>dapper/mods/Lorenz96/miyoshi2011.py</code></p> <code>None</code> <code>name</code> <code>str</code> <p>Label for the <code>HMM</code>.</p> <code>None</code>"},{"location":"reference/mods/#mods.HiddenMarkovModel.simulate","title":"<code>simulate(desc='Truth &amp; Obs')</code>","text":"<p>Generate synthetic truth and observations.</p>"},{"location":"reference/mods/#mods.Operator","title":"<code>Operator</code>","text":"<p>               Bases: <code>NicePrint</code></p> <p>Container for the dynamical and the observational maps.</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>int</code> <p>Length of output vectors.</p> required <code>model</code> <code>function</code> <p>The actual operator.</p> <code>None</code> <code>noise</code> <code>RV</code> <p>The associated additive noise. The noise can also be a scalar or an array, producing <code>GaussRV(C=noise)</code>.</p> <code>None</code> Note <p>Any remaining keyword arguments are written to the object as attributes.</p>"},{"location":"reference/mods/#mods.TimeDependentOperator","title":"<code>TimeDependentOperator</code>","text":"<p>Wrapper for <code>Operator</code> that enables time dependence.</p> <p>The time instance should be specified by <code>ko</code>, i.e. the index of an observation time.</p> <p>Examples: <code>docs/examples/time-dep-obs-operator.py</code> and <code>dapper/mods/QG/sakov2008.py</code>.</p>"},{"location":"reference/mods/#mods.TimeDependentOperator.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Can be initialized like <code>Operator</code>, in which case the resulting object will always return the same <code>Operator</code> nomatter the input time.</p> <p>If initialized with 1 argument: <code>dict(time_dependent=func)</code> then <code>func</code> must return an <code>Operator</code> object.</p>"},{"location":"reference/mods/explore_props/","title":"explore_props","text":"<p>Estimtate the Lyapunov spectrum and other props. of the dynamics of the models.</p> <p>Modules:</p> Name Description <code>series</code> <p>Time series management and processing.</p> <code>viz</code> <p>Plot utilities.</p>"},{"location":"reference/mods/integration/","title":"integration","text":"<p>Time stepping (integration) tools.</p>"},{"location":"reference/mods/integration/#mods.integration.FD_Jac","title":"<code>FD_Jac(func, eps=1e-07)</code>","text":"<p>Finite-diff approx. of Jacobian of <code>func</code>.</p> <p>The function <code>func(x)</code> must be compatible with <code>x.ndim == 1</code> and <code>2</code>, where, in the 2D case, each row is seen as one function input.</p> <p>Returns:</p> Type Description <code>function</code> <p>The first input argument is that of which the derivative is taken.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dstep_dx = FD_Jac(step)\n</code></pre>"},{"location":"reference/mods/integration/#mods.integration.integrate_TLM","title":"<code>integrate_TLM(TLM, dt, method='approx')</code>","text":"<p>Compute the resolvent.</p> <p>The resolvent may also be called</p> <ul> <li>the Jacobian of the step func.</li> <li>the integral of (with M as the TLM):</li> </ul> \\[ \\frac{d U}{d t} = M U, \\quad U_0 = I . \\] <p>The tangent linear model (TLM)</p> <p>is assumed constant (for each <code>method</code> below).</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <ul> <li><code>'approx'</code>  : derived from the forward-euler scheme.</li> <li><code>'rk4'</code>     : higher-precision approx.</li> <li><code>'analytic'</code>: exact.</li> </ul> <p>Warning</p> <p>\"'analytic' typically requries higher inflation in the ExtKF.\"</p> <code>'approx'</code> See Also <p><code>FD_Jac</code>.</p>"},{"location":"reference/mods/integration/#mods.integration.rk4","title":"<code>rk4(f, x, t, dt, stages=4, s=0)</code>","text":"<p>Runge-Kutta (explicit, non-adaptive) numerical (S)ODE solvers.</p> <p>For ODEs, the order of convergence equals the number of <code>stages</code>.</p> <p>For SDEs with additive noise (<code>s&gt;0</code>), the order of convergence (both weak and strong) is 1 for <code>stages</code> equal to 1 or 4. These correspond to the classic Euler-Maruyama scheme and the Runge-Kutta scheme for S-ODEs respectively, see grudzien2020a for a DA-specific discussion on integration schemes and their discretization errors.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>function</code> <p>The time derivative of the dynamical system. Must be of the form <code>f(t, x)</code></p> required <code>x</code> <code>ndarray or float</code> <p>State vector of the forcing term</p> required <code>t</code> <code>float</code> <p>Starting time of the integration</p> required <code>dt</code> <code>float</code> <p>Integration time step.</p> required <code>stages</code> <code>int</code> <p>The number of stages of the RK method. When <code>stages=1</code>, this becomes the Euler (-Maruyama) scheme. Default: 4.</p> <code>4</code> <code>s</code> <code>float</code> <p>The diffusion coeffient (std. dev) for models with additive noise. Default: 0, yielding deterministic integration.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>State vector at the new time, <code>t+dt</code></p>"},{"location":"reference/mods/integration/#mods.integration.with_recursion","title":"<code>with_recursion(func, prog=False)</code>","text":"<p>Make function recursive in its 1<sup>st</sup> arg.</p> <p>Return a version of <code>func</code> whose 2<sup>nd</sup> argument (<code>k</code>) specifies the number of times to times apply func on its output.</p> <p>!!! warning Only the first argument to <code>func</code> will change,     so, for example, if <code>func</code> is <code>step(x, t, dt)</code>,     it will get fed the same <code>t</code> and <code>dt</code> at each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>function</code> <p>Function to recurse with.</p> required <code>prog</code> <code>bool or str</code> <p>Enable/Disable progressbar. If <code>str</code>, set its name to this.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fun_k</code> <code>function</code> <p>A function that returns the sequence generated by recursively running <code>func</code>, i.e. the trajectory of system's evolution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def dxdt(x):\n...     return -x\n&gt;&gt;&gt; step_1  = with_rk4(dxdt, autonom=True)\n&gt;&gt;&gt; step_k  = with_recursion(step_1)\n&gt;&gt;&gt; x0      = np.arange(3)\n&gt;&gt;&gt; x7      = step_k(x0, 7, t0=np.nan, dt=0.1)[-1]\n&gt;&gt;&gt; x7_true = x0 * np.exp(-0.7)\n&gt;&gt;&gt; np.allclose(x7, x7_true)\nTrue\n</code></pre>"},{"location":"reference/mods/integration/#mods.integration.with_rk4","title":"<code>with_rk4(dxdt, autonom=False, stages=4, s=0)</code>","text":"<p>Wrap <code>dxdt</code> in <code>rk4</code>.</p>"},{"location":"reference/mods/utils/","title":"utils","text":"<p>Utilities to help define hidden Markov models.</p>"},{"location":"reference/mods/utils/#mods.utils.NamedFunc","title":"<code>NamedFunc</code>","text":"<p>Provides custom repr for functions.</p>"},{"location":"reference/mods/utils/#mods.utils.Id_Obs","title":"<code>Id_Obs(Nx)</code>","text":"<p>Specify identity observations of entire state.</p> <p>It is not a function of time.</p> <p>Parameters:</p> Name Type Description Default <code>Nx</code> <code>int</code> <p>Length of state vector</p> required <p>Returns:</p> Name Type Description <code>Obs</code> <code>dict</code> <p>Observation operator including size of the observation space, observation operator/model and tangent linear observation operator</p>"},{"location":"reference/mods/utils/#mods.utils.Id_op","title":"<code>Id_op()</code>","text":"<p>Id operator (named). Returns first argument.</p>"},{"location":"reference/mods/utils/#mods.utils.direct_obs_matrix","title":"<code>direct_obs_matrix(Nx, obs_inds)</code>","text":"<p>Generate matrix that \"picks\" state elements <code>obs_inds</code> out of <code>range(Nx)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Nx</code> <code>int</code> <p>Length of state vector</p> required <code>obs_inds</code> <code>ndarray</code> <p>Indices of elements of the state vector that are (directly) observed.</p> required <p>Returns:</p> Name Type Description <code>H</code> <code>ndarray</code> <p>The observation matrix for direct partial observations.</p>"},{"location":"reference/mods/utils/#mods.utils.ens_compatible","title":"<code>ens_compatible(func)</code>","text":"<p>Decorate to transpose before and after, i.e. <code>func(input.T).T</code>.</p> <p>This is helpful to make functions compatible with both 1d and 2d ndarrays.</p> <p>This is not <code>the_way\u2122</code></p> <p>Other tricks (ref <code>mods</code>) are sometimes more practical.</p> <p>Examples:</p> <p><code>mods.Lorenz63.dxdt</code>, <code>mods.DoublePendulum.dxdt</code></p> See Also <p>np.atleast_2d, np.squeeze, np.vectorize</p>"},{"location":"reference/mods/utils/#mods.utils.linear_model_setup","title":"<code>linear_model_setup(ModelMatrix, dt0)</code>","text":"<p>Make the Dyn/Obs field of a HMM representing a linear model.</p> <p>Let M be the model matrix. Then</p> \\[     x(t+dt) = M^{dt/dt0} x(t), \\] <p>i.e.</p> \\[ \\frac{dx}{dt} = \\frac{\\log(M)}{dt0} x(t). \\] <p>In typical use, <code>dt0==dt</code> (where <code>dt</code> is defined by the chronology). Anyways, <code>dt</code> must be an integer multiple of <code>dt0</code>.</p> <p>Returns:</p> Type Description <code>A `dict` with keys: 'M', 'model', 'linear'.</code>"},{"location":"reference/mods/utils/#mods.utils.linspace_int","title":"<code>linspace_int(Nx, Ny, periodic=True)</code>","text":"<p>Provide a range of <code>Ny</code> equispaced integers between <code>0</code> and <code>Nx-1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>Nx</code> <code>int</code> <p>Range of integers</p> required <code>Ny</code> <code>int</code> <p>Number of integers</p> required <code>periodic</code> <code>bool</code> <p>Whether the vector is periodic. Determines if <code>Nx == 0</code>. Default: True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>integers</code> <code>ndarray</code> <p>The list of integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; linspace_int(10, 10)\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; linspace_int(10, 4)\narray([0, 2, 5, 7])\n&gt;&gt;&gt; linspace_int(10, 5)\narray([0, 2, 4, 6, 8])\n</code></pre>"},{"location":"reference/mods/utils/#mods.utils.name_func","title":"<code>name_func(name)</code>","text":"<p>Decorator for creating NamedFunc.</p>"},{"location":"reference/mods/utils/#mods.utils.partial_Id_Obs","title":"<code>partial_Id_Obs(Nx, obs_inds)</code>","text":"<p>Specify identity observations of a subset of obs. indices.</p> <p>It is not a function of time.</p> <p>Parameters:</p> Name Type Description Default <code>Nx</code> <code>int</code> <p>Length of state vector</p> required <code>obs_inds</code> <code>ndarray</code> <p>The observed indices.</p> required <p>Returns:</p> Name Type Description <code>Obs</code> <code>dict</code> <p>Observation operator including size of the observation space, observation operator/model and tangent linear observation operator</p>"},{"location":"reference/mods/DoublePendulum/","title":"DoublePendulum","text":"<p>The motion of a pendulum with another pendulum attached to its end.</p> <p>Refs:</p> <ul> <li>Wiki</li> <li>MPL   which is based on this c code:   USYD</li> </ul> <p>Modules:</p> Name Description <code>demo</code> <p>Demonstrate the Double-Pendulum model.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p> <code>settings101</code> <p>Settings that produce somewhat interesting/challenging DA problems.</p>"},{"location":"reference/mods/DoublePendulum/#mods.DoublePendulum.dxdt","title":"<code>dxdt(x)</code>","text":"<p>Evolution equation (coupled ODEs) specifying the dynamics.</p>"},{"location":"reference/mods/DoublePendulum/#mods.DoublePendulum.energy","title":"<code>energy(x)</code>","text":"<p>Compute total energy of system.</p>"},{"location":"reference/mods/DoublePendulum/demo/","title":"demo","text":"<p>Demonstrate the Double-Pendulum model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/DoublePendulum/settings101/","title":"settings101","text":"<p>Settings that produce somewhat interesting/challenging DA problems.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Id/","title":"Id","text":"<p>The identity model (that does nothing, i.e. sets <code>output = input</code>).</p> <p>This means that the state dynamics are just Brownian motion.</p> <p>Next to setting the state to a constant, this is the simplest model you can think of.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Ikeda/","title":"Ikeda","text":"<p>The \"Ikeda map\" is a discrete-time dynamical system of size 2.</p> <p>Source: Wiki and Colin Grudzien.</p> <p>See <code>demo</code> for more info.</p> <p>Modules:</p> Name Description <code>LP</code> <p>On-line (live) plots of the DA process for various models and methods.</p> <code>demo</code> <p>Demonstrate the Ikeda map.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p> <code>some_settings_01</code> <p>Settings that produce somewhat interesting/challenging DA problems.</p>"},{"location":"reference/mods/Ikeda/#mods.Ikeda.aux","title":"<code>aux(x, y)</code>","text":"<p>Comps used both by step and its jacobian.</p>"},{"location":"reference/mods/Ikeda/demo/","title":"demo","text":"<p>Demonstrate the Ikeda map.</p> <p>Plot settings inspired by Wikipedia.</p> <p>Modules:</p> Name Description <code>core</code> <p>The \"Ikeda map\" is a discrete-time dynamical system of size 2.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Ikeda/some_settings_01/","title":"some_settings_01","text":"<p>Settings that produce somewhat interesting/challenging DA problems.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/KS/","title":"KS","text":"<p>Kuramoto-Sivashinsky (KS) system: the simplest (?) PDE admitting chaos.</p> <p>Defined by:</p> <pre><code>u_t = -u*u_x - u_xx - u_xxxx\n</code></pre> <ul> <li>See compare_schemes.py for a comparison of time-step integration schemes.</li> <li>See demo.py for further description.</li> </ul> <p>Modules:</p> Name Description <code>bocquet2019</code> <p>Settings as in bocquet2019a.</p> <code>compare_schemes</code> <p>Compare integration time-stepping schemes for KS equation.</p> <code>demo</code> <p>Demonstrate the Kuramoto-Sivashinsky (KS) system.</p>"},{"location":"reference/mods/KS/#mods.KS.Model","title":"<code>Model(dt=0.25, DL=32, Nx=128)</code>","text":"<p>Define <code>step</code>, <code>x0</code>, <code>etc</code>. Alternative schemes (<code>step_XXX</code>) also implemented.</p>"},{"location":"reference/mods/KS/bocquet2019/","title":"bocquet2019","text":"<p>Settings as in bocquet2019a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/KS/compare_schemes/","title":"compare_schemes","text":"<p>Compare integration time-stepping schemes for KS equation.</p> <p>Conclusions: 'ETD-RK4' is superior to 'SI-RK3',</p> <ul> <li>Superiority deteriorates when adding noise to ICs.</li> <li>Superiority fairly independent of N.</li> <li>The naive methods (rk4, rk1) are quite terrible.</li> </ul>"},{"location":"reference/mods/KS/demo/","title":"demo","text":"<p>Demonstrate the Kuramoto-Sivashinsky (KS) system.</p>"},{"location":"reference/mods/LA/","title":"LA","text":"<p>Linear advection (i.e. translation) in 1D.</p> <p>Optimal solution provided by Kalman filter (ExtKF). System is typically used with a relatively large size (Nx=1000), but initialized with a moderate wavenumber (k), which a DA method should hopefully be able to exploit.</p> <p>The system (and the impact on the DA) can also be adjusted by selecting non-optimal time steps, and/or various noise configs.</p> <p>A summary for the purpose of DA is provided in section 3.3 of thesis found at ora.ox.ac.uk/objects/uuid:9f9961f0-6906-4147-a8a9-ca9f2d0e4a12</p> <p>Modules:</p> Name Description <code>demo</code> <p>Demonstrate the Linear Advection (LA) model.</p> <code>evensen2009</code> <p>A mix of evensen2009a and sakov2008a.</p> <code>raanes2015</code> <p>Reproduce results from Fig. 2 of raanes2014.</p> <code>small</code> <p>Smaller version.</p>"},{"location":"reference/mods/LA/#mods.LA.Fmat","title":"<code>Fmat(Nx, c, dx, dt)</code>","text":"<p>Generate transition matrix.</p> <ul> <li><code>Nx</code> - System size</li> <li><code>c</code>  - Velocity of wave. Wave travels to the rigth for <code>c&gt;0</code>.</li> <li><code>dx</code> - Grid spacing</li> <li><code>dt</code> - Time step</li> </ul> <p>Note that the 1<sup>st</sup>-order upwind scheme used here is exact (vis-a-vis the analytic solution) only for <code>dt = abs(dx/c)</code>, in which case it corresponds to <code>np.roll(x,1,axis=x.ndim-1)</code>, i.e. circshift in Matlab.</p>"},{"location":"reference/mods/LA/#mods.LA.basis_vector","title":"<code>basis_vector(Nx, k)</code>","text":"<p>Generate basis vectors.</p> <ul> <li>Nx - state vector length</li> <li>k  - max wavenumber (wavelengths to fit into interval 1:Nx)</li> </ul>"},{"location":"reference/mods/LA/#mods.LA.homogeneous_1D_cov","title":"<code>homogeneous_1D_cov(M, d, kind='Expo')</code>","text":"<p>Generate a covariance matrix for a 1D homogenous random field.</p> <p>Generate initial correlations for Linear Advection experiment.</p> <p><code>d</code> - decorr length, where the <code>unit distance = M(i)-M(i-1)</code> for all <code>i</code>.</p>"},{"location":"reference/mods/LA/#mods.LA.sinusoidal_sample","title":"<code>sinusoidal_sample(Nx, k, N)</code>","text":"<p>Generate N basis vectors, and center them.</p> <p>The centring is not naturally a part of the basis generation, but serves to avoid the initial transitory regime if the model is dissipative(, and more ?).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; E = sinusoidal_sample(100, 4, 5)\n&gt;&gt;&gt; plt.plot(E.T)\n</code></pre>"},{"location":"reference/mods/LA/demo/","title":"demo","text":"<p>Demonstrate the Linear Advection (LA) model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LA/evensen2009/","title":"evensen2009","text":"<p>A mix of evensen2009a and sakov2008a.</p> <p>Note</p> <p>Since there is no noise, and the system is stable, the rmse's from this HMM go to zero as <code>T</code> goes to infinity. Thus, benchmarks largely depend on the initial error, and so these absolute rmse values are not so useful for quantatative evaluation of DA methods. For that purpose, see <code>mods.LA.raanes2015</code> instead.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LA/raanes2015/","title":"raanes2015","text":"<p>Reproduce results from Fig. 2 of raanes2014.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LA/small/","title":"small","text":"<p>Smaller version.</p> <p>This is convenient because it's faster in liveplotting, and convenient for debugging, e.g., <code>tests.test_iEnKS</code>.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz05/","title":"Lorenz05","text":"<p>A multi-scale, smooth version of the classic Lorenz-96.</p> <p>This is an implementation of \"Model III\" of lorenz2005a.</p> <p>Similar to <code>mods.LorenzUV</code> this model is designed to contain two different scales. However, in \"Model III\" the two scales are not kept separate, but superimposed, and the large scale variables are (adjustably) spatially smooth.</p> <p>Interestingly, the model is known as \"Lorenz 04\" in DART, where it was coded by Hansen (colleague of Lorenz) in 2004 (prior to publication).</p> <p>Special cases of this model are:</p> <ul> <li>Set <code>J=1</code> to get \"Model II\".</li> <li>Set <code>K=1</code> (and <code>J=1</code>) to get \"Model I\",   which is the same as the Lorenz-96 model.</li> </ul> <p>An implementation using explicit for-loops can be found in commit 6193532b . It uses numba (pip install required) for speed gain, but is still very slow. The implementation hereunder uses efficient numpy vectorization =&gt; much faster.</p> <p>With rk4 the largest stable time step (for free run) seems to be somewhere around what Lorenz used, namely <code>dt=0.05/12</code>.</p> <p>Modules:</p> Name Description <code>demo</code> <p>Demonstrate the Lorenz-05 model.</p>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.Model","title":"<code>Model</code>","text":"<p>The model configuration.</p> <p>Functionality that does not directly depend on the model parameters has been left outside of this class.</p> <p>Using OOP (rather than module-level encapsulation) facilitates working with multiple parameter settings simultaneously.</p>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.Model.decompose","title":"<code>decompose(z)</code>","text":"<p>Split <code>z</code> into <code>x</code> and <code>y</code> fields, where <code>x</code> is the large-scale component.</p>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.boxcar","title":"<code>boxcar(x, n, method='direct')</code>","text":"<p>Moving average (boxcar filter) on <code>x</code> using <code>n</code> nearest (periodically) elements.</p> <p>For symmetry, if <code>n</code> is pair, the actual number of elements used is <code>n+1</code>, and the outer elements weighted by <code>0.5</code> to compensate for the <code>+1</code>.</p> <p>This is the modified sum of lorenz2005a, used e.g. in eqn. 9. For intuition: this type of summation (and the associated Sigma prime notation) may also be found for the \"Trapezoidal rule\" and in the inverse DFT used in spectral methods on a periodic domain.</p> <p>Apart from this weighting, this constitutes merely a boxcar filter. There are of course several well-known implementations.  The computational suggestion suggested by Lorenz below eqn 10 could maybe be implemented (with vectorisation) using <code>cumsum</code>, but this seems tricky due to weighting and periodicity.</p> <p>1 2 3</p> <p>In testing with default system parameters, and ensemble size N=50, the \"direct\" method is generally 2x faster than the \"fft\" method, and the \"oa\" method is a little slower again. If <code>K</code> or <code>J</code> is increased, then the \"fft\" method becomes the fastest.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([0, 1, 2], dtype=float)\n&gt;&gt;&gt; np.allclose(boxcar(x, 1), x)\nTrue\n&gt;&gt;&gt; boxcar(x, 2)\narray([0.75, 1.  , 1.25])\n&gt;&gt;&gt; boxcar(x, 3)\narray([1., 1., 1.])\n&gt;&gt;&gt; x = np.arange(10, dtype=float)\n&gt;&gt;&gt; boxcar(x, 2)\narray([2.5, 1. , 2. , 3. , 4. , 5. , 6. , 7. , 8. , 6.5])\n&gt;&gt;&gt; boxcar(x, 5)\narray([4., 3., 2., 3., 4., 5., 6., 7., 6., 5.])\n</code></pre>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.prodsum_K1","title":"<code>prodsum_K1(x, y)</code>","text":"<p>Compute <code>prodsum(x, y, 1)</code> efficiently.</p>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.prodsum_self","title":"<code>prodsum_self(x, k)</code>","text":"<p>Compute <code>prodsum(x, x, k)</code> efficiently: eqn 10 of lorenz2005a.</p>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.shift","title":"<code>shift(x, k)</code>","text":"<p>Rolls <code>x</code> leftwards. I.e. <code>output[i] = input[i+k]</code>.</p> <p>Notes about speed that usually hold when testing with ensemble DA: - This implementation is somewhat faster than <code>x[..., np.mod(ii + k, M)]</code>. - Computational savings of re-using already shifted vectors (or matrices)   compared to just calling this function again are negligible.</p>"},{"location":"reference/mods/Lorenz05/#mods.Lorenz05.summation_kernel","title":"<code>summation_kernel(width)</code>","text":"<p>Prepare computation of the modified sum in lorenz2005a.</p> <p>Note: This gets repeatedly called, but actually the input is only ever <code>width = K</code> or <code>2*J</code>, so we should really cache or pre-compute. But with default system parameters and N=50, the savings are negligible.</p>"},{"location":"reference/mods/Lorenz05/demo/","title":"demo","text":"<p>Demonstrate the Lorenz-05 model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz05/settings01/","title":"settings01","text":"<p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/","title":"Lorenz63","text":"<p>The classic exhibitor of chaos, consisting of 3 coupled ODEs.</p> <p>The ODEs are derived by modelling, with many simplifications, the fluid convection between horizontal plates with different temperatures.</p> <p>Its phase-plot (with typical param settings) looks like a butterfly.</p> <p>See demo.py for more info.</p> <p>Modules:</p> Name Description <code>anderson2010rhf</code> <p>Settings from anderson2010.</p> <code>bocquet2012</code> <p>Reproduce results from Fig 11 of bocquet2012a.</p> <code>demo</code> <p>Demonstrate the Lorenz-63 model.</p> <code>extras</code> <p>Extra functionality (not necessary for the EnKF or the particle filter).</p> <code>mandel2016</code> <p>Settings from mandel2016hybrid.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p> <code>ramgraber2022</code> <p>Try settings of Figure 6A of Ramgraber's transport smoothing paper</p> <code>sakov2012</code> <p>Reproduce results from Table 1 sakov2012a.</p> <code>wiljes2017</code> <p>Settings from wiljes2016.</p>"},{"location":"reference/mods/Lorenz63/#mods.Lorenz63.dxdt","title":"<code>dxdt(x)</code>","text":"<p>Evolution equation (coupled ODEs) specifying the dynamics.</p>"},{"location":"reference/mods/Lorenz63/anderson2010rhf/","title":"anderson2010rhf","text":"<p>Settings from anderson2010.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/bocquet2012/","title":"bocquet2012","text":"<p>Reproduce results from Fig 11 of bocquet2012a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/demo/","title":"demo","text":"<p>Demonstrate the Lorenz-63 model.</p> <p>For a deeper introduction, see</p> <p>\"DA-tutorials/T4 - Dynamical systems, chaos, Lorenz.ipynb\"</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/extras/","title":"extras","text":"<p>Extra functionality (not necessary for the EnKF or the particle filter).</p> <p>Modules:</p> Name Description <code>LP</code> <p>On-line (live) plots of the DA process for various models and methods.</p> <code>core</code> <p>The classic exhibitor of chaos, consisting of 3 coupled ODEs.</p>"},{"location":"reference/mods/Lorenz63/extras/#mods.Lorenz63.extras.d2x_dtdx","title":"<code>d2x_dtdx(x)</code>","text":"<p>Tangent linear model (TLM). I.e. the Jacobian of dxdt(x).</p>"},{"location":"reference/mods/Lorenz63/extras/#mods.Lorenz63.extras.dstep_dx","title":"<code>dstep_dx(x, t, dt)</code>","text":"<p>Compute resolvent (propagator) of the TLM. I.e. the Jacobian of <code>step(x)</code>.</p>"},{"location":"reference/mods/Lorenz63/mandel2016/","title":"mandel2016","text":"<p>Settings from mandel2016hybrid.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/ramgraber2022/","title":"ramgraber2022","text":"<p>Try settings of Figure 6A of Ramgraber's transport smoothing paper</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/sakov2012/","title":"sakov2012","text":"<p>Reproduce results from Table 1 sakov2012a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz63/wiljes2017/","title":"wiljes2017","text":"<p>Settings from wiljes2016.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz84/","title":"Lorenz84","text":"<p>A chaotic system of size 3, like Lorenz-63, but with +complex geometry.</p> <p>Refs: lorenz1984, lorenz2005</p> <p>Modules:</p> Name Description <code>demo</code> <p>Demonstrate the Lorenz-84 model.</p> <code>harder</code> <p>Harder settings than in <code>mods.Lorenz84.pajonk2012</code>.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p> <code>pajonk2012</code> <p>Settings from pajonk2012a.</p>"},{"location":"reference/mods/Lorenz84/demo/","title":"demo","text":"<p>Demonstrate the Lorenz-84 model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz84/harder/","title":"harder","text":"<p>Harder settings than in <code>mods.Lorenz84.pajonk2012</code>.</p> <p>This was adjudged by noting that with their settings, the average val. of <code>trHK</code> is 0.013.</p> <p>Here we increase <code>dko</code> to make the DA problem more difficult.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz84/pajonk2012/","title":"pajonk2012","text":"<p>Settings from pajonk2012a.</p> <p>There is nothing to reproduce from the paper as there are no statistically converged numbers.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/","title":"Lorenz96","text":"<p>A 1D emulator of chaotic atmospheric behaviour.</p> <p>lorenz1996predictability</p> <p>For a short introduction, see</p> <ul> <li><code>demo</code> and</li> <li>\"Dynamical systems, chaos, Lorenz.ipynb\" from the DA-tutorials</li> </ul> <p>Note: the implementation is <code>len(x)</code>-agnostic.</p> <p>Modules:</p> Name Description <code>anderson2009</code> <p>A land-ocean setup from anderson2009a.</p> <code>bocquet2010</code> <p>From Fig. 1 of bocquet2010a.</p> <code>bocquet2010_m40</code> <p>From bocquet2010a (again), but <code>ndim=40</code> (i.e. Fig. 5 of paper).</p> <code>bocquet2015loc</code> <p>Settings as in bocquet2016.</p> <code>demo</code> <p>Demonstrate the Lorenz-96 model.</p> <code>extras</code> <p>Extra functionality (not necessary for the EnKF or the particle filter).</p> <code>frei2013bridging</code> <p>Settings as in frei2013a.</p> <code>hoteit2015</code> <p>Reproduce results from hoteit2015a.</p> <code>miyoshi2011</code> <p>A land-ocean setup.</p> <code>pinheiro2019</code> <p>Settings from pinheiro2019a.</p> <code>raanes2016</code> <p>Reproduce raanes2015a.</p> <code>sakov2008</code> <p>Settings as in sakov2008b.</p> <code>spantini2019</code> <p>Try reproducing Figure 9 of Spantini's coupling/nonlinear-transport paper.</p> <code>spectral_obs</code> <p>Settings for a \"spectral\" obs operator.</p> <code>todter2015</code> <p>Concerns figure 4 of todter2015a.</p> <code>todter2015_G</code> <p>From <code>mods.Lorenz96.todter2015</code> again, but with Gaussian likelihood.</p>"},{"location":"reference/mods/Lorenz96/anderson2009/","title":"anderson2009","text":"<p>A land-ocean setup from anderson2009a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/bocquet2010/","title":"bocquet2010","text":"<p>From Fig. 1 of bocquet2010a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/bocquet2010_m40/","title":"bocquet2010_m40","text":"<p>From bocquet2010a (again), but <code>ndim=40</code> (i.e. Fig. 5 of paper).</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/bocquet2015loc/","title":"bocquet2015loc","text":"<p>Settings as in bocquet2016.</p>"},{"location":"reference/mods/Lorenz96/demo/","title":"demo","text":"<p>Demonstrate the Lorenz-96 model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/extras/","title":"extras","text":"<p>Extra functionality (not necessary for the EnKF or the particle filter).</p> <p>Modules:</p> Name Description <code>LP</code> <p>On-line (live) plots of the DA process for various models and methods.</p>"},{"location":"reference/mods/Lorenz96/extras/#mods.Lorenz96.extras.d2x_dtdx","title":"<code>d2x_dtdx(x)</code>","text":"<p>Tangent linear model (TLM). I.e. the Jacobian of dxdt(x).</p>"},{"location":"reference/mods/Lorenz96/extras/#mods.Lorenz96.extras.dstep_dx","title":"<code>dstep_dx(x, t, dt)</code>","text":"<p>Compute resolvent (propagator) of the TLM. I.e. the Jacobian of <code>step(x)</code>.</p>"},{"location":"reference/mods/Lorenz96/frei2013bridging/","title":"frei2013bridging","text":"<p>Settings as in frei2013a.</p> <p>They also cite its use in the following:</p> <p>bengtsson2003, lei2011, frei2013.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/hoteit2015/","title":"hoteit2015","text":"<p>Reproduce results from hoteit2015a.</p>"},{"location":"reference/mods/Lorenz96/miyoshi2011/","title":"miyoshi2011","text":"<p>A land-ocean setup.</p> <p>Refs: miyoshi2011a, which was inspired by lorenz1998.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/pinheiro2019/","title":"pinheiro2019","text":"<p>Settings from pinheiro2019a.</p> <p>Modules:</p> Name Description <code>model</code> <p>A 1D emulator of chaotic atmospheric behaviour.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/raanes2016/","title":"raanes2016","text":"<p>Reproduce raanes2015a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/sakov2008/","title":"sakov2008","text":"<p>Settings as in sakov2008b.</p> <p>This HMM is used (with small variations) in many DA papers, for example</p> <p>bocquet2011, sakov2012a, bocquet2015, bocquet2013.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/spantini2019/","title":"spantini2019","text":"<p>Try reproducing Figure 9 of Spantini's coupling/nonlinear-transport paper.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/spectral_obs/","title":"spectral_obs","text":"<p>Settings for a \"spectral\" obs operator.</p> <p>Lorenz-96 is highly sensitive to large gradients. Therefore, if we only observe every 4<sup>th</sup> (e.g.) state component, the members might \"blow up\" during the forecast, because the assimilation created large gradients. (Of course, this will depend on R and dto). Therefore, the HMM below instead uses \"global obs\", where each observation captures information about the entire state vector. The idea is that we can then remove observations, (rows of H) one-by-one, to a much larger degree than for H = Identity.</p> <p>Ideally, we want the observations to be independent, and possibly of the same magnitude (i.e. we that the rows of H be orthonormal). Furthermore, we want that each observation gives equal weight (\"consideration\") to each state component. This can be shown to be equivalent to requiring that the state component is equally resolved by each observation, and moreover, that the magnitude (abs) of each element of H be a constant (1/sqrt(Nx)).</p> <p>Can such an H be constructed/found? In the 2d case: H = [1, 1; 1, -1] / sqrt(2). In the 3d case: no, as can be shown by enumeration. (note, however, how easy my geometric intuition was fooled. Try rotating the 3-dim stensil. Intuitively I thought that it would yield the 3d H of \u00b1 1's). ... In fact, only in the 2^n - dimensional case is it possible (our conjecture: Madleine/Patrick, based on analogy with the FFT).</p> <p>Another idea is then to evaluate the value of 40 orthogonal basis functions at 40 equidistant locations (corresponding to the indices of Lorenz-96). This will not yield a matrix of \u00b1 1's, but should nevertheless give nicely distributed weights.</p> <p>Note that the legendre polynomials are not orthogonal when (the inner product is) evaluated on discrete, equidistant points. Moreover, the actual orthogonal polynomial basis (which I think goes under the name of Gram polynomials, and can be constructed by qr-decomp (gram-schmidt) of a 40-dim Vandermonde matrix). would in fact not be a good idea: it is well-known that not only will the qr-decomp be numerically unstable, the exact polynomial interpolant of 40 equidistant points is subject to the \"horrible\" Runge phenomenon.</p> <p>Another basis is the harmonic (sine/cosine) functions. Advantages:  - will be orthogonal when evaluated on 40 discrete equidistant points.  - the domain of Lorenz-96 is periodic: as are sine/cosine.  - (conjecture) in the 2^n dim. case, it yields a matrix of \u00b1 1's.  - nice \"spectral/frequency\" interpretation of each observation. Disadvatages:  - 40 is not 2^n for any n  - Too obvious (not very original).</p> <p>In conclusion, we will use the harmonic functions.</p> <p>Update: It appears that we were wrong concerning the 2^n case. That is, sine/cosine functions do not yield only \u00b1 1's for n&gt;2 The question then remains (to be proven combinatorically?) if the \u00b1 1 matrices exist for dim&gt;4 Furthermore, experiments do not seem to indicate that I can push Ny much lower than for the case H = Identity, even though the rmse is a lot lower with spectral H. Am I missing something?</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/todter2015/","title":"todter2015","text":"<p>Concerns figure 4 of todter2015a.</p> <p>Modules:</p> Name Description <code>RVs</code> <p>Classes of random variables.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96/todter2015_G/","title":"todter2015_G","text":"<p>From <code>mods.Lorenz96.todter2015</code> again, but with Gaussian likelihood.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96s/","title":"Lorenz96s","text":"<p>A perfect-random version of Lorenz-96.</p> <p>Used by grudzien2020a to study the precision of stochastic integration schemes.</p> <p>Both the model and truth are to be integrated by the same random model (with almost surely different outcomes).  For simplicity, this case should be used with <code>Q = 0</code>, i.e. with no model error (as perceived by the DA schemes).  Inflation, localisation, and other auxiliary techiques may be used to handle sampling error and perform regularization.</p> <p>The truth twin should be generated by the order 2.0 Taylor scheme below, for the accuracy with respect to convergence in the strong sense. See grudzien2020a for a full discussion of benchmarks on this model and statistically robust configurations.</p> <p>This study uses no multiplicative inflation / localization or other regularization instead using a large ensemble size in the perturbed observation EnKF as a simple estimator to study the asymptotic filtering statistics under different model scenarios.</p> <p>The purpose of the study in grudzien2020a was to explore the relationships between:</p> <ul> <li>numerical discretization error in truth twins;</li> <li>numerical discretization error in model twins;</li> <li>model uncertainty in perfect-random models;</li> <li>filter divergence and / or bias in filtering forecast statistics;</li> </ul> <p>Numerical discretization error increases with dt, with the strong / weak order of convergence discussed in the refs.  Although the orders of convergence of the stochastic Runge-Kutta and the Euler-Maruyama model match, it is shown that the step size configuration above keeps the discretization error for the model and truth twins bounded by approximately \\(10^{-3}\\) in expectation.</p> <p>Model uncertainty increases with the diffusion, representing the \"instantaneous\" standard deviation of the model noise at any moment. Larger diffusion thus corresponds to a wider variance of the relizations of the diffeomorphsims that generate the model / truth twin between observation times.</p> <p>It is demonstrated by grudzien2020a that the model error due to discretization of the SDE equations of motion is most detrimental to the filtering cycle when model uncertainty is low and observation precision is high.  In other configurations, such as those with high model uncertainty, the differences between ensembles with low discretization error (those using the Runge-Kutta scheme) and high discretization error (those using the Euler-Maruyama scheme) tend to be relaxed.</p> <p>Set-up with three different <code>step</code> functions, using different SDE integrators. The truth twin is generated by the order 2.0 Taylor scheme, for accuracy with respect to convergence in the strong sense for generating the observation sequence. The model simulation step sizes are varied in the settings below to demonstrate the differences between the commonly uses Euler-Maruyama and the more statistically robust Runge-Kutta method for SDE integration. See README in <code>mods.Lorenz96s</code>.</p> <p>Modules:</p> Name Description <code>grudzien2020</code> <p>Settings as in grudzien2020a.</p>"},{"location":"reference/mods/Lorenz96s/#mods.Lorenz96s.l96s_tay2_step","title":"<code>l96s_tay2_step(x, t, dt, s)</code>","text":"<p>Advance state of L96s model using order-2.0 Taylor scheme.</p> <p>This is the method that should be used to generate the truth twin for this model due to the high-accuracy with respect to convergence in the strong sense. The ensemble model twin will be generated by on of the wrappers below.  The order 2.0 Taylor-Stratonovich discretization scheme implemented here is the basic formulation which makes a Fourier truncation at p=1 for the Brownian bridge process. See grudzien2020a for full details of the scheme and other versions.</p>"},{"location":"reference/mods/Lorenz96s/#mods.Lorenz96s.steppers","title":"<code>steppers(kind)</code>","text":"<p>Wrapper around the different model integrators / time steppers.</p> <p>Note that they all forward (i.e. use) the diffusion parameter.</p>"},{"location":"reference/mods/Lorenz96s/grudzien2020/","title":"grudzien2020","text":"<p>Settings as in grudzien2020a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/Lorenz96s/grudzien2020/#mods.Lorenz96s.grudzien2020.HMMs","title":"<code>HMMs(stepper='Tay2', resolution='Low', R=1)</code>","text":"<p>Define the various HMMs used.</p>"},{"location":"reference/mods/LorenzUV/","title":"LorenzUV","text":"<p>The 2-scale/layer/speed coupled version of Lorenz-96.</p> <p>See wilks2005 - U:  large amp, low frequency vars: convective events - V:  small amp, high frequency vars: large-scale synoptic events</p> <p>Typically, the DA system will only use the truncated system (containing only the U variables), where the V's are parameterized as model noise, while the truth is simulated by the full system.</p> <p>Stochastic parmateterization: Wilks: benefit of including stochastic noise negligible unless its temporal auto-corr is taken into account (as AR(1)) (but spatial auto-corr can be neglected). But AR(1) noise is technically difficult because DAPPER is built around the Markov assumption.</p> <p>Modules:</p> Name Description <code>L96</code> <p>A 1D emulator of chaotic atmospheric behaviour.</p> <code>LP</code> <p>On-line (live) plots of the DA process for various models and methods.</p> <code>demo</code> <p>Demonstrate the Lorenz two-speed/scale/layer model.</p> <code>illust_LorenzUV</code> <p>Prettier, static illustration of Lorenz two-speed/scale/layer model.</p> <code>illust_parameterizations</code> <p>Illusrate parameterizations.</p> <code>lorenz96</code> <p>As in lorenz1996predictability.</p> <code>wilks05</code> <p>Uses <code>nU</code>, <code>J</code>, <code>F</code> as in <code>mods.LorenzUV</code> ie. from wilks2005.</p>"},{"location":"reference/mods/LorenzUV/#mods.LorenzUV.model_instance","title":"<code>model_instance</code>","text":"<p>Use OOP to facilitate having multiple parameter settings simultaneously.</p> <p>Default parameters from wilks2005.</p>"},{"location":"reference/mods/LorenzUV/#mods.LorenzUV.model_instance.dxdt","title":"<code>dxdt(x)</code>","text":"<p>Compute full (coupled) <code>dxdt</code>.</p>"},{"location":"reference/mods/LorenzUV/#mods.LorenzUV.model_instance.dxdt_parameterized","title":"<code>dxdt_parameterized(x, t)</code>","text":"<p>Compute truncated <code>dxdt</code> with parameterization of fast variables (<code>V</code>).</p>"},{"location":"reference/mods/LorenzUV/#mods.LorenzUV.model_instance.dxdt_trunc","title":"<code>dxdt_trunc(x)</code>","text":"<p>Compute truncated <code>dxdt:</code> slow variables (<code>U</code>) only.</p>"},{"location":"reference/mods/LorenzUV/#mods.LorenzUV.reversible","title":"<code>reversible(fun)</code>","text":"<p>Reverse input/output (instead of manipulating indices).</p>"},{"location":"reference/mods/LorenzUV/demo/","title":"demo","text":"<p>Demonstrate the Lorenz two-speed/scale/layer model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LorenzUV/illust_LorenzUV/","title":"illust_LorenzUV","text":"<p>Prettier, static illustration of Lorenz two-speed/scale/layer model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LorenzUV/illust_parameterizations/","title":"illust_parameterizations","text":"<p>Illusrate parameterizations.</p> <p>Plot scattergram of \"unresolved tendency\" and the parameterization that emulate it.</p> <p>We plot the diff:   <code>model_step/dt - true_step/dt    (1)</code> Whereas Wilks plots   <code>model_dxdt    - true_step/dt    (2)</code> Another option is:   <code>model_dxdt    - true_dxdt       (3)</code></p> <p>Thus, for us (eqn 1), the model integration scheme matters. Also, Wilks uses - <code>dt = 0.001</code> for truth - <code>dt = 0.005</code> for model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LorenzUV/illust_parameterizations/#mods.LorenzUV.illust_parameterizations.blend_rgb","title":"<code>blend_rgb(rgb, a, bg_rgb=(1, 1, 1))</code>","text":"<p>Fake RGB transparency by blending it to some background.</p> <p>Useful for creating gradients.</p> <p>Also useful for creating 'transparency' for exporting to eps. But there's no actualy transparency, so superposition of lines will not work. For that: export to pdf, or make do without.</p> <ul> <li><code>rgb</code>: N-by-3 rgb, or a color code.</li> <li><code>a</code>: alpha value</li> <li><code>bg_rgb</code>: background in rgb. Default: white</li> </ul> <p>Based on stackoverflow.com/a/33375738/38281</p>"},{"location":"reference/mods/LorenzUV/lorenz96/","title":"lorenz96","text":"<p>As in lorenz1996predictability.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LorenzUV/wilks05/","title":"wilks05","text":"<p>Uses <code>nU</code>, <code>J</code>, <code>F</code> as in <code>mods.LorenzUV</code> ie. from wilks2005.</p> <p>Obs settings taken from different places (=&gt; quasi-linear regime).</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LorenzUV/wilks05/#mods.LorenzUV.wilks05.polynom_prmzt","title":"<code>polynom_prmzt(x, t, order)</code>","text":"<p>Polynomial (deterministic) parameterization of fast variables (Y).</p> <p>NB: Only valid for system settings of Wilks'2005.</p> <p>Note: In order to observe an improvement in DA performance w       higher orders, the EnKF must be reasonably tuned with       There is very little improvement gained above order=1.</p>"},{"location":"reference/mods/LotkaVolterra/","title":"LotkaVolterra","text":"<p>The generalized predator-prey model, with settings for chaotic dynamics.</p> <p>Refs: Wiki, vano2006.</p> <p>Modules:</p> Name Description <code>demo</code> <p>Demonstrate the Lotka-Volterra model.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p> <code>settings101</code> <p>Settings that produce somewhat interesting/challenging DA problems.</p>"},{"location":"reference/mods/LotkaVolterra/demo/","title":"demo","text":"<p>Demonstrate the Lotka-Volterra model.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/LotkaVolterra/settings101/","title":"settings101","text":"<p>Settings that produce somewhat interesting/challenging DA problems.</p> <p><code>dt</code> has been chosen after noting that using <code>dt</code> up to 0.7 does not change the chaotic properties much, as adjudged with eye-ball and Lyapunov measures.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/QG/","title":"QG","text":"<p>Quasi-geostraphic 2D flow. Described in detail by sakov2008b.</p> <p>Adapted from Pavel Sakov's enkf-matlab package.</p> <p>More info:</p> <ul> <li><code>governing_eqn.png</code></li> <li><code>demo.py</code></li> <li>\u03c8 (psi) is the stream function (i.e. surface elevation)</li> <li>Doubling time \"between 25 and 50\"</li> <li>Note Sakov's trick of increasing RKH2 from 2.0e-12 to 2.0e-11 to stabilize   the ensemble integration, which may be necessary for EnKF's with small N.   See example in <code>counillon2009</code>.</li> </ul> <p>Modules:</p> Name Description <code>LP</code> <p>On-line (live) plots of the DA process for various models and methods.</p> <code>counillon2009</code> <p>Reproduce experiments from counillon2009a.</p> <code>demo</code> <p>Demonstrate the QG (quasi-geostrophic) model.</p> <code>f90</code> <p>This dir contains the Fortran-90 code of the model.</p> <code>illust_obs</code> <p>Stream function and observation time series for QG (quasi-geostrophic) model.</p> <code>modelling</code> <p>Contains models included with DAPPER.</p> <code>sakov2008</code> <p>Reproduce results from sakov2008b.</p>"},{"location":"reference/mods/QG/#mods.QG.model_config","title":"<code>model_config</code>","text":"<p>Define model.</p> <p>Helps ensure consistency between prms file (that Fortran module reads) and Python calls to step(), for example for dt.</p>"},{"location":"reference/mods/QG/#mods.QG.model_config.__init__","title":"<code>__init__(name, prms, mp=True)</code>","text":"<p>Use <code>prms={}</code> to get the default configuration.</p>"},{"location":"reference/mods/QG/#mods.QG.model_config.step","title":"<code>step(E, t, dt)</code>","text":"<p>Vector and 2D-array (ens) input, with multiproc for ens case.</p>"},{"location":"reference/mods/QG/#mods.QG.model_config.step_1","title":"<code>step_1(x0, t, dt)</code>","text":"<p>Step a single state vector.</p>"},{"location":"reference/mods/QG/counillon2009/","title":"counillon2009","text":"<p>Reproduce experiments from counillon2009a.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/QG/demo/","title":"demo","text":"<p>Demonstrate the QG (quasi-geostrophic) model.</p>"},{"location":"reference/mods/QG/illust_obs/","title":"illust_obs","text":"<p>Stream function and observation time series for QG (quasi-geostrophic) model.</p> <p>Modules:</p> Name Description <code>dpr</code> <p>Root package of DAPPER</p>"},{"location":"reference/mods/QG/sakov2008/","title":"sakov2008","text":"<p>Reproduce results from sakov2008b.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/mods/QG/f90/","title":"f90","text":"<p>This dir contains the Fortran-90 code of the model.</p>"},{"location":"reference/mods/VL20/","title":"VL20","text":"<p>Single-scale Lorenz-96 with an added thermodynamic component.</p> <p>Refs: vissio2020</p> <p>Modules:</p> Name Description <code>demo</code> <p>Demonstrate the Vissio-Lucarini-20 model.</p>"},{"location":"reference/mods/VL20/#mods.VL20.model_instance","title":"<code>model_instance</code>","text":"<p>Use OOP to facilitate having multiple parameter settings simultaneously.</p>"},{"location":"reference/mods/VL20/#mods.VL20.model_instance.d2x_dtdx","title":"<code>d2x_dtdx(x)</code>","text":"<p>Tangent linear model</p>"},{"location":"reference/mods/VL20/#mods.VL20.model_instance.dxdt","title":"<code>dxdt(x)</code>","text":"<p>Full (coupled) dxdt.</p>"},{"location":"reference/mods/VL20/#mods.VL20.model_instance.unpack","title":"<code>unpack(x)</code>","text":"<p>Unpack model input to model parameters and state vector</p>"},{"location":"reference/mods/VL20/demo/","title":"demo","text":"<p>Demonstrate the Vissio-Lucarini-20 model.</p> <p>Reproduce Hovmoller diagram Fig 4. in vissio2020.</p> <p>Modules:</p> Name Description <code>modelling</code> <p>Contains models included with DAPPER.</p>"},{"location":"reference/tools/","title":"tools","text":"<p>If the only tool you have is a hammer, it's hard to eat spaghetti.</p> <p>Modules:</p> Name Description <code>chronos</code> <p>Time sequence management, notably <code>Chronology</code> and <code>Ticker</code>.</p> <code>colors</code> <p>Color definitions &amp; functionality for matplotlib and the terminal.</p> <code>datafiles</code> <p>Manage the data files created by DAPPER.</p> <code>linalg</code> <p>Linear algebra.</p> <code>liveplotting</code> <p>On-line (live) plots of the DA process for various models and methods.</p> <code>localization</code> <p>Localization tools, including distance and tapering comps.</p> <code>matrices</code> <p>Covariance matrix tools.</p> <code>multiproc</code> <p>Paralellisation via multiprocessing. Limit num. of CPUs used by <code>numpy</code> to 1.</p> <code>progressbar</code> <p>Make <code>progbar</code> (wrapper around <code>tqdm</code>) and <code>read1</code>.</p> <code>randvars</code> <p>Classes of random variables.</p> <code>remote</code> <p>Tools related to running experimentes remotely</p> <code>rounding</code> <p>Functions for rounding numbers.</p> <code>seeding</code> <p>Random number generation.</p> <code>series</code> <p>Time series management and processing.</p> <code>viz</code> <p>Plot utilities.</p>"},{"location":"reference/tools/chronos/","title":"chronos","text":"<p>Time sequence management, notably <code>Chronology</code> and <code>Ticker</code>.</p>"},{"location":"reference/tools/chronos/#tools.chronos.Chronology","title":"<code>Chronology</code>","text":"<p>Time schedules with consistency checks.</p> <ul> <li>Uses int records, so <code>tt[k] == k*dt</code>.</li> <li>Uses generators, so time series may be arbitrarily long.</li> </ul> <p>Example illustration:</p> <pre><code>                     [----dto------]\n              [--dt--]\ntt:    0.0    0.2    0.4    0.6    0.8    1.0    T\nkk:    0      1      2      3      4      5      K\n       |------|------|------|------|------|------|\nko:    None   None   0      None   1      None   Ko\nkko:                 2             4             6\n                     [----dko------]\n</code></pre> <p>Warning</p> <p>By convention, there is no obs at 0. This is hardcorded in DAPPER, whose cycling starts by the forecast.</p> <p>Identities (subject to precision):</p> <pre><code>len(kk)  == len(tt)  == K   +1\nlen(kko) == len(tto) == Ko+1\n\nkko[0]   == dko      == dto/dt == K/(Ko+1)\nkko[-1]  == K        == T/dt\nKo       == T/dto-1\n</code></pre> <p>These attributes may be set (altered) after init: <code>dt, dko, K, T</code>. Setting other attributes (alone) is ambiguous (e.g. should <code>dto*=2</code> yield a doubling of <code>T</code> too?), and so should/will raise an exception.</p>"},{"location":"reference/tools/chronos/#tools.chronos.Chronology.mask","title":"<code>mask</code>","text":"<p>Example use: <code>kk_BI = kk[mask]</code></p>"},{"location":"reference/tools/chronos/#tools.chronos.Chronology.masko","title":"<code>masko</code>","text":"<p>Example use: <code>kko_BI = kko[masko]</code></p>"},{"location":"reference/tools/chronos/#tools.chronos.Chronology.ticker","title":"<code>ticker</code>","text":"<p>Fancy version of <code>range(1,K+1)</code>.</p> <p>Also yields <code>t</code>, <code>dt</code>, and <code>ko</code>.</p>"},{"location":"reference/tools/chronos/#tools.chronos.Chronology.copy","title":"<code>copy()</code>","text":"<p>Copy via state vars.</p>"},{"location":"reference/tools/chronos/#tools.chronos.Chronology.cycle","title":"<code>cycle(ko)</code>","text":"<p>The range (in <code>kk</code>) between observation <code>ko-1</code> and <code>ko</code>.</p> <p>Also yields <code>t</code> and <code>dt</code>.</p>"},{"location":"reference/tools/chronos/#tools.chronos.Ticker","title":"<code>Ticker</code>","text":"<p>Iterator over kk and <code>kko</code>, yielding <code>(k,ko,t,dt)</code>.</p> <p>Includes <code>__len__</code> for progressbar usage.</p> <p><code>ko = kko.index(k)</code>, or <code>None</code> otherwise, but computed without this repeated look-up operation.</p>"},{"location":"reference/tools/colors/","title":"colors","text":"<p>Color definitions &amp; functionality for matplotlib and the terminal.</p>"},{"location":"reference/tools/colors/#tools.colors.color_text","title":"<code>color_text(text, *color_codes)</code>","text":"<p>Color a string for the terminal.</p> <p>Multiple <code>color_codes</code> can be combined. Look them up in <code>colorama.Back</code> and <code>colorama.Fore</code>. Use the single code <code>None</code> for no coloring.</p>"},{"location":"reference/tools/colors/#tools.colors.coloring","title":"<code>coloring(*color_codes)</code>","text":"<p>Color printing using 'with'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with coloring(colorama.Fore.GREEN):\n...    print(\"--- This is in color ---\")\n</code></pre>"},{"location":"reference/tools/datafiles/","title":"datafiles","text":"<p>Manage the data files created by DAPPER.</p> <p>Modules:</p> Name Description <code>uplink</code> <p>Tools related to running experimentes remotely</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.create_run_dir","title":"<code>create_run_dir(save_as, mp)</code>","text":"<p>Validate <code>save_as</code> and create dir <code>rc.dirs.data / save_as</code> and sub-dirs.</p> <p>The data gets saved here unless <code>save_as</code> is <code>False</code>/<code>None</code>.</p> <p>Note: multiprocessing (locally or in the cloud) requires saving/loading data.</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.find_latest_run","title":"<code>find_latest_run(root)</code>","text":"<p>Find the latest experiment (dir containing many)</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.load_HMM","title":"<code>load_HMM(save_as)</code>","text":"<p>Load HMM from <code>xp.com</code> from given dir.</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.load_xps","title":"<code>load_xps(save_as)</code>","text":"<p>Load <code>xps</code> (as a <code>list</code>) from given dir.</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.overwrite_xps","title":"<code>overwrite_xps(xps, save_as, nDir=100)</code>","text":"<p>Save xps in save_as, but safely (by first saving to tmp).</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.reduce_inodes","title":"<code>reduce_inodes(save_as, nDir=100)</code>","text":"<p>Reduce the number of <code>xp</code> dirs.</p> <p>Done by packing multiple <code>xp</code>s into lists (<code>xps</code>). This reduces the number of files (inodes) on the system, which is limited.</p> <p>It also deletes files \"xp.var\" and \"out\", whose main content tends to be the printed progbar. This probably leads to some reduced loading time.</p> <p>FAQ: Why isn't the default for <code>nDir</code> simply 1? So that we can get a progressbar when loading.</p>"},{"location":"reference/tools/datafiles/#tools.datafiles.save_xps","title":"<code>save_xps(xps, save_as, nDir=100)</code>","text":"<p>Save <code>xps</code> (list of <code>xp</code>s) in <code>nDir</code> subfolders: <code>save_as/i</code>.</p> Example <p>Rename attr. <code>n_iter</code> to <code>nIter</code> in some saved data:</p> <pre><code>proj_name = \"Stein\"\ndd = rc.dirs.data / proj_name\nsave_as = dd / \"run_2020-09-22__19:36:13\"\n\nfor save_as in dd.iterdir():\n    save_as = dd / save_as\n\n    xps = load_xps(save_as)\n    HMM = load_HMM(save_as)\n\n    for xp in xps:\n        if hasattr(xp,\"n_iter\"):\n            xp.nIter = xp.n_iter\n            del xp.n_iter\n\n    overwrite_xps(xps, save_as)\n</code></pre>"},{"location":"reference/tools/linalg/","title":"linalg","text":"<p>Linear algebra.</p>"},{"location":"reference/tools/linalg/#tools.linalg.mldiv","title":"<code>mldiv(A, b)</code>","text":"<p>A\\b.</p>"},{"location":"reference/tools/linalg/#tools.linalg.mrdiv","title":"<code>mrdiv(b, A)</code>","text":"<p>b/A.</p>"},{"location":"reference/tools/linalg/#tools.linalg.pad0","title":"<code>pad0(x, N)</code>","text":"<p>Pad <code>x</code> with zeros so that <code>len(x)==N</code>.</p>"},{"location":"reference/tools/linalg/#tools.linalg.svd0","title":"<code>svd0(A)</code>","text":"<p>Similar to Matlab's <code>svd(A,0)</code>.</p> <p>Compute the</p> <ul> <li>full    svd if <code>nrows &gt; ncols</code></li> <li>reduced svd otherwise.</li> </ul> <p>As in Matlab: <code>svd(A,0)</code>, except that the input and output are transposed, in keeping with DAPPER convention. It contrasts with <code>scipy.linalg.svd(full_matrice=False)</code> and Matlab's <code>svd(A,'econ')</code>, both of which always compute the reduced svd.</p> See Also <p>tsvd : rank (and threshold) truncation.</p>"},{"location":"reference/tools/linalg/#tools.linalg.svdi","title":"<code>svdi(U, s, VT)</code>","text":"<p>Reconstruct matrix from <code>sla.svd</code> or <code>tsvd</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; A = np.arange(12).reshape((3,-1))\n&gt;&gt;&gt; B = svdi(*tsvd(A, 1.0))\n&gt;&gt;&gt; np.allclose(A, B)\nTrue\n</code></pre> See Also <p>sla.diagsvd</p>"},{"location":"reference/tools/linalg/#tools.linalg.tinv","title":"<code>tinv(A, *kargs, **kwargs)</code>","text":"<p>Psuedo-inverse using <code>tsvd</code>.</p> See Also <p>sla.pinv2.</p>"},{"location":"reference/tools/linalg/#tools.linalg.trank","title":"<code>trank(A, *kargs, **kwargs)</code>","text":"<p>Compute rank via <code>tsvd</code>, i.e. as \"seen\" by <code>tsvd</code>.</p>"},{"location":"reference/tools/linalg/#tools.linalg.truncate_rank","title":"<code>truncate_rank(s, threshold, avoid_pathological)</code>","text":"<p>Find <code>r</code> such that <code>s[:r]</code> contains the threshold proportion of <code>s</code>.</p>"},{"location":"reference/tools/linalg/#tools.linalg.tsvd","title":"<code>tsvd(A, threshold=0.99999, avoid_pathological=True)</code>","text":"<p>Compute the truncated svd.</p> <p>Also automates 'full_matrices' flag.</p> <p>Parameters:</p> Name Type Description Default <code>avoid_pathological</code> <code>bool</code> <p>Avoid truncating (e.g.) the identity matrix. NB: only applies for float threshold.</p> <code>True</code> <code>threshold</code> <code>float or int</code> <ul> <li>if <code>float</code>, <code>&lt; 1.0</code> then \"rank\" = lowest number   such that the \"energy\" retained &gt;= threshold</li> <li>if <code>int</code>,  <code>&gt;= 1</code>   then \"rank\" = threshold</li> </ul> <code>0.99999</code>"},{"location":"reference/tools/liveplotting/","title":"liveplotting","text":"<p>On-line (live) plots of the DA process for various models and methods.</p> <p>Liveplotters are given by a list of tuples as property or arguments in <code>mods.HiddenMarkovModel</code>.</p> <ul> <li> <p>The first element of the tuple determines whether the liveplotter is shown if the names of liveplotters are not given by <code>liveplots</code> argument in <code>assimilate</code>.</p> </li> <li> <p>The second element in the tuple gives the corresponding liveplotter function/class. See example of function <code>LPs</code> in <code>mods.Lorenz63</code>.</p> </li> </ul> <p>The liveplotters can be fine-tuned by each DA experiments via argument of <code>liveplots</code> when calling <code>assimilate</code>.</p> <ul> <li> <p><code>liveplots = True</code> turns on liveplotters set to default in the first argument of the <code>HMM.liveplotter</code> and default liveplotters defined in this module (<code>sliding_diagnostics</code> and <code>weight_histogram</code>).</p> </li> <li> <p><code>liveplots</code> can also be a list of specified names of liveplotter, which is the name of the corresponding liveplotting classes/functions.</p> </li> </ul> <p>Modules:</p> Name Description <code>pb</code> <p>Make <code>progbar</code> (wrapper around <code>tqdm</code>) and <code>read1</code>.</p> <code>viz</code> <p>Plot utilities.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.LivePlot","title":"<code>LivePlot</code>","text":"<p>Live plotting manager.</p> <p>Deals with</p> <ul> <li>Pause, skip.</li> <li>Which liveploters to call.</li> <li><code>plot_u</code></li> <li>Figure window (title and number).</li> </ul>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.LivePlot.__init__","title":"<code>__init__(stats, liveplots, key0=(0, None, 'u'), E=None, P=None, speed=1.0, replay=False, **kwargs)</code>","text":"<p>Initialize plots.</p> <ul> <li>liveplots: figures to plot; alternatives:<ul> <li><code>\"default\"/[]/True</code>: All default figures for this HMM.</li> <li><code>\"all\"</code>            : Even more.</li> <li>non-empty <code>list</code>   : Only the figures with these numbers                      (int) or names (str).</li> <li><code>False</code>            : None.</li> </ul> </li> <li>speed: speed of animation.<ul> <li><code>&gt;100</code>: instantaneous</li> <li><code>1</code>   : (default) as quick as possible allowing for           plt.draw() to work on a moderately fast computer.</li> <li><code>&lt;1</code>  : slower.</li> </ul> </li> </ul>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.LivePlot.update","title":"<code>update(key, E, P)</code>","text":"<p>Update liveplots</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.correlations","title":"<code>correlations</code>","text":"<p>Plots the state (auto-)correlation matrix.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.sliding_diagnostics","title":"<code>sliding_diagnostics</code>","text":"<p>Plots a sliding window (like a heart rate monitor) of certain diagnostics.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.spectral_errors","title":"<code>spectral_errors</code>","text":"<p>Plots the (spatial-RMS) error as a functional of the SVD index.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.weight_histogram","title":"<code>weight_histogram</code>","text":"<p>Plots histogram of weights. Refreshed each analysis.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.circulant_ACF","title":"<code>circulant_ACF(C, do_abs=False)</code>","text":"<p>Compute the auto-covariance-function corresponding to <code>C</code>.</p> <p>This assumes it is the cov/corr matrix of a 1D periodic domain.</p> <p>Vectorized or FFT implementations are possible.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.d_ylim","title":"<code>d_ylim(data, ax=None, cC=0, cE=1, pp=(1, 99), Min=-1e+20, Max=+1e+20)</code>","text":"<p>Provide new ylim's intelligently, from percentiles of the data.</p> <ul> <li><code>data</code>: iterable of arrays for computing percentiles.</li> <li> <p><code>pp</code>: percentiles</p> </li> <li> <p><code>ax</code>: If present, then the delta_zoom in/out is also considered.</p> </li> <li> <p><code>cE</code>: exansion (widenting) rate \u2208 [0,1].     Default: 1, which immediately expands to percentile.</p> </li> <li> <p><code>cC</code>: compression (narrowing) rate \u2208 [0,1].     Default: 0, which does not allow compression.</p> </li> <li> <p><code>Min</code>/<code>Max</code>: bounds</p> </li> </ul> <p>Despite being a little involved, the cost of this subroutine is typically not substantial because there's usually not that much data to sort through.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.duplicate_with_blanks_for_resampled","title":"<code>duplicate_with_blanks_for_resampled(E, dims, key, has_w)</code>","text":"<p>Particle filter: insert breaks for resampled particles.</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.not_empty","title":"<code>not_empty(xx)</code>","text":"<p>Works for non-iterable and iterables (including ndarrays).</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.update_alpha","title":"<code>update_alpha(key, stats, lines, scatters=None)</code>","text":"<p>Adjust color alpha (for particle filters).</p>"},{"location":"reference/tools/liveplotting/#tools.liveplotting.validate_lag","title":"<code>validate_lag(Tplot, tseq)</code>","text":"<p>Return validated <code>T_lag</code> such that is is:</p> <ul> <li>equal to <code>Tplot</code> with fallback: <code>HMM.tseq.Tplot</code>.</li> <li>no longer than <code>HMM.tseq.T</code>.</li> </ul> <p>Also return corresponding <code>K_lag</code>, <code>a_lag</code>.</p>"},{"location":"reference/tools/localization/","title":"localization","text":"<p>Localization tools, including distance and tapering comps.</p> <p>A good introduction to localization: Sakov (2011), Computational Geosciences: 'Relation between two common localisation methods for the EnKF'.</p>"},{"location":"reference/tools/localization/#tools.localization.dist2coeff","title":"<code>dist2coeff(dists, radius, tag=None)</code>","text":"<p>Compute tapering coefficients corresponding to a distances.</p> <p>NB: The radius is internally adjusted such that, independently of 'tag', <code>coeff==np.exp(-0.5)</code> when <code>distance==radius</code>.</p> <p>This is largely based on Sakov's enkf-matlab code. Two bugs have here been fixed: - The constants were slightly wrong, as noted in comments below. - It forgot to take sqrt() of coeffs when applying them through 'local analysis'.</p>"},{"location":"reference/tools/localization/#tools.localization.inds_and_coeffs","title":"<code>inds_and_coeffs(dists, radius, cutoff=0.001, tag=None)</code>","text":"<p>Compute indices and coefficients of localization.</p> <ul> <li>inds   : the indices of pts that are \"close to\" centre.</li> <li>coeffs : the corresponding tapering coefficients.</li> </ul>"},{"location":"reference/tools/localization/#tools.localization.nd_Id_localization","title":"<code>nd_Id_localization(shape, batch_shape=None, obs_inds=None, periodic=True)</code>","text":"<p>Localize Id (direct) point obs of an N-D, homogeneous, rectangular domain.</p>"},{"location":"reference/tools/localization/#tools.localization.pairwise_distances","title":"<code>pairwise_distances(A, B=None, domain=None)</code>","text":"<p>Euclidian distance (not squared) between pts. in <code>A</code> and <code>B</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>array of shape `(nPoints, nDims)`.</code> <p>A collection of points.</p> required <code>B</code> <p>Same as <code>A</code>, but <code>nPoints</code> can differ.</p> <code>None</code> <code>domain</code> <code>tuple</code> <p>Assume the domain is a periodic hyper-rectangle whose edges along dimension <code>i</code> span from 0 to <code>domain[i]</code>. NB: Behaviour not defined if <code>any(A.max(0) &gt; domain)</code>, and likewise for <code>B</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Array of of shape `(nPointsA, nPointsB)`.</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; A = [[0, 0], [0, 1], [1, 0], [1, 1]]\n&gt;&gt;&gt; with np.printoptions(precision=2):\n...     print(pairwise_distances(A))\n[[0.   1.   1.   1.41]\n [1.   0.   1.41 1.  ]\n [1.   1.41 0.   1.  ]\n [1.41 1.   1.   0.  ]]\n</code></pre> <p>The function matches <code>pdist(..., metric='euclidean')</code>, but is faster:</p> <pre><code>&gt;&gt;&gt; from scipy.spatial.distance import pdist, squareform\n&gt;&gt;&gt; bool((pairwise_distances(A) == squareform(pdist(A))).all())\nTrue\n</code></pre> <p>As opposed to <code>pdist</code>, it also allows comparing <code>A</code> to a different set of points, <code>B</code>, without the augmentation/block tricks needed for pdist.</p> <pre><code>&gt;&gt;&gt; A = np.arange(4)[:, None]\n&gt;&gt;&gt; pairwise_distances(A, [[2]]).T\narray([[2., 1., 0., 1.]])\n</code></pre> <p>Illustration of periodicity:</p> <pre><code>&gt;&gt;&gt; pairwise_distances(A, domain=(4, ))\narray([[0., 1., 2., 1.],\n       [1., 0., 1., 2.],\n       [2., 1., 0., 1.],\n       [1., 2., 1., 0.]])\n</code></pre> <p>NB: If an input array is 1-dim, it is seen as a single point.</p> <pre><code>&gt;&gt;&gt; pairwise_distances(np.arange(4))\narray([[0.]])\n</code></pre>"},{"location":"reference/tools/localization/#tools.localization.rectangular_partitioning","title":"<code>rectangular_partitioning(shape, steps, do_ind=True)</code>","text":"<p>N-D rectangular batch generation.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>len(grid[dim]) for dim in range(ndim)</code> required <code>steps</code> <code>step_len[dim] for dim in range(ndim)</code> required <p>Returns:</p> Type Description <code>A list of batches,</code> <code>where each element (batch) is a list of indices.</code> Example <p>shape   = [4, 13] ... batches = rectangular_partitioning(shape, [2, 4], do_ind=False) ... nB      = len(batches) ... values  = np.random.choice(np.arange(nB), nB, 0) ... Z       = np.zeros(shape) ... for ib, b in enumerate(batches): ...     Z[tuple(b)] = values[ib] ... plt.imshow(Z)  # doctest: +SKIP</p>"},{"location":"reference/tools/matrices/","title":"matrices","text":"<p>Covariance matrix tools.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat","title":"<code>CovMat</code>","text":"<p>Covariance matrix class.</p> <p>Main tasks:</p> <ul> <li>Unify the covariance representations: full, diagonal, reduced-rank sqrt.</li> <li>Streamline init. and printing.</li> <li>Convenience transformations with caching/memoization.   This (hiding it internally) would be particularly useful   if the covariance matrix changes with time (but repeat).</li> </ul>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.Left","title":"<code>Left</code>","text":"<p>Left sqrt.</p> <p><code>L</code> such that $$ C = L L^T .$$</p> <p>Note that <code>L</code> is typically rectangular, but not triangular, and that its width is somewhere betwen the rank and <code>M</code>.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.M","title":"<code>M</code>","text":"<p><code>ndims</code></p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.Right","title":"<code>Right</code>","text":"<p>Right sqrt. Ref <code>CovMat.Left</code>.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.V","title":"<code>V</code>","text":"<p>Eigenvectors, output corresponding to ews.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.ews","title":"<code>ews</code>","text":"<p>Eigenvalues. Only outputs the positive values (i.e. len(ews)==rk).</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.full","title":"<code>full</code>","text":"<p>Full covariance matrix</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.kind","title":"<code>kind</code>","text":"<p>Form in which matrix was specified.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.rk","title":"<code>rk</code>","text":"<p>Rank, i.e. the number of positive eigenvalues.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.trunc","title":"<code>trunc</code>","text":"<p>Truncation threshold.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.__init__","title":"<code>__init__(data, kind='full_or_diag', trunc=1.0)</code>","text":"<p>Construct object.</p> <p>The covariance (say P) can be input (specified in the following ways):</p> <pre><code>kind    | data\n--------|-------------\n'full'  | full M-by-M array (P)\n'diag'  | diagonal of P (assumed diagonal)\n'E'     | ensemble (N-by-M) with sample cov P\n'A'     | as 'E', but pre-centred by mean(E,axis=0)\n'Right' | any R such that P = R.T@R (e.g. weighted form of 'A')\n'Left'  | any L such that P = L@L.T\n</code></pre>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.diag","title":"<code>diag()</code>","text":"<p>Diagonal of covariance matrix</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.has_done_EVD","title":"<code>has_done_EVD()</code>","text":"<p>Whether or not eigenvalue decomposition has been done for matrix.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.pinv","title":"<code>pinv()</code>","text":"<p>Pseudo-inverse. Uses trunc-level.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.sym_sqrt","title":"<code>sym_sqrt()</code>","text":"<p>S such that C = S@S (and i.e. S is square). Uses trunc-level.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.sym_sqrt_inv","title":"<code>sym_sqrt_inv()</code>","text":"<p>S such that C^{-1} = S@S (and i.e. S is square). Uses trunc-level.</p>"},{"location":"reference/tools/matrices/#tools.matrices.CovMat.transform_by","title":"<code>transform_by(fun)</code>","text":"<p>Generalize scalar functions to covariance matrices (via Taylor expansion).</p>"},{"location":"reference/tools/matrices/#tools.matrices.lazy_property","title":"<code>lazy_property</code>","text":"<p>Lazy evaluation of property.</p> <p>Should represent non-mutable data, as it replaces itself.</p> <p>From https://stackoverflow.com/q/3012421</p>"},{"location":"reference/tools/matrices/#tools.matrices.basis_beginning_with_ones","title":"<code>basis_beginning_with_ones(ndim)</code>","text":"<p>Basis whose first vector is ones(ndim).</p>"},{"location":"reference/tools/matrices/#tools.matrices.chol_reduce","title":"<code>chol_reduce(Right)</code>","text":"<p>Return rnk-by-ndim R such that <code>R.T@R - R.T@R \u2248 0</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from dapper.stats import mean0\n&gt;&gt;&gt; X = mean0(np.random.randn(20, 5), axis=1)\n&gt;&gt;&gt; C = X.T @ X\n&gt;&gt;&gt; # sla.cholesky(C) throws error\n&gt;&gt;&gt; R = chol_reduce(X)\n&gt;&gt;&gt; R.shape[1] == 5\nTrue\n</code></pre>"},{"location":"reference/tools/matrices/#tools.matrices.funm_psd","title":"<code>funm_psd(a, fun, check_finite=False)</code>","text":"<p>Matrix function evaluation for pos-sem-def mat.</p> <p>Adapted from <code>sla.funm</code> doc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def sqrtm_psd(A):\n...     return funm_psd(A, sqrt)\n</code></pre>"},{"location":"reference/tools/matrices/#tools.matrices.genOG","title":"<code>genOG(M)</code>","text":"<p>Generate random orthonormal matrix.</p>"},{"location":"reference/tools/matrices/#tools.matrices.genOG_1","title":"<code>genOG_1(N, opts=())</code>","text":"<p>Random orthonormal mean-preserving matrix.</p> <p>Source: ienks code of Sakov/Bocquet.</p>"},{"location":"reference/tools/matrices/#tools.matrices.genOG_modified","title":"<code>genOG_modified(M, opts=(0, 1.0))</code>","text":"<p>Do <code>genOG</code> with modifications.</p> <p>Caution: although 'degree' \u2208 (0,1) for all versions,          they're not supposed going to be strictly equivalent.</p> <p>Testing: scripts/sqrt_rotations.py</p>"},{"location":"reference/tools/matrices/#tools.matrices.randcorr","title":"<code>randcorr(M)</code>","text":"<p>(Makeshift) random corr mat.</p>"},{"location":"reference/tools/matrices/#tools.matrices.randcov","title":"<code>randcov(M)</code>","text":"<p>(Makeshift) random cov mat.</p>"},{"location":"reference/tools/multiproc/","title":"multiproc","text":"<p>Paralellisation via multiprocessing. Limit num. of CPUs used by <code>numpy</code> to 1.</p>"},{"location":"reference/tools/multiproc/#tools.multiproc.Pool","title":"<code>Pool(NPROC=None)</code>","text":"<p>Initialize a multiprocessing <code>Pool</code>.</p> <ul> <li>Uses <code>pathos/dill</code> for serialisation.</li> <li>Provides unified interface for multiprocessing on/off (as a function of NPROC).</li> </ul> <p>There is some overhead associated with the pool creation, so you likely want to re-use a pool rather than repeatedly creating one. Consider using <code>functools.partial</code> to fix kwargs.</p> <p>Note</p> <p>In contrast to reading, in-place writing does not work with multiprocessing. This changes with \"shared\" arrays, but that has not been tested here. By contrast, multi*threading* shares the process memory, but was significantly slower in the tested (pertinent) cases.</p> <p>Warning</p> <p><code>multiprocessing</code> does not mix with <code>matplotlib</code>, so ensure <code>func</code> does not reference <code>xp.stats.LP_instance</code>. In fact, <code>func</code> should not reference <code>xp</code> at all, because it takes time to serialize.</p> <p>See example use in <code>mods.QG</code> and <code>da_methods.ensemble.LETKF</code>.</p>"},{"location":"reference/tools/progressbar/","title":"progressbar","text":"<p>Make <code>progbar</code> (wrapper around <code>tqdm</code>) and <code>read1</code>.</p>"},{"location":"reference/tools/progressbar/#tools.progressbar.new_term_settings","title":"<code>new_term_settings()</code>","text":"<p>Make stdin.read non-echo and non-block</p>"},{"location":"reference/tools/progressbar/#tools.progressbar.pdesc","title":"<code>pdesc(desc)</code>","text":"<p>Get progbar description by introspection.</p>"},{"location":"reference/tools/progressbar/#tools.progressbar.read1","title":"<code>read1()</code>","text":"<p>Get 1 character. Non-blocking, non-echoing.</p>"},{"location":"reference/tools/randvars/","title":"randvars","text":"<p>Classes of random variables.</p>"},{"location":"reference/tools/randvars/#tools.randvars.GaussRV","title":"<code>GaussRV</code>","text":"<p>               Bases: <code>RV_with_mean_and_cov</code></p> <p>Gaussian (Normal) multivariate random variable.</p>"},{"location":"reference/tools/randvars/#tools.randvars.LaplaceParallelRV","title":"<code>LaplaceParallelRV</code>","text":"<p>               Bases: <code>RV_with_mean_and_cov</code></p> <p>A NON-elliptical multivariate version of Laplace (double exponential) RV.</p>"},{"location":"reference/tools/randvars/#tools.randvars.LaplaceRV","title":"<code>LaplaceRV</code>","text":"<p>               Bases: <code>RV_with_mean_and_cov</code></p> <p>Laplace (double exponential) multivariate random variable.</p> <p>This is an elliptical generalization. Ref: Eltoft (2006) \"On the Multivariate Laplace Distribution\".</p>"},{"location":"reference/tools/randvars/#tools.randvars.RV","title":"<code>RV</code>","text":"<p>               Bases: <code>NicePrint</code></p> <p>Class to represent random variables.</p>"},{"location":"reference/tools/randvars/#tools.randvars.RV.__init__","title":"<code>__init__(M, **kwargs)</code>","text":"<p>Initalization arguments:</p> <ul> <li><code>M    &lt;int&gt;</code> : ndim</li> <li><code>is0  &lt;bool&gt;</code> : if <code>True</code>, the random variable is identically 0</li> <li><code>func &lt;func(N)&gt;</code> : use this sampling function. Example:                    <code>RV(M=4,func=lambda N: rng.random((N,4))</code></li> <li><code>file &lt;str&gt;</code> : draw from file. Example:                    <code>RV(M=4,file=dpr.rc.dirs.data/'tmp.npz')</code></li> </ul> <p>The following kwords (versions) are available, but should not be used for anything serious (use instead subclasses, like <code>GaussRV</code>).</p> <ul> <li><code>icdf &lt;func(x)&gt;</code> : marginal/independent  \"inverse transform\" sampling.                    Example: <code>RV(M=4,icdf = scipy.stats.norm.ppf)</code></li> <li><code>cdf &lt;func(x)&gt;</code>  : as icdf, but with approximate icdf, from interpolation.                    Example: <code>RV(M=4,cdf = scipy.stats.norm.cdf)</code></li> <li><code>pdf  &lt;func(x)&gt;</code> : \"acceptance-rejection\" sampling. Not implemented.</li> </ul>"},{"location":"reference/tools/randvars/#tools.randvars.RV_with_mean_and_cov","title":"<code>RV_with_mean_and_cov</code>","text":"<p>               Bases: <code>RV</code></p> <p>Generic multivariate random variable characterized by mean and cov.</p> <p>This class must be subclassed to provide sample(), i.e. its main purpose is provide a common convenience constructor.</p>"},{"location":"reference/tools/randvars/#tools.randvars.RV_with_mean_and_cov.__init__","title":"<code>__init__(mu=0, C=0, M=None)</code>","text":"<p>Init allowing for shortcut notation.</p>"},{"location":"reference/tools/randvars/#tools.randvars.RV_with_mean_and_cov.sample","title":"<code>sample(N)</code>","text":"<p>Sample N realizations. Returns N-by-M (ndim) sample matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plt.scatter(*(UniRV(C=randcov(2)).sample(10**4).T))\n</code></pre>"},{"location":"reference/tools/randvars/#tools.randvars.StudRV","title":"<code>StudRV</code>","text":"<p>               Bases: <code>RV_with_mean_and_cov</code></p> <p>Student-t multivariate random variable.</p> <p>Assumes the covariance exists, which requires <code>degreee-of-freedom (dof) &gt; 1+ndim</code>. Also requires that dof be integer, since chi2 is sampled via Gaussians.</p>"},{"location":"reference/tools/randvars/#tools.randvars.UniParallelRV","title":"<code>UniParallelRV</code>","text":"<p>               Bases: <code>RV_with_mean_and_cov</code></p> <p>Uniform multivariate random variable.</p> <p>Has a parallelogram-shaped support, as determined by the cholesky factor applied to the (corners of) the hypercube.</p>"},{"location":"reference/tools/randvars/#tools.randvars.UniRV","title":"<code>UniRV</code>","text":"<p>               Bases: <code>RV_with_mean_and_cov</code></p> <p>Uniform multivariate random variable.</p> <p>Has an elliptic-shape support. Ref: Voelker et al. (2017) \"Efficiently sampling vectors and coordinates from the n-sphere and n-ball\"</p>"},{"location":"reference/tools/rounding/","title":"rounding","text":"<p>Functions for rounding numbers.</p>"},{"location":"reference/tools/rounding/#tools.rounding.UncertainQtty","title":"<code>UncertainQtty</code>","text":"<p>Data container associating uncertainty (confidence) to a quantity.</p> <p>Includes intelligent rounding and printing functionality.</p> <p>Usually, the precision parameter will be set to the (potentially estimated) standard deviation of an uncertain quantity. However, this class in itself does not define the <code>prec</code> attribute by anything else than what it does: impact the rounding &amp; printing of <code>val</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for c in [.01, .1, .2, .9, 1]:\n...    print(UncertainQtty(1.2345, c))\n1.23 \u00b10.01\n1.2 \u00b10.1\n1.2 \u00b10.2\n1.2 \u00b10.9\n1 \u00b11\n</code></pre> <pre><code>&gt;&gt;&gt; for c in [.01, 1e-10, 1e-17, 0]:\n...    print(UncertainQtty(1.2, c))\n1.20 \u00b10.01\n1.2000000000 \u00b11e-10\n1.19999999999999996 \u00b11e-17\n1.2000000000 \u00b10\n</code></pre> <p>Note that in the case of a confidence of exactly 0, it defaults to 10 decimal places. Meanwhile, a NaN confidence yields printing using <code>rc.sigfig</code>:</p> <pre><code>&gt;&gt;&gt; print(UncertainQtty(1.234567, np.nan))\n1.235 \u00b1nan\n</code></pre> <p>Also note the effect of large uncertainty:</p> <pre><code>&gt;&gt;&gt; for c in [1, 9, 10, 11, 20, 100, np.inf]:\n...    print(UncertainQtty(12, c))\n12 \u00b11\n12 \u00b19\n10 \u00b110\n10 \u00b110\n10 \u00b120\n0 \u00b1100\n0 \u00b1inf\n</code></pre>"},{"location":"reference/tools/rounding/#tools.rounding.UncertainQtty.__repr__","title":"<code>__repr__()</code>","text":"<p>Essentially the same as <code>__str__</code>.</p>"},{"location":"reference/tools/rounding/#tools.rounding.UncertainQtty.__str__","title":"<code>__str__()</code>","text":"<p>Returns 'val \u00b1prec', using tools.rounding.UncertainQtty.round</p> <p>and some finesse.</p>"},{"location":"reference/tools/rounding/#tools.rounding.UncertainQtty.round","title":"<code>round()</code>","text":"<p>Round intelligently.</p> <ul> <li><code>prec</code> to 1 sig.fig.</li> <li><code>val</code> to <code>round2(val, prec)</code>.</li> </ul>"},{"location":"reference/tools/rounding/#tools.rounding.is_whole","title":"<code>is_whole(x, **kwargs)</code>","text":"<p>Check if a number is a whole/natural number to precision given by <code>np.isclose</code>.</p> <p>For actual type checking, use <code>isinstance(x, (int, np.integer))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float or ndarray</code> <p>Values to be checked</p> required <p>Returns:</p> Name Type Description <code>l</code> <code>bool</code> <p>True if rounded x is close to x, otherwise False</p>"},{"location":"reference/tools/rounding/#tools.rounding.log10int","title":"<code>log10int(x)</code>","text":"<p>Compute decimal order, rounded down.</p> <p>Conversion to <code>int</code> means that we cannot return nan's or \u00b1 infinity, even though this could be meaningful. Instead, we return integers of magnitude a little less than IEEE floating point max/min-ima instead. This avoids a lot of clauses in the parent/callers to this function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; log10int([1e-1, 1e-2, 1, 3, 10, np.inf, -np.inf, np.nan])\narray([  -1,   -2,    0,    0,    1,  300, -300, -300])\n</code></pre>"},{"location":"reference/tools/rounding/#tools.rounding.np_vectorize","title":"<code>np_vectorize(f)</code>","text":"<p>Like <code>np.vectorize</code>, but with some embellishments.</p> <ul> <li>Includes <code>functools.wraps</code></li> <li>Applies <code>.item()</code> to output if input was a scalar.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>Your function.</p> required <p>Returns:</p> Name Type Description <code>vectorized</code> <code>callable</code> <p>Your function, now element-wise applicable to an iterable.</p>"},{"location":"reference/tools/rounding/#tools.rounding.round2","title":"<code>round2(x, prec=1.0)</code>","text":"<p>Round x to the decimal order appropriate for the precision.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Value to be rounded.</p> required <code>prec</code> <code>float</code> <p>Precision, before prettify, which is given by $$ \\text{prec} = 10^{\\text{floor}(-\\log_{10}|\\text{prec}|)} $$</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Rounded value (always a float).</code> See Also <p><code>round2sigfig</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; round2(1.65, 0.543)\n1.6\n&gt;&gt;&gt; round2(1.66, 0.543)\n1.7\n&gt;&gt;&gt; round2(1.65, 1.234)\n2.0\n</code></pre>"},{"location":"reference/tools/rounding/#tools.rounding.round2sigfig","title":"<code>round2sigfig(x, sigfig=1)</code>","text":"<p>Round to significant figures.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Value to be rounded.</p> required <code>sigfig</code> <p>Number of significant figures to include.</p> <code>1</code> <p>Returns:</p> Type Description <code>rounded value (always a float).</code> See Also <p>np.round : rounds to a given number of decimals. <code>round2</code> : rounds to a given precision.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; round2sigfig(1234.5678, 1)\n1000.0\n&gt;&gt;&gt; round2sigfig(1234.5678, 4)\n1235.0\n&gt;&gt;&gt; round2sigfig(1234.5678, 6)\n1234.57\n</code></pre>"},{"location":"reference/tools/seeding/","title":"seeding","text":"<p>Random number generation.</p>"},{"location":"reference/tools/seeding/#tools.seeding.set_seed","title":"<code>set_seed(sd='clock')</code>","text":"<p>Set state of DAPPER random number generator.</p>"},{"location":"reference/tools/series/","title":"series","text":"<p>Time series management and processing.</p>"},{"location":"reference/tools/series/#tools.series.DataSeries","title":"<code>DataSeries</code>","text":"<p>               Bases: <code>StatPrint</code></p> <p>Basically just an <code>np.ndarray</code>. But adds:</p> <ul> <li>Possibility of adding attributes.</li> <li>The class (type) provides way to acertain if an attribute is a series.</li> </ul> <p>Note: subclassing <code>ndarray</code> is too dirty =&gt; We'll just use the <code>array</code> attribute, and provide <code>{s,g}etitem</code>.</p>"},{"location":"reference/tools/series/#tools.series.FAUSt","title":"<code>FAUSt</code>","text":"<p>               Bases: <code>DataSeries</code>, <code>StatPrint</code></p> <p>Container for time series of a statistic from filtering.</p> <p>Four attributes, each of which is an ndarray:</p> <ul> <li><code>.f</code> for forecast      , <code>(Ko+1,)+item_shape</code></li> <li><code>.a</code> for analysis      , <code>(Ko+1,)+item_shape</code></li> <li><code>.s</code> for smoothed      , <code>(Ko+1,)+item_shape</code></li> <li><code>.u</code> for universial/all, <code>(K   +1,)+item_shape</code></li> </ul> <p>If <code>store_u=False</code>, then <code>.u</code> series has shape <code>(1,)+item_shape</code>, wherein only the most-recently-written item is stored.</p> <p>Series can also be indexed as in</p> <pre><code>self[ko,'a']\nself[whatever,ko,'a']\n# ... and likewise for 'f' and 's'. For 'u', can use:\nself[k,'u']\nself[k,whatever,'u']\n</code></pre> <p>Note</p> <p>If a data series only pertains to analysis times, then you should use a plain np.array instead.</p>"},{"location":"reference/tools/series/#tools.series.FAUSt.__init__","title":"<code>__init__(K, Ko, item_shape, store_u, store_s, **kwargs)</code>","text":"<p>Construct object.</p> <ul> <li><code>item_shape</code> : shape of an item in the series.</li> <li><code>store_u</code>    : if False: only the current value is stored.</li> <li><code>kwargs</code>     : passed on to ndarrays.</li> </ul>"},{"location":"reference/tools/series/#tools.series.RollingArray","title":"<code>RollingArray</code>","text":"<p>ND-Array that implements \"leftward rolling\" along axis 0.</p> <p>Used for data that gets plotted in sliding graphs.</p>"},{"location":"reference/tools/series/#tools.series.StatPrint","title":"<code>StatPrint</code>","text":"<p>               Bases: <code>NicePrint</code></p> <p>Set <code>NicePrint</code> options suitable for stats.</p>"},{"location":"reference/tools/series/#tools.series.auto_cov","title":"<code>auto_cov(xx, nlags=4, zero_mean=False, corr=False)</code>","text":"<p>Auto covariance function, computed along axis 0.</p> <ul> <li><code>nlags</code>: max lag (offset) for which to compute acf.</li> <li><code>corr</code> : normalize acf by <code>acf[0]</code> so as to return auto-CORRELATION.</li> </ul> <p>With <code>corr=True</code>, this is identical to <code>statsmodels.tsa.stattools.acf(xx,True,nlags)</code></p>"},{"location":"reference/tools/series/#tools.series.estimate_corr_length","title":"<code>estimate_corr_length(xx)</code>","text":"<p>Estimate the correlation length of a time series.</p> <p>For explanation, see <code>mods.LA.homogeneous_1D_cov</code>. Also note that, for exponential corr function, as assumed here,</p> \\[\\text{corr}(L) = \\exp(-1) \\approx 0.368\\]"},{"location":"reference/tools/series/#tools.series.fit_acf_by_AR1","title":"<code>fit_acf_by_AR1(acf_empir, nlags=None)</code>","text":"<p>Fit an empirical auto cov function (ACF) by that of an AR1 process.</p> <ul> <li><code>acf_empir</code>: auto-corr/cov-function.</li> <li><code>nlags</code>: length of ACF to use in AR(1) fitting</li> </ul>"},{"location":"reference/tools/series/#tools.series.mean_with_conf","title":"<code>mean_with_conf(xx)</code>","text":"<p>Compute the mean of a 1d iterable <code>xx</code>.</p> <p>Also provide confidence of mean, as estimated from its correlation-corrected variance.</p>"},{"location":"reference/tools/series/#tools.series.monitor_setitem","title":"<code>monitor_setitem(cls)</code>","text":"<p>Modify cls to track of whether its <code>__setitem__</code> has been called.</p> <p>See sub.py for a sublcass solution (drawback: creates a new class).</p>"},{"location":"reference/tools/viz/","title":"viz","text":"<p>Plot utilities.</p> <p>Modules:</p> Name Description <code>series</code> <p>Time series management and processing.</p>"},{"location":"reference/tools/viz/#tools.viz.NoneDict","title":"<code>NoneDict</code>","text":"<p>               Bases: <code>DotDict</code></p> <p>DotDict with getattr that (silently) falls back to <code>None</code>.</p>"},{"location":"reference/tools/viz/#tools.viz.amplitude_animation","title":"<code>amplitude_animation(EE, dt=None, interval=1, periodicity=None, blit=True, fignum=None, repeat=False)</code>","text":"<p>Animation of line chart.</p> <p>Using an ensemble of the shape (time, ensemble size, state vector length).</p> <p>Parameters:</p> Name Type Description Default <code>EE</code> <code>ndarray</code> <p>Ensemble arry of the shape (K, N, Nx). K is the length of time, N is the ensemble size, and Nx is the length of state vector.</p> required <code>dt</code> <code>float</code> <p>Time interval of each frame.</p> <code>None</code> <code>interval</code> <code>float</code> <p>Delay between frames in milliseconds. Defaults to 200.</p> <code>1</code> <code>periodicity</code> <code>bool</code> <p>The mode of the wrapping. \"+1\": the first element is appended after the last. \"\u00b105\": adding the midpoint of the first and last elements. Default: \"+1\"</p> <code>None</code> <code>blit</code> <code>bool</code> <p>Controls whether blitting is used to optimize drawing. Default: True</p> <code>True</code> <code>fignum</code> <code>int</code> <p>Figure index. Default: None</p> <code>None</code> <code>repeat</code> <code>bool</code> <p>If True, repeat the animation. Default: False</p> <code>False</code>"},{"location":"reference/tools/viz/#tools.viz.axis_scale_by_array","title":"<code>axis_scale_by_array(ax, arr, axis='y', nbins=3)</code>","text":"<p>Scale axis so that the arr entries appear equidistant.</p> <p>The full transformation is piecewise-linear.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>axes</code> required <code>arr</code> <code>ndarray</code> <p>Array for plotting</p> required <code>axis</code> <code>str</code> <p>Scaled axis, which can be 'x', 'y' or 'z'. Defaults: 'y'</p> <code>'y'</code> <code>nbins</code> <code>int</code> <p>Number of major ticks. Defaults: 3</p> <code>3</code>"},{"location":"reference/tools/viz/#tools.viz.collapse_str","title":"<code>collapse_str(string, length=6)</code>","text":"<p>Truncate a string (in the middle) to the given <code>length</code></p>"},{"location":"reference/tools/viz/#tools.viz.default_fig_adjustments","title":"<code>default_fig_adjustments(tables, xticks_from_data=False)</code>","text":"<p>Beautify. These settings do not generalize well.</p>"},{"location":"reference/tools/viz/#tools.viz.default_styles","title":"<code>default_styles(coord, baseline_legends=False)</code>","text":"<p>Quick and dirty (but somewhat robust) styling.</p>"},{"location":"reference/tools/viz/#tools.viz.estimate_good_plot_length","title":"<code>estimate_good_plot_length(xx, tseq=None, mult=100)</code>","text":"<p>Estimate the range of the xx slices for plotting.</p> <p>The length is based on the estimated time scale (wavelength) of the system. Provide sensible fall-backs (better if tseq is supplied).</p> <p>Parameters:</p> Name Type Description Default <code>xx</code> <code>ndarray</code> <p>Plotted array</p> required <code>tseq</code> <code>[`tools.chronos.Chronology`][]</code> <p>object with property dko. Defaults: None</p> <code>None</code> <code>mult</code> <code>int</code> <p>Number of waves for plotting. Defaults: 100</p> <code>100</code> <p>Returns:</p> Name Type Description <code>K</code> <code>int</code> <p>length for plotting</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; K_lag = estimate_good_plot_length(stats.xx, tseq, mult=80)\n</code></pre>"},{"location":"reference/tools/viz/#tools.viz.integer_hist","title":"<code>integer_hist(E, N, centrd=False, weights=None, **kwargs)</code>","text":"<p>Histogram for integers.</p> <p>Parameters:</p> Name Type Description Default <code>E</code> <code>ndarray</code> <p>Ensemble array.</p> required <code>N</code> <code>int</code> <p>Number of histogram bins.</p> required <code>centrd</code> <code>bool</code> <p>If True, each bin is centered in the midpoint. Default: False</p> <code>False</code> <code>weights</code> <code>float</code> <p>Weights for histogram. Default: None</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>keyword arguments for matplotlib.hist</p> <code>{}</code>"},{"location":"reference/tools/viz/#tools.viz.make_label","title":"<code>make_label(coord, no_key=NO_KEY, exclude=())</code>","text":"<p>Make label from coord.</p>"},{"location":"reference/tools/viz/#tools.viz.not_available_text","title":"<code>not_available_text(ax, txt=None, fs=20)</code>","text":"<p>Plot given text on the figure</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>axes</code> required <code>txt</code> <code>str</code> <p>Printed text. Defaults: '[Not available]'</p> <code>None</code> <code>fs</code> <code>float</code> <p>Font size. Defaults: 20.</p> <code>20</code>"},{"location":"reference/tools/viz/#tools.viz.plot_err_components","title":"<code>plot_err_components(stats)</code>","text":"<p>Plot components of the error.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>Stats</code> required Note <p>It was chosen to <code>plot(ii, mean_in_time(abs(err_i)))</code>, and thus the corresponding spread measure is MAD. If one chose instead: <code>plot(ii, std_spread_in_time(err_i))</code>, then the corresponding measure of spread would have been <code>spread</code>. This choice was made in part because (wrt. subplot 2) the singular values (<code>svals</code>) correspond to rotated MADs, and because <code>rms(umisf)</code> seems too convoluted for interpretation.</p>"},{"location":"reference/tools/viz/#tools.viz.plot_hovmoller","title":"<code>plot_hovmoller(xx, tseq=None)</code>","text":"<p>Plot Hovm\u00f6ller diagram.</p> <p>Parameters:</p> Name Type Description Default <code>xx</code> <code>ndarray</code> <p>Plotted array</p> required <code>tseq</code> <code>[`tools.chronos.Chronology`][]</code> <p>object with property dko. Defaults: None</p> <code>None</code>"},{"location":"reference/tools/viz/#tools.viz.plot_pause","title":"<code>plot_pause(interval)</code>","text":"<p>Like <code>plt.pause</code>, but better.</p> <ul> <li>Actually works in Jupyter notebooks.</li> <li>In GUI windows: don't focus window.   NB: doesn't create windows either.   For that, use <code>plt.pause</code> or <code>plt.show</code> instead.</li> </ul>"},{"location":"reference/tools/viz/#tools.viz.plot_rank_histogram","title":"<code>plot_rank_histogram(stats)</code>","text":"<p>Plot rank histogram of ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> required"},{"location":"reference/tools/viz/#tools.viz.set_ilim","title":"<code>set_ilim(ax, i, Min=None, Max=None)</code>","text":"<p>Set bounds on axis i.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>axes</code> required <code>i</code> <code>int</code> <p>1: x-axis; 2: y-axis; 3: z-axis</p> required <code>Min</code> <code>float</code> <p>Lower bound limit. Defaults: None</p> <code>None</code> <code>Max</code> <code>float</code> <p>Upper bound limit. Defaults: None</p> <code>None</code>"},{"location":"reference/tools/viz/#tools.viz.setup_wrapping","title":"<code>setup_wrapping(M, periodicity=None)</code>","text":"<p>Make state indices representative for periodic system.</p> <p>More accurately: Wrap the state indices and create a function that does the same for state vectors (or and ensemble thereof).</p> <p>Parameters:</p> Name Type Description Default <code>M</code> <code>int</code> <p>Length of the periodic domain</p> required <code>periodicity</code> <code>bool</code> <p>The mode of the wrapping. \"+1\": the first element is appended after the last. \"\u00b105\": adding the midpoint of the first and last elements. Default: \"+1\"</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ii</code> <code>ndarray</code> <p>indices of periodic domain</p> <code>wrap</code> <code>func</code> <p>transform non-periodic data into periodic data</p>"},{"location":"reference/tools/viz/#tools.viz.stretch","title":"<code>stretch(a, b, factor=1, int_=False)</code>","text":"<p>Stretch distance <code>a-b</code> by factor.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Lower bound of domain.</p> required <code>b</code> <code>float</code> <p>Upper bound of domain.</p> required <code>factor</code> <code>float</code> <p>Streching factor. Defaults: 1</p> <code>1</code> <code>int_</code> <code>bool</code> <p>If True, the domain bounds are integer. Defaults: False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>a</code> <code>float</code> <p>Lower bound of domain.</p> <code>b</code> <code>float</code> <p>Upper bound of domain.</p>"},{"location":"reference/tools/viz/#tools.viz.xtrema","title":"<code>xtrema(xx, axis=None)</code>","text":"<p>Get minimum and maximum of a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>xx</code> <code>ndarray</code> required <code>axis</code> <code>int</code> <p>Specific axis for min and max. Defaults: None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>a</code> <code>float</code> <p>min value</p> <code>b</code> <code>float</code> <p>max value</p>"},{"location":"reference/tools/remote/","title":"remote","text":"<p>Tools related to running experimentes remotely</p> <p>Requires rsync, gcloud and ssh access to the DAPPER cluster.</p> <p>Modules:</p> Name Description <code>uplink</code> <p>Tools related to running experimentes remotely</p>"},{"location":"reference/tools/remote/uplink/","title":"uplink","text":"<p>Tools related to running experimentes remotely</p> <p>Requires rsync, gcloud and ssh access to the DAPPER cluster.</p>"},{"location":"reference/tools/remote/uplink/#tools.remote.uplink.SubmissionConnection","title":"<code>SubmissionConnection</code>","text":"<p>Establish multiplexed ssh to a given submit-node for a given xps_path.</p>"},{"location":"reference/tools/remote/uplink/#tools.remote.uplink.SubmissionConnection.remote_cmd","title":"<code>remote_cmd(cmd_string, **kwargs)</code>","text":"<p>Run command at self.host via multiplexed ssh.</p>"},{"location":"reference/tools/remote/uplink/#tools.remote.uplink.get_ip","title":"<code>get_ip(instance)</code>","text":"<p>Get ip-address of instance.</p> <p>NB: the use of IP rather than the <code>Host</code> listed in <code>.ssh/config</code> (eg <code>condor-submit.us-central1-f.mc-tut</code>, as generated by <code>gcloud compute config-ssh</code>) requires <code>AddKeysToAgent yes</code> under <code>Host *</code> in <code>.ssh/config</code>, and that you've already logged into the instance once using (eg) <code>ssh condor-submit.us-central1-f.mc-tut</code>.</p>"},{"location":"reference/tools/remote/uplink/#tools.remote.uplink.sub_run","title":"<code>sub_run(*args, check=True, capture_output=True, text=True, **kwargs)</code>","text":"<p>Do <code>subprocess.run</code>, with responsive defaults.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gitfiles = sub_run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", \"HEAD\"])  # or:\n&gt;&gt;&gt; # gitfiles = sub_run(\"git ls-tree -r --name-only HEAD\", shell=True)\n</code></pre>"},{"location":"reference/tools/remote/uplink/#tools.remote.uplink.submit_job_GCP","title":"<code>submit_job_GCP(xps_path, **kwargs)</code>","text":"<p>GCP/HTCondor launcher</p>"}]}