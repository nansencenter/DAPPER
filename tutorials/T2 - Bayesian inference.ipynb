{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gaussian (i.e. Normal) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the random variable with a Gaussian distribution with mean $\\mu$ (`mu`) and variance $P$. We write its probability density function (**pdf**) as\n",
    "$$ p(x) = N(x|\\mu,P) = (2 \\pi P)^{-1/2} e^{-(x-\\mu)^2/2P} \\, .  \\qquad \\qquad (1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exc 2.2:** Code it up (complete the code below)! Hints:\n",
    "* Note that `**` is the power operator in Python.\n",
    "* As in Matlab, $e^x$ is available as `exp(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Univariate (scalar), Gaussian pdf\n",
    "def pdf_G_1(x,mu,P):\n",
    "    # pdf_values = ### INSERT ANSWER HERE ###\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('pdf_G_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu  = 0                 # mean     of distribution\n",
    "P   = 25                # variance of distribution\n",
    "P12 = sqrt(P)           # std. dev of distribution\n",
    "\n",
    "# Plotting\n",
    "N  = 201                # num of grid points\n",
    "xx = linspace(-20,20,N) # grid\n",
    "dx = xx[1]-xx[0]        # grid spacing\n",
    "pp = pdf_G_1(xx,mu,P)   # pdf values\n",
    "plt.subplot(211)        # allocate plot panel\n",
    "plt.plot(xx,pp);        # plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could for example be the pdf of a stochastic noise variable. It could also describe our uncertainty about a parameter (or state), which we model as randomness in the Bayesian paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.4:** Change `P` in the above code, and re-run the cell. Look at the figure.\n",
    " * How does the pdf curve change when you increase P?\n",
    " * Re-set `P=25` and re-run (this is a convienient value for examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.6:** Recall $p(x)$ from eqn (1). The following are helpful points to remember how it looks. Use pen, paper, and calculus. Hint: it's typically easier to analyse $\\log p(x)$ rather than $p(x)$ itself.\n",
    " * Where is the location of the mode (maximum) of the distribution? I.e. where $\\frac{d p(x)}{d x} = 0$.\n",
    " * Where is the inflection point? I.e. where $\\frac{d^2 p(x)}{d x^2} = 0$.\n",
    " * What is the value of $\\frac{d^2 \\log p(x)}{d x^2}$ at the mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The multivariate (i.e. vector) case\n",
    "Here's the pdf of the *multivariate* Gaussian:\n",
    "\\begin{align}\n",
    "N(x|\\mu,P) &= |2 \\pi P|^{-\\frac{1}{2}} e^{-\\frac{1}{2}\\|x-\\mu\\|^2_P} \\, , \\\\\\\n",
    "\\end{align}\n",
    "where $|.|$ represents the determinant, and $\\|.\\|_W$ represents the norm with weighting: $\\|x\\|^2_W = x^T W^{-1} x$.\n",
    "\n",
    "The following implements this pdf. Take a moment to digest the code; in particular, it should be noted that `pdf_G_m()` can accept multiple `x` vectors at once (assembled into a matrix), whence the `xx` naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(xx,W):\n",
    "    # Computes the norm of each row vector of xx, as weighted by W.\n",
    "    return np.sum((xx @ inv(W)) * xx, axis=1)\n",
    "\n",
    "def pdf_G_m(xx,mu,P):\n",
    "    return 1/sqrt(det(2*pi*P))*exp(-0.5*weighted_norm22(xx-mu,P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (equi-density) curves.\n",
    "The plot appears in the above figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_2_array(grid): return array([xi.ravel() for xi in grid]).T\n",
    "def square_reshape(X):  return X.reshape(int(sqrt(len(X))),-1)\n",
    "\n",
    "grid = np.meshgrid(xx,xx)\n",
    "grid = list_2_array(grid)\n",
    "\n",
    "pp = pdf_G_m(grid, 0, P*array([[1,0.7],[0.7,1]]))\n",
    "pp = square_reshape(pp)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.contour(xx,xx,pp);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.8:**\n",
    " * Set the correlation to 0. How do the contours look?\n",
    " * Set the correlations to 0.99. How do the contours look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.9:** Go play the [correlation game](http://guessthecorrelation.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' rule\n",
    "Bayes' rule is how we do inference. For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "$$ p(x|y) = \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\qquad \\qquad (2)$$\n",
    "\n",
    "or, in words:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; = \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}}\n",
    "$$.\n",
    "\n",
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability#Kolmogorov_definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers generally work with discrete, numerical representations of mathematical entities.\n",
    "Numerically, pdfs may be represented by their `values` on a grid, such as `xx` from above. Bayes' rule (2) then consists of *(grid-)point-wise* multiplication, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values,lklhd_values,dx):\n",
    "    pp = prior_values * lklhd_values   # pointwise multiplication\n",
    "    posterior_values = pp/(sum(pp)*dx) # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.12:** Why does `Bayes_rule()` not need the values of the denominator, $p(y)$, as input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR grid normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since normalization is so simple, we often don't bother to do it until it's strictly necessary. Therefore we often simplify Bayes' rule (2) as\n",
    "$$ p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\qquad \\qquad (3) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The code below show's Bayes' rule in action. Again, remember that the only thing it's doing is multiplying the prior and likelihood at each gridpoint. Move the sliders with the arrow keys to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "b = 0 # prior mean\n",
    "B = 1 # prior variance\n",
    "\n",
    "@interact(y=(-10,10,1),log_R=(-2,5,0.5))\n",
    "def animate_Gaussian_Bayes(y=4.0,log_R=1):\n",
    "\n",
    "    R = exp(log_R)\n",
    "\n",
    "    prior     = lambda x: pdf_G_1(x,b,B)\n",
    "    lklhd     = lambda x: pdf_G_1(y,x,R)\n",
    "    \n",
    "    post_vals = Bayes_rule(prior(xx),lklhd(xx),xx[1]-xx[0])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(xx,prior(xx)     ,label='prior N(x|0,1)')\n",
    "    plt.plot(xx,lklhd(xx)     ,label='likelihood N(y|x,R)')\n",
    "    plt.plot(xx,post_vals     ,label='posterior - pointwise')\n",
    "    \n",
    "    ### Uncomment this block AFTER doing the exercise ###\n",
    "    ### that defines Bayes_rule_Gaussian()            ###\n",
    "    #mu, P     = Bayes_rule_Gaussian(b,B,y,R)\n",
    "    #postr     = lambda x: pdf_G_1(x,mu,P)\n",
    "    #plt.plot(xx,postr(xx),'--',label='posterior - parametric')\n",
    "    \n",
    "    plt.ylim(ymax=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.14:** Answer the following by moving the sliders and seeing what happens.\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Where is the posterior when $R = B$ ? (try moving around $y$)\n",
    " * Does the posterior scale (width) depend on $y$?\n",
    " * Forgetting about its location and scale, what is the shape of the posterior? Does this depend on $R$ or $y$? Can you see why?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.15*:** Implement a \"uniform\" (or \"flat\" or \"box\") distribution pdf and call it `pdf_U_1(x,mu,P)`. These <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments\">formulae</a> for its mean/variance will be useful. In the above animations, replace `pdf_G_1` with your new `pdf_U_1` (both for the prior and likelihood). Assure that everything is working correctly. \n",
    " - Why (in the figure) are the walls of the pdf (ever so slightly) inclined?\n",
    " - What happens when you move the prior and likelihood very far apart? Is the fault of the implementation, or the fundamental assumptions (uniform distribution)?\n",
    " - Re-do Exc 2.14, now with `pdf_U_1`.\n",
    " - Now test a Gaussian prior with a uniform likelihood.\n",
    " - Restore `pdf_G_1` (both the prior and likelihood) in the animation (for later use).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('pdf_U_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes a preposterious notion in high dimensions.\n",
    "\n",
    "**Exc 2.16:**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $m$ dimensions? (Imagine an $m$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quanitites, on each grid point, on a discretized model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($m$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=10$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Dimensionality a')\n",
    "#show_answer('Dimensionality b')\n",
    "#show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case.\n",
    "With a prior $p(x) = N(x|b,B)$ and a likelihood $p(y|x) = N(y|x,R)$, the posterior will be given by\n",
    "\\begin{align}\n",
    "p(x|y)\n",
    "&= N(x|\\mu,P) \\qquad \\qquad (4) \n",
    "\\, ,\n",
    "\\end{align}\n",
    "where, in the univarite (1-dimensional) case:\n",
    "\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\qquad \\qquad (5) \\\\\\\n",
    "  \\mu &= P(b/B + y/R) \\, .  \\qquad \\qquad (6) \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\mu$.\n",
    "*Hint: you need eqns (1) and (3).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Do some light algebra to show that eqns (5) and (6) can be rewritten as\n",
    "\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\qquad \\qquad (8) \\\\\\\n",
    "  \\mu &= b + K (y-b)  \\qquad \\quad (9) \\, ,\n",
    "\\end{align}\n",
    "where $K = B/(B+R)$, which is called the \"Kalman gain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.22*:** Consider the formula for $K$ and its role in the previous couple of equations... Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KG 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayes_rule_Gaussian(b,B,y,R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return mu,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Then, go back to the animation above, and uncomment the block that makes use of `Bayes_rule_Gaussian()`. Make sure its curve coincides with that which uses pointwise multiplication (i.e. `Bayes_rule()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.28:** More questions related to the above animation:\n",
    " * Does the width (i.e. scale) for the posterior depend on the location $y$ of the likelihood?\n",
    " * Is the width (i.e. scale) for the posterior always smaller that that of prior and likelihood? What does this mean information-wise?\n",
    "   * Do you think this is always the case, also for non-Gaussian distributions?\n",
    " * What if you're pretty sure about something, and you get a wildly different indication (observation). What is your posterior certainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.30*:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3 - Univariate Kalman filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
