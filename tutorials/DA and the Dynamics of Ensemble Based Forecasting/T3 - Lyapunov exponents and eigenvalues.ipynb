{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *\n",
    "from IPython.display import display\n",
    "from scipy.integrate import odeint\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyapunov exponents and eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Lypunov exponent** can be understood loosely as a kind of generalized eigenvalue for time-depenent linear transformations, or for the linearization of a nonlinear evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do eigenvalues tell us about a matrix and why might the above results seem intuitive? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the equation for the <em>evolution</em> of the pertubations <span style='font-size:1.25em'>$\\boldsymbol{\\delta}^i_k$</span></a>.  We can write,\n",
    "<h3>\n",
    "$$\\begin{align}\n",
    "& \\boldsymbol{\\delta}_k^i = \\mathbf{x}_k^c - \\mathbf{x}_k^i \\\\\n",
    "\\Rightarrow & \\dot{\\boldsymbol{\\delta}}_k^i = f(\\mathbf{x}_k^c) - f(\\mathbf{x}_k^i).\n",
    "\\end{align}$$\n",
    "</h3>\n",
    "But for small perturbations, we can reasonably make an approximation with a Taylor expansion,\n",
    "<h3>\n",
    "$$\\begin{align}\n",
    " f(\\mathbf{x}_k^c) - f(\\mathbf{x}_k^i) \\approx \\nabla f\\rvert_{\\mathbf{x}c}  \\boldsymbol{\\delta}^i_k, & &   \n",
    "\\end{align}$$\n",
    "</h3> \n",
    "where the term, \n",
    "<h2>\n",
    "$$f\\rvert_{\\mathbf{x}c}$$\n",
    "</h2>\n",
    "is the gradient with respect to the state variables, i.e., the **[Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)**, evaluated at the control trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for small perturbations, the evolution is well approximated by the linear Jacobian equations, and we can think of these linear equations having some kind of generalized eigenvalues, describing the invariant (exponential) growth and decay rates for the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The power method\n",
    "\n",
    "The method of breeding errors above is conceptually very similar to the classical [power method](https://en.wikipedia.org/wiki/Power_iteration) for finding the leading eigenvalue of a diagonalizable matrix:\n",
    "\n",
    "* Suppose <span style='font-size:1.25em'>$\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$</span> is a diagonalizable matrix, with eigenvalues,\n",
    "<h3>\n",
    "$$\n",
    "\\rvert \\mu_1 \\rvert > \\rvert\\mu_2\\rvert \\geq \\cdots \\geq \\rvert\\mu_n\\rvert,\n",
    "$$\n",
    "</h3>\n",
    "i.e., <span style='font-size:1.25em'>$\\mathbf{M}$</span> has a single eigenvalue of magnitude greather than all its others.   \n",
    "\n",
    "* Let <span style='font-size:1.25em'>$\\mathbf{v}_0 \\in \\mathbb{R}^n$</span> be a randomly selected vector, with respect to the Gaussian distribution on <span style='font-size:1.25em'>$\\mathbb{R}^n$</span>. \n",
    "\n",
    "* We define the algorithm,\n",
    "<h3>\n",
    "$$\\begin{align}\n",
    "\\mathbf{v}_{k+1} \\triangleq \\frac{\\mathbf{M} \\mathbf{v}_k}{ \\left\\rvert \\mathbf{M} \\mathbf{v}_k\\right\\rvert} & &\n",
    "\\widehat{\\mu}_{k+1} \\triangleq \\mathbf{v}_{k+1}^{\\rm T} \\mathbf{M} \\mathbf{v}_{k+1} \n",
    "\\end{align}$$\n",
    "</h3>\n",
    "as the power method.  \n",
    "\n",
    "It is easy to verify that with probability one, the sequence <span style='font-size:1.25em'>$\\widehat{\\mu}_k$</span> converges to the dominant eigenvalue, <span style='font-size:1.25em'>$\\mu_1$</span>, and <span style='font-size:1.25em'>$\\mathbf{v}_k$</span> converges to an eigenvector for the dominant eigenvalue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 4.20**: Fill in the code below to write an algorithm for the power method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(M, v, number_iterations):\n",
    "    \"\"\"takes a diagonalizable matrix M and returns approximations for the leading eigenvector/eigenvalue\"\"\"\n",
    "    \n",
    "    for i in range(number_iterations):\n",
    "        ### fill in missing lines here\n",
    "\n",
    "    return v, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example solution\n",
    "\n",
    "# show_answer('power_method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 4.22**: Test your solution to **Exc 4.20**.  Use the code and slider below to study the rate of convergence.  In this case, the matrix will have eigenvalues \n",
    "<h3>$$\\begin{align}\n",
    "\\left\\{r^i : \\hspace{2mm} i =0, 1, 2, \\hspace{2mm} \\text{and} \\hspace{2mm} r\\in(1,2]\\right\\}\n",
    "\\end{align}$$</h3>\n",
    "The parameter <span style='font-size:1.25em'>$k$</span> defines how many iterations of the power method are computed.  How does the value <span style='font-size:1.25em'>$r$</span> affect the number of iterations necessary to reach convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_power_convergence_rate(k=1, r=1.5):    \n",
    "    \n",
    "    # We define a well conditioned matrix M, depending on the ratio of the eigenvalues\n",
    "    M = array([r ** i for i in range(3)])\n",
    "    M = np.diag(M)\n",
    "    e_3 = array([0, 0, 1])\n",
    "\n",
    "    # define a random initial condition\n",
    "    np.random.seed(0)\n",
    "    v = randn(3)\n",
    "    v = v / sqrt(v.T @ v)\n",
    "    \n",
    "    # and storage for the series of approximations\n",
    "    v_hist = zeros(k+1)\n",
    "    v_hist[0] = e_3.T @ v \n",
    "    \n",
    "    mu_hist = zeros(k+1)\n",
    "    mu_hist[0] = v.T @ M @ v\n",
    "    \n",
    "    # for the number of iterations k, return the power method approximation\n",
    "    for it in range(1,k+1):\n",
    "        np.random.seed(0)\n",
    "        v, mu = power_method(M, v, it)\n",
    "        v_hist[it] = np.arccos(e_3.T @ v)\n",
    "        mu_hist[it] = mu\n",
    "     \n",
    "    # PLOTTING\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    ax1.plot(range(0,k+1), v_hist)\n",
    "    ax2.plot(range(0,k+1), mu_hist)\n",
    "    ax1.set_ybound([0,1.05])\n",
    "    ax2.set_ybound([.9,4])\n",
    "    \n",
    "    t_scl = np.floor_divide(k+1, 10)\n",
    "    \n",
    "    ax1.set_xticks(range(0, k+1, t_scl + 1))\n",
    "    ax2.set_xticks(range(0, k+1, t_scl + 1))\n",
    "    \n",
    "    ax1.text(0, 1.07, r'Angle between $\\mathbf{v}_k$ and eigenvector', size=20)\n",
    "    ax2.text(0, 4.05, r'Value of $\\mu_k$', size=20)\n",
    "    ax1.tick_params(\n",
    "        labelsize=20)\n",
    "    ax2.tick_params(\n",
    "        labelsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "w = interactive(animate_power_convergence_rate, k=(1,15), r=(1.05,2, .05))\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.24.a </b>: Suppose the power method is performed on a generic diagonalizable matrix <span style='font-size:1.25em'>$\\mathbf{M}\\in\\mathbb{R}^{n\\times n}$</span>, with eigenvalues\n",
    "<h3>$$\\begin{align}\n",
    "\\rvert \\mu_1 \\rvert > \\rvert\\mu_2 \\rvert\\geq \\cdots \\geq \\rvert\\mu_n \\rvert,\n",
    "\\end{align}$$</h3>\n",
    "with a randomly selected initial vector <span style='font-size:1.25em'>$\\mathbf{v}_0$</span>, with respect to the Gaussian distribution on <span style='font-size:1.25em'>$\\mathbb{R}^n$</span>.\n",
    "\n",
    "Can you conjecture what is the order of convegence for the sequences <span style='font-size:1.25em'>$\\mathbf{v}_k$</span> and <span style='font-size:1.25em'>$\\widehat{\\mu}_k$</span>? \n",
    "\n",
    "**Hint**: the rate depends on the eigenvalues.\n",
    "\n",
    "**Exc 4.42.b***: Prove the rate of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# show_answer('power_method_convergence_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.28* </b>: We have brushed over why the algorithm described above converges with *probability one*, can you prove why this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# show_answer('probability_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.30.a </b>: Let <span style='font-size:1.25em'>$\\widehat{\\mu}_k$</span> be defined as in **Exc 4.24**.  Suppose we define a sequence of values,\n",
    "<h3>$$\\begin{align}\n",
    "\\widehat{\\lambda}_T = \\frac{1}{T} \\sum_{k=1}^T\\log\\left(\\rvert \\widehat{\\mu}_k\\right \\rvert).\n",
    "\\end{align}$$</h3>\n",
    "Answer the following:\n",
    "<ol>\n",
    "  <li> Can you conjecture what <span style='font-size:1.25em'>$\\widehat{\\lambda}_T$</span> converges to as <span style='font-size:1.25em'>$T \\rightarrow \\infty$</span>?  \n",
    "  \n",
    "  **Hint**: Use the fact that <span style='font-size:1.25em'>$\\widehat{\\mu}_k \\rightarrow \\mu_1$</span> as <span style='font-size:1.25em'>$k \\rightarrow \\infty$</span></li>\n",
    "  \n",
    "  <li> Suppose we define the Lyapunov exponents as the log-average growth rates of the matrix <span style='font-size:1.25em'>$\\mathbf{M}$</span>.  What can you guess about the relationship between the eigenvalues and the Lyapunov exponents of the matrix <span style='font-size:1.25em'>$\\mathbf{M}$</span>?</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.30.b*</b>: Prove that the limit\n",
    "<h3>$$\\begin{align}\n",
    "\\lim_{T \\rightarrow \\infty} \\widehat{\\lambda}_T\n",
    "\\end{align}$$</h3>\n",
    "exists, and what quantity it converges to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers\n",
    "\n",
    "# show_answer('lyapunov_exp_power_method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The QR algorithm\n",
    "\n",
    "The power method is an intuitive method for finding the dominant eigenvalue for a special class of matrices.  However, we generally want to find directions that may also be growing, though more slowly than the dominant direction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, if we are tracking a control trajectory with data assimilation and we corrected the forecast errors only in the direction of dominant error growth, we may still lose track of the control trajectory, only it would be more slowly than the dominant rate of growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a simple generalization of the power method for finding higher dimensional subspaces.  We may consider *separating* perturbations into directions that grow at different rates.  One easy way to perform this is to construct a *moving frame* in the span of the perturbations.  If there is only one perturbation, then the power method constructs precisely a 1-dimensional moving frame, with a vector that is always of norm 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are two perturbations we can construct a moving frame in the span of the perturbations with a [Gram-Schmidt](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) step.  A visualization of the Gram-Schmidt process for three vectors is picuted in the visualization below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:900px'>\n",
    "<img src=\"./resources/Gram-Schmidt_orthonormalization_process.gif\">\n",
    "</div>\n",
    "\n",
    "**By Lucas V. Barbosa [Public domain], <a href=\"https://commons.wikimedia.org/wiki/File:Gram-Schmidt_orthonormalization_process.gif\">from Wikimedia Commons</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, suppose we have two initial, orthogonal vectors\n",
    "<h3>$$\n",
    "\\mathbf{x}_0^1, \\mathbf{x}_0^2\n",
    "$$</h3>\n",
    "which we will propagate forward.  We define for each $j=1,2$,\n",
    "<h3>$$\n",
    "\\widehat{\\mathbf{x}}^j_1 \\triangleq \\mathbf{M} \\mathbf{x}^j_0.\n",
    "$$</h3>\n",
    "The first vector will follow the usual power method, i.e.,\n",
    "<h3>$$\n",
    "\\mathbf{x}^1_1 \\triangleq \\frac{\\widehat{\\mathbf{x}}_1^1}{\\left\\rvert \\widehat{\\mathbf{x}}_1^1\\right\\rvert},\n",
    "$$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want to separate the second vector <span style='font-size:1.25em'>$\\widehat{\\mathbf{x}}_1^2$</span> so the new perturbations don't align.  We thus remove the components in the direction of <span style='font-size:1.25em'>$\\mathbf{x}_1^1$</span>, before we normalize <span style='font-size:1.25em'>$\\widehat{\\mathbf{x}}_1^2$</span>.  \n",
    "<h3>$$\\begin{align}\n",
    "\\mathbf{y}^2_1 &\\triangleq \\widehat{\\mathbf{x}}_1^2- \\langle \\mathbf{x}_1^1,  \\widehat{\\mathbf{x}}^2_1\\rangle \\mathbf{x}_1^1 \\\\\n",
    "\\mathbf{x}^2_1 & \\triangleq \\frac{\\mathbf{y}_1^2}{\\left\\rvert \\mathbf{y}_1^2 \\right\\rvert}\n",
    "\\end{align}$$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see by definition that <span style='font-size:1.25em'>$\\mathbf{x}_1^1, \\mathbf{x}_1^2$</span> are orthogonal, but we can also show an important dynamical property with this transformation.  Define the following coefficients,\n",
    "<h3>$$\n",
    "\\begin{align}\n",
    "U^{11}_1 &=\\left\\rvert \\widehat{\\mathbf{x}}_1^1\\right\\rvert \\\\\n",
    "U^{22}_1 &=\\left\\rvert \\mathbf{y}_1^2 \\right\\rvert \\\\\n",
    "U^{12}_1 &= \\langle \\mathbf{x}^1_1, \\widehat{\\mathbf{x}}_1^2\\rangle \n",
    "\\end{align}\n",
    "$$<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 4.32**: Can you write the recursion for the vectors <span style='font-size:1.25em'>$\\mathbf{x}_0^1, \\mathbf{x}_0^2$</span> transformed into <span style='font-size:1.25em'>$\\mathbf{x}_1^1,\\mathbf{x}_1^2$</span> with the coefficients <span style='font-size:1.25em'>$U^{ij}_1$</span> defined above in matrix form?  Can you write the recursion for an arbitrary number of steps $k\\in\\{1,2,\\cdots\\}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# show_answer('gram-schmidt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above procedure defines the *naive* QR algorithm --- one should note that there are more computationally efficient versions of this algorithm utilized in standard linear algebra software libraries.  However, this simple intuition forms the basis for many powerful theoretical results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QR algorithm (in its refined version) is the standard method for computing the <b>[Schur decomposition](https://en.wikipedia.org/wiki/Schur_decomposition)</b> for a matrix, which is used for many purposes as it is a numerically stable alternative to the <b>[Jordan Cannonical Form](https://en.wikipedia.org/wiki/Jordan_normal_form)</b>, pictued below:\n",
    "\n",
    "<div style='width:900px'>\n",
    "<img src=\"./resources/Jordan_blocks.svg\">\n",
    "</div>\n",
    "\n",
    "**By Jakob.scholbach [<a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a> or <a href=\"http://www.gnu.org/copyleft/fdl.html\">GFDL</a>], <a href=\"https://commons.wikimedia.org/wiki/File:Jordan_blocks.svg\">from Wikimedia Commons</a>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jordan Canonical form is highly appealing as it is the diagonal or \"almost-diagonal\" form of a matrix.  However, this is highly unstable to compute in most applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Schur decomposition relaxes this further, from \"almost-diagonal\" to upper triangular, another useful form for a matrix.  In particular, the Schur decomposition is one approach to find **all eigenvalues** for a matrix, separated into a **chain of descending growth and decay rates**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.34</b>: Suppose a matrix <span style='font-size:1.25em'>$\\mathbf{M}$</span> has a Schur decomposition, given as,\n",
    "<h3> $$ \\begin{align}\n",
    "\\mathbf{M} = \\mathbf{Q} \\mathbf{U} \\mathbf{Q}^{\\rm T},\n",
    "\\end{align}$$ </h3>\n",
    "where <span style='font-size:1.25em'>$\\mathbf{U}$</span> is strictly upper triangular, and <span style='font-size:1.25em'>$\\mathbf{Q}$</span> is orthogonal such that <span style='font-size:1.25em'>$\\mathbf{Q}^{\\rm T} = \\mathbf{Q}^{-1}$</span>.  Can you prove that the eigenvalues of <span style='font-size:1.25em'>$\\mathbf{M}$</span> are the diagonal elements of <span style='font-size:1.25em'>$\\mathbf{U}$?</span> \n",
    "\n",
    "If <span style='font-size:1.25em'>$\\mathbf{Q}^j$</span> is the $j$-th column of <span style='font-size:1.25em'>$\\mathbf{Q}^j$</span>, what does the product\n",
    "<h3>$$\\begin{align}\n",
    "\\left(\\mathbf{Q}^j\\right)^{\\rm T} \\mathbf{M} \\mathbf{Q}^j\n",
    "\\end{align}$$</h3>\n",
    "equal in terms of the earlier quantities? <b>Hint</b>: how does this relate to the power method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# show_answer('schur_decomposition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.36</b>: Can you conjecture what the Schur decomposition will take in the case that the matrix <span style='font-size:1.25em'>$\\mathbf{M}$</span> has complex eigenvalues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# show_answer('real_schur')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Lyapunov vectors and ensemble based covariances](T4 - Lyapunov vectors and ensemble based covariances.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
