{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *\n",
    "from IPython.display import display\n",
    "from scipy.integrate import odeint\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyapunov vectors and ensemble based covariances\n",
    "\n",
    "We return now to the discussion of perturbations in nonlinear models.  Recall the equations for the evolution of perturbations.  We may define *linear* dynamics, generated by the Jacobian equation,\n",
    "<h2>$$\\begin{align}\n",
    "\\frac{{\\rm d} \\mathbf{x}}{{\\rm d} t} = \\nabla f_{\\rvert_{\\mathbf{x}c}},\n",
    "\\end{align}$$</h2>\n",
    "computed along some control trajectory <span style='font-size:1.25em'> $\\mathbf{x}^c(t)$</span>.  This system of equations is known as the <a href=\"http://glossary.ametsoc.org/wiki/Tangent_linear_model\" target=\"blank\"><b>tangent-linear model</b></a>.  The tangent space can be understood as the <b>space of perturbations</b> at a point (or along a trajectory).  It can be used to define a vector field on a \"space\" describing the time evolution of trajectories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:900px'>\n",
    "<img src=\"./resources/Tangentialvektor.svg\">\n",
    "</div>\n",
    "\n",
    "<b>By derivative work: McSush (talk)Tangentialvektor.png: TNThe original uploader was TN at German Wikipedia (Tangentialvektor.png) [Public domain], <a href=\"https://commons.wikimedia.org/wiki/File:Tangentialvektor.svg\">via Wikimedia Commons</a></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen with the [breeding of errors](#breeding), we can compute the log-average growth rate of the dominant growing mode of the tangent-linear model to approximate the leading Lyapunov exponent.  In a linear system, with fixed matrix <span style='font-size:1.25em'> $\\mathbf{M}$</span> this corresponds exactly to taking the log-average growth rate in the power method.  Recall the generalization of the power method to the QR algorithm.  We might hope to find **all of the log-average growth rates** in the tangent-linear model by computing the log averages of the diagonal elements in the QR factors <span style='font-size:1.25em'> $\\mathbf{U}_k$</span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Gram-Schmidt factors form a basis for the tangent linear model where:\n",
    "<ol>\n",
    "    <li> the leading vector aligns with the dominant growth mode;</li>\n",
    "    <li> the subsequent vectors separate out the lower order growth rates.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the tangent-linear model is computed discretely in time, where <span style=font-size:1.25em> $\\textbf{M}_k$</span> takes the perturbations at time <span style=font-size:1.25em>$t_{k-1}$</span> to time <span style=font-size:1.25em>$t_k$</span>.  Suppose that in matrix form, <span style=font-size:1.25em>$\\mathbf{M}_k$</span>, produces the following QR factorization,\n",
    "<h3>$$\\begin{align}\n",
    "\\mathbf{M}_k \\mathbf{E}_{k-1} &= \\mathbf{E}_k \\mathbf{U}_k\n",
    "\\end{align}$$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 4.36**: Suggest a constructive definition for:\n",
    "<ol>\n",
    "    <li> the Lyapunov exponents of the nonlinear model</li>\n",
    "    <li> the \"Lyapunov vectors\" </li>\n",
    "</ol>\n",
    "<b>Note:</b> We will not stress yet what kind of Lyapunov vector we are constructing.  It turns out that Lyapunov exponents are generally <b>globally defined</b>, but there are several types of Lyapunov vectors that are locally defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer \n",
    "\n",
    "# show_answer('lyapunov_vs_es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of vector this construction leads to is the **\"backward\" Lyapunov vectors**.  They are denoted \"backward\" because they contain information from the in the past, leading to the current time, i.e., the \"errors of the day\" described by Toth and Kalnay.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we remark that the forcing singular vectors mentioned above can be shown to converge to the \"forward\" Lyapunov vectors.  These are denoted \"forward\" because they contain information about the future evolution of pertubations, pulled back to the current time.  A third type of Lyapunov vector is also often studied, which are called \"covariant\".  Similar to how eigenvectors always are mapped to a scaled copy of themselves, covariant vectors share an analogous property with respect to time-varying dynamics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full discussion of the significance of forward, backward, and covariant vectors goes well beyond the scope of this tutorial.  For a comprehensive discussion of Lyapunov theory, aimed at practicioners, it is recommended to read the work of [Legras & Vautard](http://www.lmd.ens.fr/legras/publis/liapunov.pdf) and [Kutpsov & Parlitz](https://arxiv.org/abs/1105.5228)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble based covariances\n",
    "\n",
    "When we restrict the forecasting problem to the situation where we assume:\n",
    "<ul>\n",
    "    <li> we can perfectly model and compute the purely deterministic dynamics; and</li>\n",
    "    <li> prediction error originates soley from the uncertainty in initial conditions,</li>\n",
    "</ul> \n",
    "it is realistic to think of the data assimilation problem as tracking the evolution of perturbations along a control trajectory.  This control trajectory is the \"true\" state of the dynamical system we are studying, which we receive observations of, and try to predict the behavior of at future times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we suppose we have a prior distribution for the state of the dynamical system.  Suppose we sample for an ensemble of \"nearby\" initial conditions.  If the ensemble remains sufficiently close to the control trajectory, and we can use the [approximation for the evolution of perturbations](#perturbation_equation) to accurately model the forecast errors, then the ensemble spread will be characterized by the backward Lyapunov vectors and their growth and decay rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the conceptual image below.  Suppose the initial prior covariance is given by the unit disk, centered on the \"true\" state of a dynamical system.  If the evolution of uncertainty can be well approximated by the tangent linear model along the \"truth\", we will expect the covariance to stretch and deform into an ellipse according to the directions of **growth** and **decay**.\n",
    "<div style='width:800px'>\n",
    "<img src=\"./resources/LyapunovDiagram.svg\">\n",
    "</div>\n",
    "<b>By Mrocklin (original creation) [<a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA 3.0</a> or <a href=\"http://www.gnu.org/copyleft/fdl.html\">GFDL</a>], <a href=\"https://commons.wikimedia.org/wiki/File:LyapunovDiagram.svg\">via Wikimedia Commons</a></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will implement a simple, \"square-root\" ensemble Kalman filter in the Lorenz-63 model.  The ensemble Kalman filter will be given **noisy observations** of a control trajectory.   Along the control trajectory, we will compute the **QR factorization** of the tangent-linear model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the **projection coefficient** of the covariance matrix <span style='font-size:1.25em'> $\\mathbf{P}_k$</span> into the $j$-th QR factor to be the quantity,\n",
    "<h3>$$\\begin{align}\n",
    "\\left(\\mathbf{Q}_k^j\\right)^{\\rm T} \\mathbf{P}_k \\mathbf{Q}^j\n",
    "\\end{align}$$</h3>\n",
    "We will compute the projection coefficients of the ensemble based covariance into each of the QR factors, as defined above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.38</b>: Can you conjecture how the projection coefficients will vary in the index $j$? <br>\n",
    "**Hint**: consider **Exc 4.34**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.40</b>: Test your conjecture from <b>Exc 4.40</b>.  Use the code below to investigate the relationship between the ensemble based covariance and its projeciton into each of the QR factors.  We plot the average projection coefficient for the EnKF covariance into each of the QR factors over the number of analyses.  Similarly, we plot the EnKF mean square error, to vefify that the ensemble mean lies within the variance of the error in these directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we may consider the QR factorizations to give approximately the \"true\" Lyapunov vectors when the log-average growth rate approaches the Lyapunov exponents for the system.  The Lyapunov exponents are approximately given by,\n",
    "<h3>$$\\begin{align}\n",
    "\\lambda_1 &\\approx 0.905 \\\\\n",
    "\\lambda_2 & = 0 \\\\\n",
    "\\lambda_3 & \\approx -14.571.\n",
    "\\end{align}$$<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGMA = 10.0\n",
    "BETA  = 8/3\n",
    "RHO   = 28.0\n",
    "\n",
    "sigma = SIGMA\n",
    "beta = BETA\n",
    "rho = RHO\n",
    "\n",
    "def dxdt(xyz, t0, sigma=SIGMA, beta=BETA, rho=RHO):\n",
    "    \"\"\"Compute the time-derivative of the Lorenz-63 system.\"\"\"\n",
    "    x, y, z = xyz\n",
    "    return array([\n",
    "        sigma * (y - x),\n",
    "        x * (rho - z) - y,\n",
    "        x * y - beta * z\n",
    "    ])\n",
    "\n",
    "def l63_jac(x):\n",
    "    jac = np.array([\n",
    "        [-sigma,  sigma,     0 ],\n",
    "        [rho - x[2], -1,  -x[0]],\n",
    "        [x[1],      x[0], -beta]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return jac\n",
    "\n",
    "def l63_step_TLM(x, Y, h):\n",
    "    \n",
    "    h_mid = h/2\n",
    "\n",
    "    # calculate the evolution of x to the midpoint\n",
    "    x_mid = l63_rk4_step(x, h_mid)\n",
    "\n",
    "    # calculate x to the next time step\n",
    "    x_next = l63_rk4_step(x_mid, h_mid)\n",
    "\n",
    "    k_y_1 = l63_jac(x).dot(Y)\n",
    "    k_y_2 = l63_jac(x_mid).dot(Y + k_y_1 * (h / 2.0))\n",
    "    k_y_3 = l63_jac(x_mid).dot(Y + k_y_2 * (h / 2.0))\n",
    "    k_y_4 = l63_jac(x_next).dot(Y + k_y_3 * h)\n",
    "\n",
    "    Y_next = Y + (h / 6.0) * (k_y_1 + 2 * k_y_2 + 2 * k_y_3 + k_y_4)\n",
    "\n",
    "    return [x_next, Y_next]\n",
    "\n",
    "def l63_rk4_step(xyz, h):\n",
    "    \"\"\" calculate the evolution of Lorenz-63 one step forward via RK-4\"\"\"\n",
    "    \n",
    "    k_xyz_1 = dxdt(xyz, h, sigma=SIGMA, beta=BETA, rho=RHO)\n",
    "    k_xyz_2 = dxdt(xyz + k_xyz_1 * (h / 2.0), h, sigma=SIGMA, beta=BETA, rho=RHO)\n",
    "    k_xyz_3 = dxdt(xyz + k_xyz_2 * (h / 2.0), h, sigma=SIGMA, beta=BETA, rho=RHO)\n",
    "    k_xyz_4 = dxdt(xyz + k_xyz_3 * h, h, sigma=SIGMA, beta=BETA, rho=RHO)\n",
    "\n",
    "    xyz_step = xyz + (h / 6.0) * (k_xyz_1 + 2 * k_xyz_2 + 2 * k_xyz_3 + k_xyz_4)\n",
    "\n",
    "    return xyz_step\n",
    "\n",
    "def animate_enkf_covariance(nanl=0):    \n",
    "    \n",
    "    # Initial conditions: perturbations around some control state\n",
    "    tanl=0.005\n",
    "    h = 0.001\n",
    "    tl_steps = int(tanl / h)\n",
    "    N = 4\n",
    "    obs_un = 0.25\n",
    "    obs_dim = 3\n",
    "    R = np.eye(obs_dim) * obs_un\n",
    "    H = np.eye(3, M=obs_dim).T\n",
    "    proj_traj = np.zeros([nanl, 3])\n",
    "    err = np.zeros([nanl])\n",
    "    \n",
    "    seed(1)\n",
    "    x_0 = array([-6.1, 1.2, 32.5])               # define the control\n",
    "    \n",
    "    # define the perturbations, randomly generated but of fixed norm epsilon\n",
    "    A_f = randn([3, N])\n",
    "    a_m = np.mean(A_f, axis=1)\n",
    "    A_f = A_f.T - a_m\n",
    "    del a_m\n",
    "    A_f = (x_0 + A_f).T\n",
    "                  \n",
    "    lam = np.zeros(3)\n",
    "    Q = np.eye(3)\n",
    "        \n",
    "    # for each analysis cycle\n",
    "    for kk in range(nanl):\n",
    "        for j in range(tl_steps):\n",
    "            x_0, Q = l63_step_TLM(x_0, Q, h)\n",
    "            for l in range(2):\n",
    "                for j in range(N):\n",
    "                    A_f[:, j] = l63_rk4_step(A_f[:, j], h / 2)\n",
    "            \n",
    "        # perform QR step and find the local Lyapunov exponents\n",
    "        Q, U = np.linalg.qr(Q)\n",
    "        lam += np.log(np.abs(np.diag(U))) / tanl\n",
    "        \n",
    "        # define an observation\n",
    "        y_0 = H @ x_0 + np.random.multivariate_normal(np.zeros([obs_dim]), R)\n",
    "        \n",
    "        # forecast mean, and rmse\n",
    "        x_f = np.mean(A_f, axis=1)\n",
    "        \n",
    "        A_f = (A_f.T - x_f).T\n",
    "        \n",
    "        # forecast covaraince\n",
    "        P_f = (N-1) ** (-1) * A_f @ A_f.T \n",
    "        \n",
    "        # form the Kalman gain    \n",
    "        K = P_f @ H.T @ np.linalg.inv(H @ P_f @ H.T + R) \n",
    "        \n",
    "        # analysis mean\n",
    "        x_a = x_f + K @ (y_0 - H @ x_f)\n",
    "        err[kk] = np.mean((x_a - x_0)**2)\n",
    "        \n",
    "        # analyze the ensemble\n",
    "        T = np.eye(N) - (N-1)**(-1) * (H @ A_f).T @ np.linalg.inv(H @ P_f @ H.T + R) @ (H @ A_f)\n",
    "        U, S, V_h = np.linalg.svd(T)\n",
    "        T = U @ np.diag(np.sqrt(S)) @ U.T\n",
    "        A_f = A_f @ T\n",
    "        \n",
    "        P_a = (N-1)**(-1) * A_f @ A_f.T\n",
    "        # find the forecast projection coefficients\n",
    "        for i in range(3):\n",
    "            proj_traj[kk, i] = Q[:, i].T @ P_a @ Q[:, i]\n",
    "            \n",
    "        A_f = (x_a + A_f.T).T\n",
    "        \n",
    "        \n",
    "    # PLOTTING\n",
    "    avg_proj = np.zeros([nanl, 3])\n",
    "    \n",
    "    # we plot the average projection coefficient into each blv\n",
    "    for i in range(1,nanl):\n",
    "        avg_proj[i, :] = np.mean(proj_traj[:i, :], axis=0)\n",
    "    \n",
    "    lam = lam / nanl\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    for i in range(3):\n",
    "        ax.plot(range(nanl), avg_proj[:, i], label='BLV projection ' + str(i + 1) + ', log-avg growth rate ' + \n",
    "                str(np.round(lam[i],decimals=3)).zfill(3))\n",
    "                \n",
    "    ax.axhline(y=np.mean(err), color='k', label='Mean square error')\n",
    "    plt.legend(fontsize=20)\n",
    "    ax.set_xbound([1, nanl])\n",
    "    ax.set_yscale('log')\n",
    "    ax.tick_params(labelsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "w = interactive(animate_enkf_covariance,nanl=(10,20010,2000))\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exc 4.42</b>: Answer the following questions.\n",
    "<ol>\n",
    "   <li>How do the projection coefficients relate to the log-average growth rates in each direction? </li> \n",
    "    <li>What is significant about the <b>effective rank</b> of the covariance?</li>\n",
    "   <li>Can you conjecture what this means about the necessary number of ensemble members to prevent filter divergence?\n",
    "       <br> <b>Hint</b>: the ensemble should capture the effective spread of the uncertainty.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
