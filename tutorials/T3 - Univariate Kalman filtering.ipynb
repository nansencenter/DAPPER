{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many mathematical methods are tagged with the label \"least squares\". They typically have one thing in common: some sum of squared terms is being minimized. Both linear regression and Kalman filtering (KF) are a form of \"least squares\".\n",
    "Do they yield the same estimate?\n",
    "\n",
    "Consider the straight line\n",
    "$$x_k = a k \\, ,    \\qquad \\qquad (1) $$\n",
    "where $k$ is an index, usually of time.\n",
    "Now suppose we have noisy observations of the line:\n",
    "\\begin{align*}\n",
    "y_k &= x_k + \\xi_k \\, . \\qquad \\qquad (2)\n",
    "\\end{align*}\n",
    "\n",
    "Linear regression minimizes\n",
    "$$J_K(\\alpha) = \\sum_{k=1}^K (y_k - \\alpha k)^2 \\, ,  \\qquad \\qquad (4)$$\n",
    "yielding the estimator\n",
    "$$\\alpha_K = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\qquad \\qquad (6)$$\n",
    "\n",
    "**Exc 3.2:** Derive (6) from (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg deriv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KF, whose **analysis step** is given by [eqns (8) and (9) of the previous tutorial](T2%20-%20Bayesian%20inference.ipynb#Exc-'Gaussian-Bayes':),  also constitutes an estimator. We could apply it to estimate the parameter $a$ directly, but we will instead configure it to estimate $x_k$ (and thereby $a$ as $\\alpha_k = x_k / k$).\n",
    "In order to do so, we need to define the **forecast step**, which consists in finding some some $F_k$ (which does not contain unknowns) so that $x_{k+1} = F_k x_k$.\n",
    "\n",
    "**Exc 3.4:** What is $F_k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg F_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The code below sets up an experiment based on eqns. (1) and (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 0.4  # Slope (xx[k] = a*k) paramterer. To be estimated.\n",
    "K = 10   # Length of experiment (final time index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = 0    # Dynam model noise strength\n",
    "R = 1    # Observation noise strength\n",
    "H = 1    # Observation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic truth (`xx`) and observations (`yy`). Again, the naming convention indicates that these variables hold time series of `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: Python indexing starts at 0.\n",
    "# Convention: no obs at k==0.\n",
    "xx = np.zeros(K+1) # states\n",
    "yy = np.zeros(K)   # obs\n",
    "\n",
    "for k in range(K):\n",
    "    xx[k+1] = a*(k+1)\n",
    "    yy[k]   = xx[k+1] + randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6:** Program the linear regresson estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg func')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implements $F_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F(k):\n",
    "    if k==0:\n",
    "        # indeterminate and inconsequential\n",
    "        return 1\n",
    "    else:\n",
    "        # Such that xx[k+1] = a*(k+1) = F(k)*xx[k]\n",
    "        return (k+1)/k\n",
    "# Note that xx[k]/k is the current running estimate of the slope 'a'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Implement a scalar version of the KF to estimate $x_k$ for $k=1,\\ldots, K$.\n",
    "The forecast step is specified by [eqns 2.3--2.4 of the DA intro](resources/DA_intro.pdf#page=9).\n",
    "The analysis step is given by eqns (5) and (6) from the [previous tutorial](T2 - Bayesian inference.ipynb#Exc-'Gaussian-Bayes':))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mua = np.zeros(K+1) # mean estimates (mu) -- analysis values (a)\n",
    "muf = np.zeros(K+1) # mean estimates (mu) -- forecast values (f)\n",
    "PPa = np.zeros(K+1) # covar estimates (P) -- analysis values (a)\n",
    "PPf = np.zeros(K+1) # covar estimates (P) -- forecast values (f)\n",
    "PPa[0] = np.inf     # Set initial uncertainty to infinity\n",
    "\n",
    "def KF(k):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "\n",
    "# Run estimations/computations\n",
    "for k in range(K): KF(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KF func')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem more complicated than linear regression. But this \"heavy machinery\" is more general/flexible, and will pay off later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the outcome of the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@interact(k=IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    plt.plot(arange(0,k+1),xx[:k+1]            ,'k' ,label='true state ($x$)')\n",
    "    plt.plot(arange(1,k+1),yy[:k]              ,'k*',label='noisy obs ($y$)')\n",
    "    \n",
    "    plt.plot(arange(k+1),arange(k+1)*lin_reg(k),'r' ,label='Linear regress.')\n",
    "\n",
    "    #plt.plot(arange(k+1),arange(k+1)*mua[k]/k ,'g' ,label='KF extrapolated')\n",
    "    #plt.plot(arange(k+1),mua[:k+1]                 ,label='KF (analyses only)')\n",
    "    pw_muf, pw_mua = piece_wise_DA_step_lines(muf,mua)\n",
    "    pw_kkf, pw_kka = piece_wise_DA_step_lines(arange(K+1))    \n",
    "    plt.plot(pw_kkf[:3*k],pw_muf[:3*k]         ,'c' ,label='KF forecasts')\n",
    "    plt.plot(pw_kka[:3*k],pw_mua[:3*k]         ,'b' ,label='KF analyses')\n",
    "\n",
    "    plt.xlim([0,K])\n",
    "    plt.ylim([-1,1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Visually: What is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('LinReg plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.12*:** Recall that\n",
    " * $x_k = a k = F_{k-1} x_{k-1}$,  with $F_{k-1} = \\frac{k}{k-1}$.\n",
    "\n",
    "The KF forecast step (here with $Q=0$) can be \"baked into\" the analysis step, forming a single couple of equations, which are recursive:\n",
    "\n",
    " * (2): $\\mu_k = P_k \\big(y_k/R + F_{k-1} \\mu_{k-1} / [F_{k-1}^2 P_{k-1}] \\big) $  \n",
    " * (3): $P_k = 1/\\big(1/R + 1/[F_{k-1}^2 P_{k-1}]\\big)$\n",
    "\n",
    "This excercise aims to show (on paper) that the KF estimate equals the linear regression estimate ($\\alpha_K$):\n",
    "$$\\alpha_K = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2}$$\n",
    "* (a). First show that $P_K = R\\frac{K^2}{\\sum_{k=1}^K k^2}$   (4).\n",
    "* (b). Then show that $\\mu_K = K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\alpha_K$   (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show_answer('KF = LinReg a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to show the equivalence is to recall that the KF optimizes, recursively, for increasing $K$,\n",
    "\n",
    "$$J(x_K) = (x_K - F \\mu_{K-1})^2/Q + (y_K-x_K)^2/R \\, ,$$\n",
    "\n",
    "where $\\mu_{K-1}$ is the argmin of $J(x_{K-1})$.\n",
    "\n",
    "Letting $Q \\rightarrow 0$ (and $P_0 \\rightarrow \\infty$)\n",
    "this becomes the same problem as for linear regression:\n",
    "\n",
    "$$J(\\alpha) = \\sum_{k=1}^K (y_k - \\alpha k)^2 \\, .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14 'Asymptotic P':\n",
    "Consider the scalar KF equations again. Suppose $Q=0$ and that the forward model is $x_{k+1} = F x_k$, with $F>1$. Also suppose that $R_k$ is constant. What does the sequence of $P_k$ converge to? Hint: start from eqn (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Asymptotic P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.16**: Have a look at the code from `show_answer('KF func')`. Comment out the \"weighted average\" form of the update, and try out the \"Kalman gain\" form. Why does it fail (mysterious hint: improper priors)? Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('KF KG fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18:** Now set $Q$ to 1 or more. What happens to the KF estimates? If you want to use the same set of observations, avoid re-executing the cell that simulated/generated them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20:** Now change $R$. The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A higher-order example\n",
    "Ok, so the KF produces reasonable results for straight lines (in so far as linear regression does!).\n",
    "What about more intricate time series?\n",
    "\n",
    "Note that the straight line (eqn 1 at the top) could result from discretizing the model\n",
    "\\begin{align*}\n",
    "\\frac{dx}{dt} &= \\alpha \\, , \\\\\n",
    "x_0 &= 0 \\, ,\n",
    "\\end{align*}\n",
    "using `dt = 1`.\n",
    "Now, instead, we're going to consider the model\n",
    "$$ \\frac{d^m x}{dt^m} = 0 \\, .$$\n",
    "\n",
    "This can be written as 1-st order vector (i.e. coupled system of) ODE:\n",
    "$$ \\frac{d x^i}{dt} = x^{i+1} \\, , \\quad \\frac{d x^m}{dt} = 0 \\, ,$$\n",
    "where $i = 1,\\ldots,m$ is the state vector element index.\n",
    "\n",
    "We'll add two terms to this model: (1) damping $\\beta x^i$, with $\\beta < 0$ and (2) noise $\\frac{d w^i}{dt}$. Thus,\n",
    "$$ \\frac{d x^i}{dt} = \\beta x^i + x^{i+1} + \\frac{d w^i}{dt} \\, ,$$\n",
    "where $w^i$ is the noise process, and $\\beta = \\log(0.9)$.\n",
    "\n",
    "Discretized, with a time step `dt=1`, this yields\n",
    "$$ x^i_{k+1} = 0.9 x^i_k + x^{i+1}_k + w^i_k\\, ,$$\n",
    "\n",
    "In summary, $\\mathbf{x}_{k+1} = \\mathbf{F} \\mathbf{x}_k$, with $\\mathbf{F}$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 4 # model order (and also ndim)\n",
    "F_matrix = 0.9*eye(m) + diag(ones(m-1),1)\n",
    "F_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a full-fledged Kalman filtering formulation of a problem on our hands. \n",
    "\n",
    "We shall not write the code for a multivariate Kalman filter. It is already in DAPPER under `[DAPPER-path]/da_methods.py` and is called `ExtKF()`. The following sets up an experiment with a synthetic truth realization of the model, along with noisy observations. For now, don't worry about the specifics. We'll get back to how to make setups later.\n",
    "\n",
    "We'll only observe the first (0th) component, so that the time series can also be analysed by simple signal processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Forecast dynamics\n",
    "f = linear_model_setup(F_matrix)\n",
    "f['noise'] = 0.0001*arange(m)\n",
    "\n",
    "# Initial conditions\n",
    "X0 = GaussRV(m=m,C=0.02*arange(m))\n",
    "\n",
    "# observe 0th component only\n",
    "h = partial_direct_obs_setup(m,[0])\n",
    "h['noise'] = 1000\n",
    "\n",
    "# Time settings\n",
    "t = Chronology(dt=1,dtObs=5,K=250)\n",
    "\n",
    "# Wrap-up\n",
    "setup = TwinSetup(f,h,t,X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates (simulates) a synthetic truth (xx) and observations (yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx,yy = simulate(setup,desc=\"Simulate\")\n",
    "#plt.plot(xx[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run assimilation methods on the data. The `ExtRTS` method refers to a smoother. It is based on the filter, but also goes back in time, \"smoothing\" out the jumps by assimilating future (relatively speaking) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_KF = ExtKF (store_u=1).assimilate(setup,xx,yy)\n",
    "stats_KS = ExtRTS(store_u=1).assimilate(setup,xx,yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods perform \"time series analysis\" of the observations, and are mainly derived from signal processing theory.\n",
    "Considering that derivatives can be approximated by differentials, it is plausible that the above model could also be written as an AR(m) process. Thus these methods should perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.signal as sp_sp\n",
    "normalize = lambda x: x / x.sum()\n",
    "truncate  = lambda x,n: np.hstack([x[:n],zeros(len(x)-n)])\n",
    "TS = {}\n",
    "\n",
    "signal = yy[:,0]\n",
    "\n",
    "TS['Gaussian'] = sp_sp.convolve(signal, normalize(sp.signal.gaussian(30,3)),'same')\n",
    "TS['Wiener']   = sp_sp.wiener(signal)\n",
    "TS['Butter']   = sp_sp.filtfilt(*sp_sp.butter(10, 0.12), signal, padlen=len(signal)//10)\n",
    "TS['Spline']   = sp.interpolate.InterpolatedUnivariateSpline(t.kkObs,signal)(t.kk)\n",
    "TS['Fourier']  = np.fft.irfft(truncate(np.fft.rfft(signal),len(signal)//14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the results. (The GUI is buggy. It might be necessary to execute the cell multiple times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "@interact(Visible=SelectMultiple(options=['Truth',\n",
    "  'Kalman smoother','Kalman filter','Butter','Gaussian','Wiener','Spline','Fourier']))\n",
    "def plot_results(Visible):\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.plot(t.kkObs,yy,'k*',label=\"Obs\")\n",
    "    if 'Truth'           in Visible: plt.plot(t.kk   ,xx[:,0]           ,'k',label=\"Truth\")\n",
    "    if 'Butter'          in Visible: plt.plot(t.kkObs,TS['Butter']      ,'r',label='Butter')\n",
    "    if 'Gaussian'        in Visible: plt.plot(t.kkObs,TS['Gaussian']    ,'g',label='Gaussian')\n",
    "    if 'Wiener'          in Visible: plt.plot(t.kkObs,TS['Wiener']      ,'y',label='Wiener')\n",
    "    if 'Spline'          in Visible: plt.plot(t.kk   ,TS['Spline']      ,'b',label='Spline')\n",
    "    if 'Fourier'         in Visible: plt.plot(t.kkObs,TS['Fourier']     ,'b',label='Trunc. Fourier')\n",
    "    if 'Kalman smoother' in Visible: plt.plot(t.kk   ,stats_KS.mu.u[:,0],'m',label=\"K. smoother\")\n",
    "    if 'Kalman filter'   in Visible:\n",
    "        #plt.plot(t.kkObs,stats_KF.mu.f[:,0],'b',label=\"K. filter (f)\")\n",
    "        #plt.plot(t.kkObs,stats_KF.mu.a[:,0],'c',label=\"K. filter (a)\")\n",
    "        #plt.plot(t.kk   ,stats_KF.mu.u[:,0],'c',label=\"K. filter\")\n",
    "        pw_muf, pw_mua = piece_wise_DA_step_lines(stats_KF.mu.f[:,0],stats_KF.mu.a[:,0])\n",
    "        pw_kkf, pw_kka = piece_wise_DA_step_lines(t.kkObs)\n",
    "        plt.plot(pw_kkf,pw_muf,'b',label=\"KF. forecast\")\n",
    "        plt.plot(pw_kka,pw_mua,'c',label=\"KF. analyses\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, it's hard to imagine better performance than from the Kalman smoother.\n",
    "However, recall the advantage of the Kalman filter (and smoother): they know the forecast model that generates the truth; they also know the noise levels Q and R (but they don't know the actual outcomes/realizations of the random noises), which also means that they do not need any *tuning*, or having to choose between the myriad of signal processing filters [out there](https://docs.scipy.org/doc/scipy/reference/signal.html#module-scipy.signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_error(estimate_at_obs_times):\n",
    "    return np.mean(np.abs(xx[t.kkObs,0] - estimate_at_obs_times))\n",
    "\n",
    "print('Gaussian'   , average_error(TS['Gaussian']))\n",
    "print('Wiener'     , average_error(TS['Wiener']))\n",
    "print('Butter'     , average_error(TS['Butter']))\n",
    "print('Spline'     , average_error(TS['Spline'][t.kkObs]))\n",
    "print('Fourier'    , average_error(TS['Fourier']))\n",
    "print('K. smoother', average_error(stats_KS.mu.u[t.kkObs,0]))\n",
    "print('K. filter'  , average_error(stats_KF.mu.a[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.22:** Theoretically, in the long run, the Kalman smoother should yield the best result. Verify this by increasing the experiment length to K=10**4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.24:** Re-run the experiment with different paramters, for example the observation noise strength or `dkObs`.\n",
    "[Results will differ even if you changed nothing because the truth noises (and obs) are stochastic.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.26:** Right before executing the assimilations (but after simulating the truth and obs), change $R$ by inserting:\n",
    "\n",
    "    setup.h.noise = GaussRV(C=0.01*eye(1))\n",
    "    \n",
    "What happens to the estimates of the Kalman filter and smoother?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exc 3.28*:** Try out different DAPPER methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Time series analysis\", which is what we've been seeing so far, is only a subset of \"Data assimilation\" (i.e. state estimation) [(much of time series analysis can be formulated as state estimation)](https://www.google.com/search?q=\"We+now+demonstrate+how+to+put+these+models+into+state+space+form\"). Moreover, DA methods also yield uncertainty quantifications, something which is generally obscure with \"time series analysis\" methods.\n",
    "\n",
    "DA really shines in the multivariate case. In part by its capacity to deal with sparsely observed systems, and in part because multivariate systems are often chaotic (which is more fun than stochastically-driven signals such as the above example). This is what we'll get to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Dynamical systems, chaos, Lorenz](T4 - Dynamical systems, chaos, Lorenz.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
