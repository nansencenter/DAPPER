{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.workspace import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we're going to code an EnKF implementation using numpy.\n",
    "The EnKF algorithm is given by eqns (2.12) - (2.17) of the [theoretical companion](./resources/DA_intro.pdf#page=11). It is also exlained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EnKF_animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of `estimate_mean_and_cov` and `estimate_cross_cov` from the previous section. Paste them in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def estimate_mean_and_cov ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "\n",
    "Before making the EnKF, we'll also set up an experiment to test it with. To that end, we'll use the Lorenz-63 model, from [T4](T4 - Dynamical systems, chaos, Lorenz.ipynb). The coupled ODEs are recalled here, but with some of the paremeters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 3 # ndim\n",
    "\n",
    "def dxdt(x):\n",
    "    sig  = 10.0\n",
    "    rho  = 28.0\n",
    "    beta = 8.0/3\n",
    "    x,y,z = x\n",
    "    d     = np.zeros(3)\n",
    "    d[0]  = sig*(y - x)\n",
    "    d[1]  = rho*x - y - x*z\n",
    "    d[2]  = x*y - beta*z\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a \"black box\" forecast model $f$ out of $\\frac{d \\mathbf{x}}{dt}$ such that $\\mathbf{x}(t+dt) = f(\\mathbf{x}(t),t,dt)$. We'll make use of the \"4th order Runge-Kutta\" integrator `rk4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(E, t0, dt):\n",
    "    \n",
    "    def step(x0):\n",
    "        return rk4(lambda t,x: dxdt(x), x0, t0, dt)\n",
    "    \n",
    "    if E.ndim == 1:\n",
    "        # Truth (single state vector) case\n",
    "        E = step(E)\n",
    "    else:\n",
    "        # Ensemble case\n",
    "        for n in range(E.shape[1]):\n",
    "            E[:,n] = step(E[:,n])\n",
    "    \n",
    "    return E\n",
    "\n",
    "\n",
    "Q_chol = zeros((m,m))\n",
    "Q      = Q_chol @ Q_chol.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the loop over each ensemble member. For better performance, this should be vectorized, if possible. Or, if the forecast model is computationally demanding (as is typically the case in real applications), the loop should be parallellized: i.e. the forecast simulations should be distributed to seperate computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the time settings that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt    = 0.01           # integrational time step\n",
    "dkObs = 25             # number of steps between observations\n",
    "dtObs = dkObs*dt       # time between observations\n",
    "KObs  = 60             # total number of observations\n",
    "K     = dkObs*(KObs+1) # total number of time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu0     = array([1.509, -1.531, 25.46])\n",
    "P0_chol = eye(3)\n",
    "P0      = P0_chol @ P0_chol.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 3 # ndim obs\n",
    "def h(E, t):\n",
    "    if E.ndim == 1: return E[:p]\n",
    "    else:           return E[:p,:]\n",
    "\n",
    "R_chol = sqrt(2)*eye(p)\n",
    "R      = R_chol @ R_chol.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic truth (`xx`) and observations (`yy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init\n",
    "xx    = zeros((K+1   ,m))\n",
    "yy    = zeros((KObs+1,p))\n",
    "xx[0] = mu0 + P0_chol @ randn(m)\n",
    "\n",
    "# Loop\n",
    "for k in range(1,K+1):\n",
    "    xx[k]  = f(xx[k-1],(k-1)*dt,dt)\n",
    "    xx[k] += Q_chol @ randn(m)\n",
    "    if not k%dkObs:\n",
    "        kObs = k//dkObs-1\n",
    "        yy[kObs] = h(xx[k],nan) + R_chol @ randn(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnKF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 6.2:** Complete the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = zeros((K+1,m))\n",
    "\n",
    "# Useful linear algebra: compute B/A\n",
    "def divide_1st_by_2nd(B,A):\n",
    "    return nla.solve(A.T,B.T).T\n",
    "\n",
    "def my_EnKF(N):\n",
    "    # Init ensemble\n",
    "    ...\n",
    "    for k in range(1,K+1):\n",
    "        # Forecast\n",
    "        t   = k*dt\n",
    "        # use model\n",
    "        E   = ...\n",
    "        # add noise\n",
    "        E  += ...\n",
    "        if not k%dkObs:\n",
    "            # Analysis\n",
    "            y        = yy[k//dkObs-1] # current observation\n",
    "            hE       = h(E,t)         # obsrved ensemble\n",
    "            # Compute ensemble moments\n",
    "            BH       = ...\n",
    "            HBH      = ...\n",
    "            # Compute Kalman Gain \n",
    "            KG       = ...\n",
    "            # Generate perturbations\n",
    "            Perturb  = ...\n",
    "            # Update ensemble with KG\n",
    "            E       += \n",
    "        # Save statistics\n",
    "        mu[k] = mean(E,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only store some stats (`mu`). This is because in large systems, keeping the entire ensemble in memory is probably too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('EnKF v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out its capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run assimilation\n",
    "my_EnKF(10)\n",
    "\n",
    "# Plot results\n",
    "plt.subplot(311)\n",
    "plt.plot(dt   *arange(K+1)     ,mu[:,0],'k')\n",
    "plt.plot(dt   *arange(K+1)     ,xx[:,0],'b')\n",
    "plt.plot(dtObs*arange(1,KObs+2),yy[:,0],'k*')\n",
    "plt.subplot(312)\n",
    "plt.plot(dt   *arange(K+1)     ,mu[:,1],'k')\n",
    "plt.plot(dt   *arange(K+1)     ,xx[:,1],'b')\n",
    "plt.plot(dtObs*arange(1,KObs+2),yy[:,1],'k*')\n",
    "plt.subplot(313)\n",
    "plt.plot(dt   *arange(K+1)     ,mu[:,2],'k')\n",
    "plt.plot(dt   *arange(K+1)     ,xx[:,2],'b')\n",
    "plt.plot(dtObs*arange(1,KObs+2),yy[:,2],'k*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 6.6:** The visuals of the plots are nice. But it would be good to have a summary statistic of the accuracy performance of the filter. Make a function `average_rmse(xx,mu)` that computes $ \\frac{1}{K+1} \\sum_{k=0}^K \\sqrt{\\frac{1}{m} \\| \\overline{\\mathbf{x}}_k - \\mathbf{x}_k \\|_2^2} \\, .$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_rmse(xx,mu):\n",
    "    ### INSERT ANSWER ###\n",
    "    return average\n",
    "\n",
    "# Test\n",
    "average_rmse(xx,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 6.8:**\n",
    " * (a). Repeat the above expriment, but now observing only the first (0th) component of the state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Repeat experiment a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * (b). Put a `seed()` command in the right place so as to be able to recreate exactly the same results from an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Repeat experiment b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * (c). Use $N=5$, and repeat the experiments. This is quite a small ensemble size, and quite often it will yield divergence: the EnKF \"definitely loses track\" of the truth, typically because of strong nonlinearity in the forecast models, and underestimation (by $\\overline{\\mathbf{P}})$ of the actual errors. Repeat the experiment with different seeds until you observe in the plots that divergence has happened.\n",
    " * (d). Implement \"multiplicative inflation\" to remedy the situation; this is a factor that should spread the ensemble further apart; a simple version is to inflate the perturbations. Implement it, and tune its value to try to avoid divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Repeat experiment cd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Benchmarking with DAPPER](T7 - Benchmarking with DAPPER.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
