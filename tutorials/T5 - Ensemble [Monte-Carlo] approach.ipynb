{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resources.resources import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble approach is an approximation in Bayesian inference. Instead of computing the full posterior distributions, we instead try to generate ensembles from them.\n",
    "\n",
    "An ensemble is an *iid* sample. I.e. a set of \"members\" (\"particles\", or \"sample points\") that have been drawn (\"sampled\") independently from the same distribution. With regards to the EnKF, these assumptions are generally tenous, but pragmatic.\n",
    "\n",
    "Ensembles can be used to characterize uncertainty: either by reconstructing (estimating) the distribution from which it is assumed drawn, or by computing various *statistics* such as the mean, median, variance, covariance, skewness, confidence intervals, etc (any function of the ensemble is a \"statistic\"). This is illustrated by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu  = 0\n",
    "P   = 25    \n",
    "P12 = sqrt(P)\n",
    "\n",
    "xx = linspace(-20,20,201)\n",
    "plt.plot(xx,ss.norm.pdf(xx,mu,sqrt(P)),label=\"True\");\n",
    "\n",
    "m = 1   # length of state vector\n",
    "N = 100 # ensemble size\n",
    "E = mu + P12*randn((N,m))\n",
    "\n",
    "plt.hist(E,normed=1,bins=max(10,N//30),label=\"Histogram estimate\")\n",
    "plt.plot(xx,ss.norm.pdf(xx,np.mean(E),sqrt(np.var(E))),label=\"Parametric estimate\")\n",
    "plot_ensemble(E)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Which approximation looks better: Histogram or the parametric? The EnKF takes advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** If the histogram bars are normalized by the value of the pdf at their location. How do you expect the resulting histogram to look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Multivariate Gaussian sampling. Suppose $\\mathbf{z}$ is a standard Gaussian, i.e. $p(\\mathbf{z}) = N(\\mathbf{z}|0,\\mathbf{I}_m)$, where $\\mathbf{I}_m$ is the $m$-dimensional identity matrix.\n",
    " * (a). Let $\\mathbf{x} = \\mathbf{L}\\mathbf{z} + \\mathbf{b}$. \n",
    "    Show that $p(\\mathbf{x}) = N(\\mathbf{x}|\\mathbf{b}, \\mathbf{L}^{}\\mathbf{L}^T)$.\n",
    "    You may take it for granted that [the sum of two Gaussian random variables is again a Gaussian](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Proof_using_convolutions).\n",
    " * (b). $\\mathbf{z}$ can be sampled using `randn((m,1))`. How (where) is `randn` defined?\n",
    " * (c). Consider the code below. How do you sample from $\\mathbf{x}$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m   = 3 # ndim\n",
    "mu  = 10*ones(m)\n",
    "P   = diag(1+arange(m))\n",
    "L   = np.linalg.cholesky(P)\n",
    "print(\"True mean and cov:\")\n",
    "print(mu)\n",
    "print(P)\n",
    "\n",
    "### INSERT ANSWER (c) ###\n",
    "z = randn((m,1))\n",
    "x = mu + L @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * (d). Now sample $N = 100$ realizations of $\\mathbf{x}$ and collect them in an $m$-by-$N$ \"ensemble matrix\" $\\mathbf{E}$. The main thing to figur out here is: how to add the mean vector to the ensemble matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N  = 100 # ensemble size\n",
    "\n",
    "### INSERT ANSWER (d) ###\n",
    "\n",
    "# Use the code below to assess whether you got it right\n",
    "x_bar = np.mean(E,axis=1)\n",
    "P_bar = np.cov(E)\n",
    "print(\"Estimated mean and cov:\")\n",
    "with printoptions(precision=1):\n",
    "    print(x_bar)\n",
    "    print(P_bar)\n",
    "plt.matshow(P_bar,cmap=\"Blues\"); plt.grid('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Gaussian sampling d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** How erroneous are the ensemble estimates on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Average sampling error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Given the previous ensemble matrix $\\mathbf{E}$, compute its sample mean $\\overline{\\mathbf{x}}$ and covariance matrix, $\\overline{\\mathbf{P}}$. Formulea are provided by eqn (2.9) of the [theoretical companion](./resources/DA_intro.pdf#page=11):\n",
    "$$ \\overline{\\mathbf{x}} = \\frac{1}{N}   \\sum_{n=1}^N \\mathbf{x}_n \\\\\n",
    "   \\overline{\\mathbf{P}} = \\frac{1}{N-1} \\sum_{n=1}^N (\\mathbf{x}_n - \\overline{\\mathbf{x}}) (\\mathbf{x}_n - \\overline{\\mathbf{x}})^T  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't use numpy's mean, cov\n",
    "def estimate_mean_and_cov(E):\n",
    "    m, N = E.shape\n",
    "    \n",
    "    ### INSERT ANSWER ###\n",
    "    \n",
    "    return x_bar, P_bar\n",
    "\n",
    "x_bar, P_bar = estimate_mean_and_cov(E)\n",
    "print(x_bar)\n",
    "print(P_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('ensemble moments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Why is the normalization by $(N-1)$ for the covariance computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Why (N-1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Like Matlab, Python (numpy) is quicker if you \"vectorize\" loops. This is emminently possible with computations of ensemble moments. \n",
    " * (a). Show that $\\overline{\\mathbf{P}}$ may also be computed as $\\mathbf{A} \\mathbf{A}^T /(N-1)$. Also see eqn (2.12) of the [theoretical companion](./resources/DA_intro.pdf#page=11)\n",
    " * (b). Code up this formula and insert it in `estimate_mean_and_cov(E)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('ensemble moments vectorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Implement the cross-covariance estimator $\\overline{Cov(\\mathbf{x}^1,\\mathbf{x}^2)} = \\frac{1}{N-1} \\sum_{n=1}^N (\\mathbf{x}^1_n - \\overline{\\mathbf{x}^1}) (\\mathbf{x}_n^2 - \\overline{\\mathbf{x}^2})^T  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_cross_cov(E1,E2):\n",
    "    ### INSERT ANSWER ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('estimate cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:**\n",
    " * (a). What's the difference between error residual?\n",
    " * (b). What's the difference between error and bias?\n",
    " * (c). Show `MSE = RMSE^2 = Bias^2 + Var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc:** Suppose $\\mathbf{x}$ is $m$-dimensional and has a covariance matrix $\\mathbf{B}$.\n",
    " * (a). What's the size of $\\mathbf{B}$?\n",
    " * (b). How many \"flops\" (approximately, i.e. to leading order) are required to solve the \"weighted average\" form of the KF update equation, eqn (A.16a) of the [DA intro](resources/DA_intro.pdf#page=29) ?\n",
    " * (c). How much memory (bytes) is required to hold its covariance matrix $\\mathbf{B}$ ?\n",
    " * (d). How many mega bytes's is this if $m$ is a million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show_answer('Cov memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the principal reasons why basic extended KF is infeasible for DA. Although not developed here, the EnKF avoids the explicit computation of covariance matrices, working instead with reduced-rank square roots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Next: [Writing your own EnKF](T6 - Writing your own EnKF.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
